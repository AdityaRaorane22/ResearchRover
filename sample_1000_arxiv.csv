terms,title,abstract,abstract_len,title_len
"['cs.CV', 'cs.LG', 'eess.IV']",D-SRGAN: DEM Super-Resolution with Generative Adversarial Networks,"LIDAR (light detection and ranging) is an optical remote-sensing technique
that measures the distance between sensor and object, and the reflected energy
from the object. Over the years, LIDAR data has been used as the primary source
of Digital Elevation Models (DEMs). DEMs have been used in a variety of
applications like road extraction, hydrological modeling, flood mapping, and
surface analysis. A number of studies in flooding suggest the usage of
high-resolution DEMs as inputs in the applications improve the overall
reliability and accuracy. Despite the importance of high-resolution DEM, many
areas in the United States and the world do not have access to high-resolution
DEM due to technological limitations or the cost of the data collection. With
recent development in Graphical Processing Units (GPU) and novel algorithms,
deep learning techniques have become attractive to researchers for their
performance in learning features from high-resolution datasets. Numerous new
methods have been proposed such as Generative Adversarial Networks (GANs) to
create intelligent models that correct and augment large-scale datasets. In
this paper, a GAN based model is developed and evaluated, inspired by single
image super-resolution methods, to increase the spatial resolution of a given
DEM dataset up to 4 times without additional information related to data.",1368,66
"['cs.LG', 'stat.ML']",One-dimensional Deep Image Prior for Time Series Inverse Problems,"We extend the Deep Image Prior (DIP) framework to one-dimensional signals.
DIP is using a randomly initialized convolutional neural network (CNN) to solve
linear inverse problems by optimizing over weights to fit the observed
measurements. Our main finding is that properly tuned one-dimensional
convolutional architectures provide an excellent Deep Image Prior for various
types of temporal signals including audio, biological signals, and sensor
measurements. We show that our network can be used in a variety of recovery
tasks including missing value imputation, blind denoising, and compressed
sensing from random Gaussian projections. The key challenge is how to avoid
overfitting by carefully tuning early stopping, total variation, and weight
decay regularization. Our method requires up to 4 times fewer measurements than
Lasso and outperforms NLM-VAMP for random Gaussian measurements on audio
signals, has similar imputation performance to a Kalman state-space model on a
variety of data, and outperforms wavelet filtering in removing additive noise
from air-quality sensor readings.",1093,65
"['cs.LG', 'cs.CL', 'stat.ML']",What Do You Mean I'm Funny? Personalizing the Joke Skill of a Voice-Controlled Virtual Assistant,"A considerable part of the success experienced by Voice-controlled virtual
assistants (VVA) is due to the emotional and personalized experience they
deliver, with humor being a key component in providing an engaging interaction.
In this paper we describe methods used to improve the joke skill of a VVA
through personalization. The first method, based on traditional NLP techniques,
is robust and scalable. The others combine self-attentional network and
multi-task learning to obtain better results, at the cost of added complexity.
A significant challenge facing these systems is the lack of explicit user
feedback needed to provide labels for the models. Instead, we explore the use
of two implicit feedback-based labelling strategies. All models were evaluated
on real production data. Online results show that models trained on any of the
considered labels outperform a heuristic method, presenting a positive
real-world impact on user satisfaction. Offline results suggest that the
deep-learning approaches can improve the joke experience with respect to the
other considered methods.",1090,96
"['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']",Answering Questions about Data Visualizations using Efficient Bimodal Fusion,"Chart question answering (CQA) is a newly proposed visual question answering
(VQA) task where an algorithm must answer questions about data visualizations,
e.g. bar charts, pie charts, and line graphs. CQA requires capabilities that
natural-image VQA algorithms lack: fine-grained measurements, optical character
recognition, and handling out-of-vocabulary words in both questions and
answers. Without modifications, state-of-the-art VQA algorithms perform poorly
on this task. Here, we propose a novel CQA algorithm called parallel recurrent
fusion of image and language (PReFIL). PReFIL first learns bimodal embeddings
by fusing question and image features and then intelligently aggregates these
learned embeddings to answer the given question. Despite its simplicity, PReFIL
greatly surpasses state-of-the art systems and human baselines on both the
FigureQA and DVQA datasets. Additionally, we demonstrate that PReFIL can be
used to reconstruct tables by asking a series of questions about a chart.",1003,76
['cs.LG'],Unsupervised Doppler Radar-Based Activity Recognition for e-healthcare,"Passive radio frequency (RF) sensing and monitoring of human daily activities
in elderly care homes has recently become an emerging topic due to the demand
with ageing population. Micro-Doppler radars are an appealing solution
considering their non-intrusiveness, deep penetration, and high-distance range.
This study presents an unsupervised framework for human activity monitoring
using Doppler streams. Two unsupervised feature extraction strategies based on
convolutional filtering and texture analysis of Doppler images are considered.
For the former, encoded features using Convolutional Variational Autoencoder
(CVAE) are compared with Convolutional Autoencoder (CAE) features. For the
latter, Grey-Level Co-occurrence Matrix (GLCM) is used. These methods are
further compared with unsupervised linear feature extraction based on Principal
Component Analysis (PCA) and Singular Value Decomposition (SVD). Using these
features, unsupervised samples clustering is performed using K-Means and
K-Medoids. Actual labels are solely used for evaluation and visualisation. The
results showcase 82.5% and 84% average testing accuracies for CVAE features and
77.5% and 72.5% average testing accuracy using texture features based on GLCM
using K-Means and K-Medoids respectively. The results show superiority of CVAE
and GLCM features compared to PCA, SVD, and CAE with more than 20% average
accuracy. Furthermore, for high-dimensional data visualisation, three manifold
learning techniques are considered including t-Distributed Stochastic Neighbour
Embedding (t-SNE), Multidimensional Scaling (MDS), and Locally Linear Embedding
(LLE). The visualisation methods are compared for projection of raw data as
well as the encoded features using CVAE. All three methods show an improved
visualisation ability when applied on the transformed CVAE data.",1843,70
"['cs.LG', 'stat.ML']",Efficient Decision Trees for Multi-class Support Vector Machines Using Entropy and Generalization Error Estimation,"We propose new methods for Support Vector Machines (SVMs) using tree
architecture for multi-class classi- fication. In each node of the tree, we
select an appropriate binary classifier using entropy and generalization error
estimation, then group the examples into positive and negative classes based on
the selected classi- fier and train a new classifier for use in the
classification phase. The proposed methods can work in time complexity between
O(log2N) to O(N) where N is the number of classes. We compared the performance
of our proposed methods to the traditional techniques on the UCI machine
learning repository using 10-fold cross-validation. The experimental results
show that our proposed methods are very useful for the problems that need fast
classification time or problems with a large number of classes as the proposed
methods run much faster than the traditional techniques but still provide
comparable accuracy.",932,114
['cs.CV'],Implicit Mesh Reconstruction from Unannotated Image Collections,"We present an approach to infer the 3D shape, texture, and camera pose for an
object from a single RGB image, using only category-level image collections
with foreground masks as supervision. We represent the shape as an
image-conditioned implicit function that transforms the surface of a sphere to
that of the predicted mesh, while additionally predicting the corresponding
texture. To derive supervisory signal for learning, we enforce that: a) our
predictions when rendered should explain the available image evidence, and b)
the inferred 3D structure should be geometrically consistent with learned pixel
to surface mappings. We empirically show that our approach improves over prior
work that leverages similar supervision, and in fact performs competitively to
methods that use stronger supervision. Finally, as our method enables learning
with limited supervision, we qualitatively demonstrate its applicability over a
set of about 30 object categories.",961,63
"['cs.CV', 'cs.CL']",Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays,"Automatic medical image report generation has drawn growing attention due to
its potential to alleviate radiologists' workload. Existing work on report
generation often trains encoder-decoder networks to generate complete reports.
However, such models are affected by data bias (e.g.~label imbalance) and face
common issues inherent in text generation models (e.g.~repetition). In this
work, we focus on reporting abnormal findings on radiology images; instead of
training on complete radiology reports, we propose a method to identify
abnormal findings from the reports in addition to grouping them with
unsupervised clustering and minimal rules. We formulate the task as cross-modal
retrieval and propose Conditional Visual-Semantic Embeddings to align images
and fine-grained abnormal findings in a joint embedding space. We demonstrate
that our method is able to retrieve abnormal findings and outperforms existing
generation models on both clinical correctness and text generation metrics.",994,83
"['cs.LG', 'stat.ML']",RL Unplugged: A Suite of Benchmarks for Offline Reinforcement Learning,"Offline methods for reinforcement learning have a potential to help bridge
the gap between reinforcement learning research and real-world applications.
They make it possible to learn policies from offline datasets, thus overcoming
concerns associated with online data collection in the real-world, including
cost, safety, or ethical concerns. In this paper, we propose a benchmark called
RL Unplugged to evaluate and compare offline RL methods. RL Unplugged includes
data from a diverse range of domains including games (e.g., Atari benchmark)
and simulated motor control problems (e.g., DM Control Suite). The datasets
include domains that are partially or fully observable, use continuous or
discrete actions, and have stochastic vs. deterministic dynamics. We propose
detailed evaluation protocols for each domain in RL Unplugged and provide an
extensive analysis of supervised learning and offline RL methods using these
protocols. We will release data for all our tasks and open-source all
algorithms presented in this paper. We hope that our suite of benchmarks will
increase the reproducibility of experiments and make it possible to study
challenging tasks with a limited computational budget, thus making RL research
both more systematic and more accessible across the community. Moving forward,
we view RL Unplugged as a living benchmark suite that will evolve and grow with
datasets contributed by the research community and ourselves. Our project page
is available on https://git.io/JJUhd.",1501,70
"['cs.LG', 'cs.AI', 'stat.ML']",Decoupling Dynamics and Reward for Transfer Learning,"Current reinforcement learning (RL) methods can successfully learn single
tasks but often generalize poorly to modest perturbations in task domain or
training procedure. In this work, we present a decoupled learning strategy for
RL that creates a shared representation space where knowledge can be robustly
transferred. We separate learning the task representation, the forward
dynamics, the inverse dynamics and the reward function of the domain, and show
that this decoupling improves performance within the task, transfers well to
changes in dynamics and reward, and can be effectively used for online
planning. Empirical results show good performance in both continuous and
discrete RL domains.",698,52
"['cs.LG', 'cs.AI', 'cs.RO']",Dream to Control: Learning Behaviors by Latent Imagination,"Learned world models summarize an agent's experience to facilitate learning
complex behaviors. While learning world models from high-dimensional sensory
inputs is becoming feasible through deep learning, there are many potential
ways for deriving behaviors from them. We present Dreamer, a reinforcement
learning agent that solves long-horizon tasks from images purely by latent
imagination. We efficiently learn behaviors by propagating analytic gradients
of learned state values back through trajectories imagined in the compact state
space of a learned world model. On 20 challenging visual control tasks, Dreamer
exceeds existing approaches in data-efficiency, computation time, and final
performance.",705,58
"['cs.LG', 'eess.SP', 'q-fin.MF', 'stat.ML']","Nonparametric Extrema Analysis in Time Series for Envelope Extraction, Peak Detection and Clustering","In this paper, we propose a nonparametric approach that can be used in
envelope extraction, peak-burst detection and clustering in time series. Our
problem formalization results in a naturally defined splitting/forking of the
time series. With a possibly hierarchical implementation, it can be used for
various applications in machine learning, signal processing and mathematical
finance. From an incoming input signal, our iterative procedure sequentially
creates two signals (one upper bounding and one lower bounding signal) by
minimizing the cumulative $L_1$ drift. We show that a solution can be
efficiently calculated by use of a Viterbi-like path tracking algorithm
together with an optimal elimination rule. We consider many interesting
settings, where our algorithm has near-linear time complexities.",809,100
"['cs.CV', 'cs.LG', 'stat.ML']",Training Effective Node Classifiers for Cascade Classification,"Cascade classifiers are widely used in real-time object detection. Different
from conventional classifiers that are designed for a low overall
classification error rate, a classifier in each node of the cascade is required
to achieve an extremely high detection rate and moderate false positive rate.
Although there are a few reported methods addressing this requirement in the
context of object detection, there is no principled feature selection method
that explicitly takes into account this asymmetric node learning objective. We
provide such an algorithm here. We show that a special case of the biased
minimax probability machine has the same formulation as the linear asymmetric
classifier (LAC) of Wu et al (2005). We then design a new boosting algorithm
that directly optimizes the cost function of LAC. The resulting
totally-corrective boosting algorithm is implemented by the column generation
technique in convex optimization. Experimental results on object detection
verify the effectiveness of the proposed boosting algorithm as a node
classifier in cascade object detection, and show performance better than that
of the current state-of-the-art.",1160,62
['cs.LG'],Causally-motivated Shortcut Removal Using Auxiliary Labels,"Robustness to certain forms of distribution shift is a key concern in many ML
applications. Often, robustness can be formulated as enforcing invariances to
particular interventions on the data generating process. Here, we study a
flexible, causally-motivated approach to enforcing such invariances, paying
special attention to shortcut learning, where a robust predictor can achieve
optimal i.i.d generalization in principle, but instead it relies on spurious
correlations or shortcuts in practice. Our approach uses auxiliary labels,
typically available at training time, to enforce conditional independences
between the latent factors that determine these labels. We show both
theoretically and empirically that causally-motivated regularization schemes
(a) lead to more robust estimators that generalize well under distribution
shift, and (b) have better finite sample efficiency compared to usual
regularization schemes, even in the absence of distribution shifts. Our
analysis highlights important theoretical properties of training techniques
commonly used in causal inference, fairness, and disentanglement literature.",1125,58
"['cs.CV', 'cs.CG', 'cs.LG']",Power-SLIC: Diagram-based superpixel generation,"Superpixel algorithms, which group pixels similar in color and other
low-level properties, are increasingly used for pre-processing in image
segmentation. Commonly important criteria for the computation of superpixels
are boundary adherence, speed, and regularity.
  Boundary adherence and regularity are typically contradictory goals. Most
recent algorithms have focused on improving boundary adherence. Motivated by
improving superpixel regularity, we propose a diagram-based superpixel
generation method called Power-SLIC.
  On the BSDS500 data set, Power-SLIC outperforms other state-of-the-art
algorithms in terms of compactness and boundary precision, and its boundary
adherence is the most robust against varying levels of Gaussian noise. In terms
of speed, Power-SLIC is competitive with SLIC.",801,47
['cs.CV'],SREdgeNet: Edge Enhanced Single Image Super Resolution using Dense Edge Detection Network and Feature Merge Network,"Deep learning based single image super-resolution (SR) methods have been
rapidly evolved over the past few years and have yielded state-of-the-art
performances over conventional methods. Since these methods usually minimized
l1 loss between the output SR image and the ground truth image, they yielded
very high peak signal-to-noise ratio (PSNR) that is inversely proportional to
these losses. Unfortunately, minimizing these losses inevitably lead to blurred
edges due to averaging of plausible solutions. Recently, SRGAN was proposed to
avoid this average effect by minimizing perceptual losses instead of l1 loss
and it yielded perceptually better SR images (or images with sharp edges) at
the price of lowering PSNR. In this paper, we propose SREdgeNet, edge enhanced
single image SR network, that was inspired by conventional SR theories so that
average effect could be avoided not by changing the loss, but by changing the
SR network property with the same l1 loss. Our SREdgeNet consists of 3
sequential deep neural network modules: the first module is any
state-of-the-art SR network and we selected a variant of EDSR. The second
module is any edge detection network taking the output of the first SR module
as an input and we propose DenseEdgeNet for this module. Lastly, the third
module is merging the outputs of the first and second modules to yield edge
enhanced SR image and we propose MergeNet for this module. Qualitatively, our
proposed method yielded images with sharp edges compared to other
state-of-the-art SR methods. Quantitatively, our SREdgeNet yielded
state-of-the-art performance in terms of structural similarity (SSIM) while
maintained comparable PSNR for x8 enlargement.",1700,115
"['stat.ML', 'cs.LG', 'cs.SI', 'math.PR']",Toward Universal Testing of Dynamic Network Models,"Numerous networks in the real world change over time, in the sense that nodes
and edges enter and leave the networks. Various dynamic random graph models
have been proposed to explain the macroscopic properties of these systems and
to provide a foundation for statistical inferences and predictions. It is of
interest to have a rigorous way to determine how well these models match
observed networks. We thus ask the following goodness of fit question: given a
sequence of observations/snapshots of a growing random graph, along with a
candidate model M, can we determine whether the snapshots came from M or from
some arbitrary alternative model that is well-separated from M in some natural
metric? We formulate this problem precisely and boil it down to goodness of fit
testing for graph-valued, infinite-state Markov processes and exhibit and
analyze a universal test based on non-stationary sampling for a natural class
of models.",935,50
"['cs.CV', 'cs.LG', 'stat.ML']",Semi-Supervised Semantic Image Segmentation with Self-correcting Networks,"Building a large image dataset with high-quality object masks for semantic
segmentation is costly and time consuming. In this paper, we introduce a
principled semi-supervised framework that only uses a small set of fully
supervised images (having semantic segmentation labels and box labels) and a
set of images with only object bounding box labels (we call it the weak set).
Our framework trains the primary segmentation model with the aid of an
ancillary model that generates initial segmentation labels for the weak set and
a self-correction module that improves the generated labels during training
using the increasingly accurate primary model. We introduce two variants of the
self-correction module using either linear or convolutional functions.
Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models
trained with a small fully supervised set perform similar to, or better than,
models trained with a large fully supervised set while requiring ~7x less
annotation effort.",1003,73
"['cs.CV', 'cs.LG']",VideoLightFormer: Lightweight Action Recognition using Transformers,"Efficient video action recognition remains a challenging problem. One large
model after another takes the place of the state-of-the-art on the Kinetics
dataset, but real-world efficiency evaluations are often lacking. In this work,
we fill this gap and investigate the use of transformers for efficient action
recognition. We propose a novel, lightweight action recognition architecture,
VideoLightFormer. In a factorized fashion, we carefully extend the 2D
convolutional Temporal Segment Network with transformers, while maintaining
spatial and temporal video structure throughout the entire model. Existing
methods often resort to one of the two extremes, where they either apply huge
transformers to video features, or minimal transformers on highly pooled video
features. Our method differs from them by keeping the transformer models small,
but leveraging full spatiotemporal feature structure. We evaluate
VideoLightFormer in a high-efficiency setting on the temporally-demanding
EPIC-KITCHENS-100 and Something-Something-V2 (SSV2) datasets and find that it
achieves a better mix of efficiency and accuracy than existing state-of-the-art
models, apart from the Temporal Shift Module on SSV2.",1197,67
"['cs.LG', 'stat.ML']",The Dilemma Between Dimensionality Reduction and Adversarial Robustness,"Recent work has shown the tremendous vulnerability to adversarial samples
that are nearly indistinguishable from benign data but are improperly
classified by the deep learning model. Some of the latest findings suggest the
existence of adversarial attacks may be an inherent weakness of these models as
a direct result of its sensitivity to well-generalizing features in high
dimensional data. We hypothesize that data transformations can influence this
vulnerability since a change in the data manifold directly determines the
adversary's ability to create these adversarial samples. To approach this
problem, we study the effect of dimensionality reduction through the lens of
adversarial robustness. This study raises awareness of the positive and
negative impacts of five commonly used data transformation techniques on
adversarial robustness. The evaluation shows how these techniques contribute to
an overall increased vulnerability where accuracy is only improved when the
dimensionality reduction technique approaches the data's optimal intrinsic
dimension. The conclusions drawn from this work contribute to understanding and
creating more resistant learning models.",1175,71
"['cs.CV', 'cs.LG']",Real-Time Panoptic Segmentation from Dense Detections,"Panoptic segmentation is a complex full scene parsing task requiring
simultaneous instance and semantic segmentation at high resolution. Current
state-of-the-art approaches cannot run in real-time, and simplifying these
architectures to improve efficiency severely degrades their accuracy. In this
paper, we propose a new single-shot panoptic segmentation network that
leverages dense detections and a global self-attention mechanism to operate in
real-time with performance approaching the state of the art. We introduce a
novel parameter-free mask construction method that substantially reduces
computational complexity by efficiently reusing information from the object
detection and semantic segmentation sub-tasks. The resulting network has a
simple data flow that does not require feature map re-sampling or clustering
post-processing, enabling significant hardware acceleration. Our experiments on
the Cityscapes and COCO benchmarks show that our network works at 30 FPS on
1024x2048 resolution, trading a 3% relative performance degradation from the
current state of the art for up to 440% faster inference.",1115,53
"['cs.LG', 'cs.AI']",Versatile Verification of Tree Ensembles,"Machine learned models often must abide by certain requirements (e.g.,
fairness or legal). This has spurred interested in developing approaches that
can provably verify whether a model satisfies certain properties. This paper
introduces a generic algorithm called Veritas that enables tackling multiple
different verification tasks for tree ensemble models like random forests (RFs)
and gradient boosting decision trees (GBDTs). This generality contrasts with
previous work, which has focused exclusively on either adversarial example
generation or robustness checking. Veritas formulates the verification task as
a generic optimization problem and introduces a novel search space
representation. Veritas offers two key advantages. First, it provides anytime
lower and upper bounds when the optimization problem cannot be solved exactly.
In contrast, many existing methods have focused on exact solutions and are thus
limited by the verification problem being NP-complete. Second, Veritas produces
full (bounded suboptimal) solutions that can be used to generate concrete
examples. We experimentally show that Veritas outperforms the previous state of
the art by (a) generating exact solutions more frequently, (b) producing
tighter bounds when (a) is not possible, and (c) offering orders of magnitude
speed ups. Subsequently, Veritas enables tackling more and larger real-world
verification scenarios.",1403,40
"['cs.LG', 'cs.AI', 'stat.ML']",Ellipsoidal Subspace Support Vector Data Description,"In this paper, we propose a novel method for transforming data into a
low-dimensional space optimized for one-class classification. The proposed
method iteratively transforms data into a new subspace optimized for
ellipsoidal encapsulation of target class data. We provide both linear and
non-linear formulations for the proposed method. The method takes into account
the covariance of the data in the subspace; hence, it yields a more generalized
solution as compared to Subspace Support Vector Data Description for a
hypersphere. We propose different regularization terms expressing the class
variance in the projected space. We compare the results with classic and
recently proposed one-class classification methods and achieve better results
in the majority of cases. The proposed method is also noticed to converge much
faster than recently proposed Subspace Support Vector Data Description.",896,52
['cs.LG'],Reframing demand forecasting: a two-fold approach for lumpy and intermittent demand,"Demand forecasting is a crucial component of demand management. While
shortening the forecasting horizon allows for more recent data and less
uncertainty, this frequently means lower data aggregation levels and a more
significant data sparsity. Sparse demand data usually results in lumpy or
intermittent demand patterns, which have sparse and irregular demand intervals.
Usual statistical and machine learning models fail to provide good forecasts in
such scenarios. Our research shows that competitive demand forecasts can be
obtained through two models: predicting the demand occurrence and estimating
the demand size. We analyze the usage of local and global machine learning
models for both cases and compare results against baseline methods. Finally, we
propose a novel evaluation criterion of lumpy and intermittent demand
forecasting models' performance. Our research shows that global classification
models are the best choice when predicting demand event occurrence. When
predicting demand sizes, we achieved the best results using Simple Exponential
Smoothing forecast. We tested our approach on real-world data consisting of 516
three-year-long time series corresponding to European automotive original
equipment manufacturers' daily demand.",1253,83
"['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', 'B.7.1; I.2.6']",Origami: A 803 GOp/s/W Convolutional Network Accelerator,"An ever increasing number of computer vision and image/video processing
challenges are being approached using deep convolutional neural networks,
obtaining state-of-the-art results in object recognition and detection,
semantic segmentation, action recognition, optical flow and superresolution.
Hardware acceleration of these algorithms is essential to adopt these
improvements in embedded and mobile computer vision systems. We present a new
architecture, design and implementation as well as the first reported silicon
measurements of such an accelerator, outperforming previous work in terms of
power-, area- and I/O-efficiency. The manufactured device provides up to 196
GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power
efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it
the first architecture scalable to TOp/s performance.",885,56
"['cs.LG', 'cs.AI']",DQ-SGD: Dynamic Quantization in SGD for Communication-Efficient Distributed Learning,"Gradient quantization is an emerging technique in reducing communication
costs in distributed learning. Existing gradient quantization algorithms often
rely on engineering heuristics or empirical observations, lacking a systematic
approach to dynamically quantize gradients. This paper addresses this issue by
proposing a novel dynamically quantized SGD (DQ-SGD) framework, enabling us to
dynamically adjust the quantization scheme for each gradient descent step by
exploring the trade-off between communication cost and convergence error. We
derive an upper bound, tight in some cases, of the convergence error for a
restricted family of quantization schemes and loss functions. We design our
DQ-SGD algorithm via minimizing the communication cost under the convergence
error constraints. Finally, through extensive experiments on large-scale
natural language processing and computer vision tasks on AG-News, CIFAR-10, and
CIFAR-100 datasets, we demonstrate that our quantization scheme achieves better
tradeoffs between the communication cost and learning performance than other
state-of-the-art gradient quantization methods.",1128,84
"['cs.CV', 'cs.RO']",Beyond Single Stage Encoder-Decoder Networks: Deep Decoders for Semantic Image Segmentation,"Single encoder-decoder methodologies for semantic segmentation are reaching
their peak in terms of segmentation quality and efficiency per number of
layers. To address these limitations, we propose a new architecture based on a
decoder which uses a set of shallow networks for capturing more information
content. The new decoder has a new topology of skip connections, namely
backward and stacked residual connections. In order to further improve the
architecture we introduce a weight function which aims to re-balance classes to
increase the attention of the networks to under-represented objects. We carried
out an extensive set of experiments that yielded state-of-the-art results for
the CamVid, Gatech and Freiburg Forest datasets. Moreover, to further prove the
effectiveness of our decoder, we conducted a set of experiments studying the
impact of our decoder to state-of-the-art segmentation techniques.
Additionally, we present a set of experiments augmenting semantic segmentation
with optical flow information, showing that motion clues can boost pure image
based semantic segmentation approaches.",1109,91
"['cs.CV', 'cs.AI']",PaXNet: Dental Caries Detection in Panoramic X-ray using Ensemble Transfer Learning and Capsule Classifier,"Dental caries is one of the most chronic diseases involving the majority of
the population during their lifetime. Caries lesions are typically diagnosed by
radiologists relying only on their visual inspection to detect via dental
x-rays. In many cases, dental caries is hard to identify using x-rays and can
be misinterpreted as shadows due to different reasons such as low image
quality. Hence, developing a decision support system for caries detection has
been a topic of interest in recent years. Here, we propose an automatic
diagnosis system to detect dental caries in Panoramic images for the first
time, to the best of authors' knowledge. The proposed model benefits from
various pretrained deep learning models through transfer learning to extract
relevant features from x-rays and uses a capsule network to draw prediction
results. On a dataset of 470 Panoramic images used for features extraction,
including 240 labeled images for classification, our model achieved an accuracy
score of 86.05\% on the test set. The obtained score demonstrates acceptable
detection performance and an increase in caries detection speed, as long as the
challenges of using Panoramic x-rays of real patients are taken into account.
Among images with caries lesions in the test set, our model acquired recall
scores of 69.44\% and 90.52\% for mild and severe ones, confirming the fact
that severe caries spots are more straightforward to detect and efficient mild
caries detection needs a more robust and larger dataset. Considering the
novelty of current research study as using Panoramic images, this work is a
step towards developing a fully automated efficient decision support system to
assist domain experts.",1704,106
['cs.CV'],MVP-Net: Multi-view FPN with Position-aware Attention for Deep Universal Lesion Detection,"Universal lesion detection (ULD) on computed tomography (CT) images is an
important but underdeveloped problem. Recently, deep learning-based approaches
have been proposed for ULD, aiming to learn representative features from
annotated CT data. However, the hunger for data of deep learning models and the
scarcity of medical annotation hinders these approaches to advance further. In
this paper, we propose to incorporate domain knowledge in clinical practice
into the model design of universal lesion detectors. Specifically, as
radiologists tend to inspect multiple windows for an accurate diagnosis, we
explicitly model this process and propose a multi-view feature pyramid network
(FPN), where multi-view features are extracted from images rendered with varied
window widths and window levels; to effectively combine this multi-view
information, we further propose a position-aware attention module. With the
proposed model design, the data-hunger problem is relieved as the learning task
is made easier with the correctly induced clinical practice prior. We show
promising results with the proposed model, achieving an absolute gain of
$\mathbf{5.65\%}$ (in the sensitivity of FPs@4.0) over the previous
state-of-the-art on the NIH DeepLesion dataset.",1257,89
"['cs.LG', 'cs.AI']",Reinforcement Learning for Sparse-Reward Object-Interaction Tasks in a First-person Simulated 3D Environment,"First-person object-interaction tasks in high-fidelity, 3D, simulated
environments such as the AI2Thor virtual home-environment pose significant
sample-efficiency challenges for reinforcement learning (RL) agents learning
from sparse task rewards. To alleviate these challenges, prior work has
provided extensive supervision via a combination of reward-shaping,
ground-truth object-information, and expert demonstrations. In this work, we
show that one can learn object-interaction tasks from scratch without
supervision by learning an attentive object-model as an auxiliary task during
task learning with an object-centric relational RL agent. Our key insight is
that learning an object-model that incorporates object-attention into forward
prediction provides a dense learning signal for unsupervised representation
learning of both objects and their relationships. This, in turn, enables faster
policy learning for an object-centric relational RL agent. We demonstrate our
agent by introducing a set of challenging object-interaction tasks in the
AI2Thor environment where learning with our attentive object-model is key to
strong performance. Specifically, we compare our agent and relational RL agents
with alternative auxiliary tasks to a relational RL agent equipped with
ground-truth object-information, and show that learning with our object-model
best closes the performance gap in terms of both learning speed and maximum
success rate. Additionally, we find that incorporating object-attention into an
object-model's forward predictions is key to learning representations which
capture object-category and object-state.",1630,108
"['cs.LG', 'cs.AI', 'cs.MA', 'cs.RO']",Lane-Merging Using Policy-based Reinforcement Learning and Post-Optimization,"Many current behavior generation methods struggle to handle real-world
traffic situations as they do not scale well with complexity. However,
behaviors can be learned off-line using data-driven approaches. Especially,
reinforcement learning is promising as it implicitly learns how to behave
utilizing collected experiences. In this work, we combine policy-based
reinforcement learning with local optimization to foster and synthesize the
best of the two methodologies. The policy-based reinforcement learning
algorithm provides an initial solution and guiding reference for the
post-optimization. Therefore, the optimizer only has to compute a single
homotopy class, e.g.\ drive behind or in front of the other vehicle. By storing
the state-history during reinforcement learning, it can be used for constraint
checking and the optimizer can account for interactions. The post-optimization
additionally acts as a safety-layer and the novel method, thus, can be applied
in safety-critical applications. We evaluate the proposed method using
lane-change scenarios with a varying number of vehicles.",1096,76
"['cs.CV', 'cs.AI', 'cs.LG']",Transformers in Vision: A Survey,"Astounding results from Transformer models on natural language tasks have
intrigued the vision community to study their application to computer vision
problems. Among their salient benefits, Transformers enable modeling long
dependencies between input sequence elements and support parallel processing of
sequence as compared to recurrent networks e.g., Long short-term memory (LSTM).
Different from convolutional networks, Transformers require minimal inductive
biases for their design and are naturally suited as set-functions. Furthermore,
the straightforward design of Transformers allows processing multiple
modalities (e.g., images, videos, text and speech) using similar processing
blocks and demonstrates excellent scalability to very large capacity networks
and huge datasets. These strengths have led to exciting progress on a number of
vision tasks using Transformer networks. This survey aims to provide a
comprehensive overview of the Transformer models in the computer vision
discipline. We start with an introduction to fundamental concepts behind the
success of Transformers i.e., self-attention, large-scale pre-training, and
bidirectional encoding. We then cover extensive applications of transformers in
vision including popular recognition tasks (e.g., image classification, object
detection, action recognition, and segmentation), generative modeling,
multi-modal tasks (e.g., visual-question answering, visual reasoning, and
visual grounding), video processing (e.g., activity recognition, video
forecasting), low-level vision (e.g., image super-resolution, image
enhancement, and colorization) and 3D analysis (e.g., point cloud
classification and segmentation). We compare the respective advantages and
limitations of popular techniques both in terms of architectural design and
their experimental value. Finally, we provide an analysis on open research
directions and possible future works.",1915,32
"['cs.LG', 'stat.ML']",Temporally-Extended ε-Greedy Exploration,"Recent work on exploration in reinforcement learning (RL) has led to a series
of increasingly complex solutions to the problem. This increase in complexity
often comes at the expense of generality. Recent empirical studies suggest
that, when applied to a broader set of domains, some sophisticated exploration
methods are outperformed by simpler counterparts, such as {\epsilon}-greedy. In
this paper we propose an exploration algorithm that retains the simplicity of
{\epsilon}-greedy while reducing dithering. We build on a simple hypothesis:
the main limitation of {\epsilon}-greedy exploration is its lack of temporal
persistence, which limits its ability to escape local optima. We propose a
temporally extended form of {\epsilon}-greedy that simply repeats the sampled
action for a random duration. It turns out that, for many duration
distributions, this suffices to improve exploration on a large set of domains.
Interestingly, a class of distributions inspired by ecological models of animal
foraging behaviour yields particularly strong performance.",1059,40
"['cs.CV', 'cs.SD', 'eess.AS']",Audiovisual Speaker Tracking using Nonlinear Dynamical Systems with Dynamic Stream Weights,"Data fusion plays an important role in many technical applications that
require efficient processing of multimodal sensory observations. A prominent
example is audiovisual signal processing, which has gained increasing attention
in automatic speech recognition, speaker localization and related tasks. If
appropriately combined with acoustic information, additional visual cues can
help to improve the performance in these applications, especially under adverse
acoustic conditions. A dynamic weighting of acoustic and visual streams based
on instantaneous sensor reliability measures is an efficient approach to data
fusion in this context. This paper presents a framework that extends the
well-established theory of nonlinear dynamical systems with the notion of
dynamic stream weights for an arbitrary number of sensory observations. It
comprises a recursive state estimator based on the Gaussian filtering paradigm,
which incorporates dynamic stream weights into a framework closely related to
the extended Kalman filter. Additionally, a convex optimization approach to
estimate oracle dynamic stream weights in fully observed dynamical systems
utilizing a Dirichlet prior is presented. This serves as a basis for a generic
parameter learning framework of dynamic stream weight estimators. The proposed
system is application-independent and can be easily adapted to specific tasks
and requirements. A study using audiovisual speaker tracking tasks is
considered as an exemplary application in this work. An improved tracking
performance of the dynamic stream weight-based estimation framework over
state-of-the-art methods is demonstrated in the experiments.",1662,90
"['cs.LG', 'cs.AI', 'cs.RO']",Affordance-based Reinforcement Learning for Urban Driving,"Traditional autonomous vehicle pipelines that follow a modular approach have
been very successful in the past both in academia and industry, which has led
to autonomy deployed on road. Though this approach provides ease of
interpretation, its generalizability to unseen environments is limited and
hand-engineering of numerous parameters is required, especially in the
prediction and planning systems. Recently, deep reinforcement learning has been
shown to learn complex strategic games and perform challenging robotic tasks,
which provides an appealing framework for learning to drive. In this work, we
propose a deep reinforcement learning framework to learn optimal control policy
using waypoints and low-dimensional visual representations, also known as
affordances. We demonstrate that our agents when trained from scratch learn the
tasks of lane-following, driving around inter-sections as well as stopping in
front of other actors or traffic lights even in the dense traffic setting. We
note that our method achieves comparable or better performance than the
baseline methods on the original and NoCrash benchmarks on the CARLA simulator.",1146,57
"['stat.ML', 'cs.LG', '60G15 (Primary) 68W10, 47B34 (Secondary)']",Linear-time inference for Gaussian Processes on one dimension,"Gaussian Processes (GPs) provide powerful probabilistic frameworks for
interpolation, forecasting, and smoothing, but have been hampered by
computational scaling issues. Here we investigate data sampled on one dimension
(e.g., a scalar or vector time series sampled at arbitrarily-spaced intervals),
for which state-space models are popular due to their linearly-scaling
computational costs. It has long been conjectured that state-space models are
general, able to approximate any one-dimensional GP. We provide the first
general proof of this conjecture, showing that any stationary GP on one
dimension with vector-valued observations governed by a Lebesgue-integrable
continuous kernel can be approximated to any desired precision using a
specifically-chosen state-space model: the Latent Exponentially Generated (LEG)
family. This new family offers several advantages compared to the general
state-space model: it is always stable (no unbounded growth), the covariance
can be computed in closed form, and its parameter space is unconstrained
(allowing straightforward estimation via gradient descent). The theorem's proof
also draws connections to Spectral Mixture Kernels, providing insight about
this popular family of kernels. We develop parallelized algorithms for
performing inference and learning in the LEG model, test the algorithm on real
and synthetic data, and demonstrate scaling to datasets with billions of
samples.",1433,61
"['cs.CV', 'cs.LG', 'cs.NE', 'stat.ML']",Virtual Worlds as Proxy for Multi-Object Tracking Analysis,"Modern computer vision algorithms typically require expensive data
acquisition and accurate manual labeling. In this work, we instead leverage the
recent progress in computer graphics to generate fully labeled, dynamic, and
photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual
world cloning method, and validate our approach by building and publicly
releasing a new video dataset, called Virtual KITTI (see
http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),
automatically labeled with accurate ground truth for object detection,
tracking, scene and instance segmentation, depth, and optical flow. We provide
quantitative experimental evidence suggesting that (i) modern deep learning
algorithms pre-trained on real data behave similarly in real and virtual
worlds, and (ii) pre-training on virtual data improves performance. As the gap
between real and virtual worlds is small, virtual worlds enable measuring the
impact of various weather and imaging conditions on recognition performance,
all other things being equal. We show these factors may affect drastically
otherwise high-performing deep models for tracking.",1177,58
"['cs.LG', 'cs.AI']",Memory and attention in deep learning,"Intelligence necessitates memory. Without memory, humans fail to perform
various nontrivial tasks such as reading novels, playing games or solving
maths. As the ultimate goal of machine learning is to derive intelligent
systems that learn and act automatically just like human, memory construction
for machine is inevitable. Artificial neural networks model neurons and
synapses in the brain by interconnecting computational units via weights, which
is a typical class of machine learning algorithms that resembles memory
structure. Their descendants with more complicated modeling techniques (a.k.a
deep learning) have been successfully applied to many practical problems and
demonstrated the importance of memory in the learning process of machinery
systems. Recent progresses on modeling memory in deep learning have revolved
around external memory constructions, which are highly inspired by
computational Turing models and biological neuronal systems. Attention
mechanisms are derived to support acquisition and retention operations on the
external memory. Despite the lack of theoretical foundations, these approaches
have shown promises to help machinery systems reach a higher level of
intelligence. The aim of this thesis is to advance the understanding on memory
and attention in deep learning. Its contributions include: (i) presenting a
collection of taxonomies for memory, (ii) constructing new memory-augmented
neural networks (MANNs) that support multiple control and memory units, (iii)
introducing variability via memory in sequential generative models, (iv)
searching for optimal writing operations to maximise the memorisation capacity
in slot-based memory networks, and (v) simulating the Universal Turing Machine
via Neural Stored-program Memory-a new kind of external memory for neural
networks.",1817,37
"['cs.LG', 'cs.DC', 'stat.ML']",Multitask and Transfer Learning for Autotuning Exascale Applications,"Multitask learning and transfer learning have proven to be useful in the
field of machine learning when additional knowledge is available to help a
prediction task. We aim at deriving methods following these paradigms for use
in autotuning, where the goal is to find the optimal performance parameters of
an application treated as a black-box function. We show comparative results
with state-of-the-art autotuning techniques. For instance, we observe an
average $1.5x$ improvement of the application runtime compared to the OpenTuner
and HpBandSter autotuners. We explain how our approaches can be more suitable
than some state-of-the-art autotuners for the tuning of any application in
general and of expensive exascale applications in particular.",748,68
['cs.CV'],Goal-Driven Sequential Data Abstraction,"Automatic data abstraction is an important capability for both benchmarking
machine intelligence and supporting summarization applications. In the former
one asks whether a machine can `understand' enough about the meaning of input
data to produce a meaningful but more compact abstraction. In the latter this
capability is exploited for saving space or human time by summarizing the
essence of input data. In this paper we study a general reinforcement learning
based framework for learning to abstract sequential data in a goal-driven way.
The ability to define different abstraction goals uniquely allows different
aspects of the input data to be preserved according to the ultimate purpose of
the abstraction. Our reinforcement learning objective does not require
human-defined examples of ideal abstraction. Importantly our model processes
the input sequence holistically without being constrained by the original input
order. Our framework is also domain agnostic -- we demonstrate applications to
sketch, video and text data and achieve promising results in all domains.",1077,39
"['stat.ML', 'cs.LG']",Integrating Domain Knowledge in Data-driven Earth Observation with Process Convolutions,"The modelling of Earth observation data is a challenging problem, typically
approached by either purely mechanistic or purely data-driven methods.
Mechanistic models encode the domain knowledge and physical rules governing the
system. Such models, however, need the correct specification of all
interactions between variables in the problem and the appropriate
parameterization is a challenge in itself. On the other hand, machine learning
approaches are flexible data-driven tools, able to approximate arbitrarily
complex functions, but lack interpretability and struggle when data is scarce
or in extrapolation regimes. In this paper, we argue that hybrid learning
schemes that combine both approaches can address all these issues efficiently.
We introduce Gaussian process (GP) convolution models for hybrid modelling in
Earth observation (EO) problems. We specifically propose the use of a class of
GP convolution models called latent force models (LFMs) for EO time series
modelling, analysis and understanding. LFMs are hybrid models that incorporate
physical knowledge encoded in differential equations into a multioutput GP
model. LFMs can transfer information across time-series, cope with missing
observations, infer explicit latent functions forcing the system, and learn
parameterizations which are very helpful for system analysis and
interpretability. We consider time series of soil moisture from active (ASCAT)
and passive (SMOS, AMSR2) microwave satellites. We show how assuming a first
order differential equation as governing equation, the model automatically
estimates the e-folding time or decay rate related to soil moisture persistence
and discovers latent forces related to precipitation. The proposed hybrid
methodology reconciles the two main approaches in remote sensing parameter
estimation by blending statistical learning and mechanistic modeling.",1877,87
"['cs.LG', 'stat.ML']",Can GAN Learn Topological Features of a Graph?,"This paper is first-line research expanding GANs into graph topology
analysis. By leveraging the hierarchical connectivity structure of a graph, we
have demonstrated that generative adversarial networks (GANs) can successfully
capture topological features of any arbitrary graph, and rank edge sets by
different stages according to their contribution to topology reconstruction.
Moreover, in addition to acting as an indicator of graph reconstruction, we
find that these stages can also preserve important topological features in a
graph.",538,46
['cs.LG'],ReviewViz: Assisting Developers Perform Empirical Study on Energy Consumption Related Reviews for Mobile Applications,"Improving the energy efficiency of mobile applications is a topic that has
gained a lot of attention recently. It has been addressed in a number of ways
such as identifying energy bugs and developing a catalog of energy patterns.
Previous work shows that users discuss the battery-related issues (energy
inefficiency or energy consumption) of the apps in their reviews. However,
there is no work that addresses the automatic extraction of battery-related
issues from users' feedback. In this paper, we report on a visualization tool
that is developed to empirically study machine learning algorithms and text
features to automatically identify the energy consumption specific reviews with
the highest accuracy. Other than the common machine learning algorithms, we
utilize deep learning models with different word embeddings to compare the
results. Furthermore, to help the developers extract the main topics that are
discussed in the reviews, two states of the art topic modeling algorithms are
applied. The visualizations of the topics represent the keywords that are
extracted for each topic along with a comparison with the results of string
matching. The developed web-browser based interactive visualization tool is a
novel framework developed with the intention of giving the app developers
insights about running time and accuracy of machine learning and deep learning
models as well as extracted topics. The tool makes it easier for the developers
to traverse through the extensive result set generated by the text
classification and topic modeling algorithms. The dynamic-data structure used
for the tool stores the baseline-results of the discussed approaches and is
updated when applied on new datasets. The tool is open-sourced to replicate the
research results.",1775,117
['cs.CV'],LUCSS: Language-based User-customized Colourization of Scene Sketches,"We introduce LUCSS, a language-based system for interactive col- orization of
scene sketches, based on their semantic understanding. LUCSS is built upon deep
neural networks trained via a large-scale repository of scene sketches and
cartoon-style color images with text descriptions. It con- sists of three
sequential modules. First, given a scene sketch, the segmenta- tion module
automatically partitions an input sketch into individual object instances.
Next, the captioning module generates the text description with spatial
relationships based on the instance-level segmentation results. Fi- nally, the
interactive colorization module allows users to edit the caption and produce
colored images based on the altered caption. Our experiments show the
effectiveness of our approach and the desirability of its compo- nents to
alternative choices.",849,69
"['cs.CV', 'eess.IV']",Progressively Unfreezing Perceptual GAN,"Generative adversarial networks (GANs) are widely used in image generation
tasks, yet the generated images are usually lack of texture details. In this
paper, we propose a general framework, called Progressively Unfreezing
Perceptual GAN (PUPGAN), which can generate images with fine texture details.
Particularly, we propose an adaptive perceptual discriminator with a
pre-trained perceptual feature extractor, which can efficiently measure the
discrepancy between multi-level features of the generated and real images. In
addition, we propose a progressively unfreezing scheme for the adaptive
perceptual discriminator, which ensures a smooth transfer process from a large
scale classification task to a specified image generation task. The qualitative
and quantitative experiments with comparison to the classical baselines on
three image generation tasks, i.e. single image super-resolution, paired
image-to-image translation and unpaired image-to-image translation demonstrate
the superiority of PUPGAN over the compared approaches.",1037,39
"['cs.LG', 'cs.SI', 'eess.SP']",AM-GCN: Adaptive Multi-channel Graph Convolutional Networks,"Graph Convolutional Networks (GCNs) have gained great popularity in tackling
various analytics tasks on graph and network data. However, some recent studies
raise concerns about whether GCNs can optimally integrate node features and
topological structures in a complex graph with rich information. In this paper,
we first present an experimental investigation. Surprisingly, our experimental
results clearly show that the capability of the state-of-the-art GCNs in fusing
node features and topological structures is distant from optimal or even
satisfactory. The weakness may severely hinder the capability of GCNs in some
classification tasks, since GCNs may not be able to adaptively learn some deep
correlation information between topological structures and node features. Can
we remedy the weakness and design a new type of GCNs that can retain the
advantages of the state-of-the-art GCNs and, at the same time, enhance the
capability of fusing topological structures and node features substantially? We
tackle the challenge and propose an adaptive multi-channel graph convolutional
networks for semi-supervised classification (AM-GCN). The central idea is that
we extract the specific and common embeddings from node features, topological
structures, and their combinations simultaneously, and use the attention
mechanism to learn adaptive importance weights of the embeddings. Our extensive
experiments on benchmark data sets clearly show that AM-GCN extracts the most
correlated information from both node features and topological structures
substantially, and improves the classification accuracy with a clear margin.",1625,59
"['cs.CV', 'cs.LG']",Exploring Vision Transformers for Fine-grained Classification,"Existing computer vision research in categorization struggles with
fine-grained attributes recognition due to the inherently high intra-class
variances and low inter-class variances. SOTA methods tackle this challenge by
locating the most informative image regions and rely on them to classify the
complete image. The most recent work, Vision Transformer (ViT), shows its
strong performance in both traditional and fine-grained classification tasks.
In this work, we propose a multi-stage ViT framework for fine-grained image
classification tasks, which localizes the informative image regions without
requiring architectural changes using the inherent multi-head self-attention
mechanism. We also introduce attention-guided augmentations for improving the
model's capabilities. We demonstrate the value of our approach by experimenting
with four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,
Stanford Dogs, and FGVC7 Plant Pathology. We also prove our model's
interpretability via qualitative results.",1018,61
['cs.CV'],Motion-Augmented Self-Training for Video Recognition at Smaller Scale,"The goal of this paper is to self-train a 3D convolutional neural network on
an unlabeled video collection for deployment on small-scale video collections.
As smaller video datasets benefit more from motion than appearance, we strive
to train our network using optical flow, but avoid its computation during
inference. We propose the first motion-augmented self-training regime, we call
MotionFit. We start with supervised training of a motion model on a small, and
labeled, video collection. With the motion model we generate pseudo-labels for
a large unlabeled video collection, which enables us to transfer knowledge by
learning to predict these pseudo-labels with an appearance model. Moreover, we
introduce a multi-clip loss as a simple yet efficient way to improve the
quality of the pseudo-labeling, even without additional auxiliary tasks. We
also take into consideration the temporal granularity of videos during
self-training of the appearance model, which was missed in previous works. As a
result we obtain a strong motion-augmented representation model suited for
video downstream tasks like action recognition and clip retrieval. On
small-scale video datasets, MotionFit outperforms alternatives for knowledge
transfer by 5%-8%, video-only self-supervision by 1%-7% and semi-supervised
learning by 9%-18% using the same amount of class labels.",1357,69
['cs.LG'],Modeling transition dynamics in MDPs with RKHS embeddings of conditional distributions,"We propose a new, nonparametric approach to estimating the value function in
reinforcement learning. This approach makes use of a recently developed
representation of conditional distributions as functions in a reproducing
kernel Hilbert space. Such representations bypass the need for estimating
transition probabilities, and apply to any domain on which kernels can be
defined. Our approach avoids the need to approximate intractable integrals
since expectations are represented as RKHS inner products whose computation has
linear complexity in the sample size. Thus, we can efficiently perform value
function estimation in a wide variety of settings, including finite state
spaces, continuous states spaces, and partially observable tasks where only
sensor measurements are available. A second advantage of the approach is that
we learn the conditional distribution representation from a training sample,
and do not require an exhaustive exploration of the state space. We prove
convergence of our approach either to the optimal policy, or to the closest
projection of the optimal policy in our model class, under reasonable
assumptions. In experiments, we demonstrate the performance of our algorithm on
a learning task in a continuous state space (the under-actuated pendulum), and
on a navigation problem where only images from a sensor are observed. We
compare with least-squares policy iteration where a Gaussian process is used
for value function estimation. Our algorithm achieves better performance in
both tasks.",1524,86
['cs.CV'],End-to-End Learning of Multi-scale Convolutional Neural Network for Stereo Matching,"Deep neural networks have shown excellent performance in stereo matching
task. Recently CNN-based methods have shown that stereo matching can be
formulated as a supervised learning task. However, less attention is paid on
the fusion of contextual semantic information and details. To tackle this
problem, we propose a network for disparity estimation based on abundant
contextual details and semantic information, called Multi-scale Features
Network (MSFNet). First, we design a new structure to encode rich semantic
information and fine-grained details by fusing multi-scale features. And we
combine the advantages of element-wise addition and concatenation, which is
conducive to merge semantic information with details. Second, a guidance
mechanism is introduced to guide the network to automatically focus more on the
unreliable regions. Third, we formulate the consistency check as an error map,
obtained by the low stage features with fine-grained details. Finally, we adopt
the consistency checking between the left feature and the synthetic left
feature to refine the initial disparity. Experiments on Scene Flow and KITTI
2015 benchmark demonstrated that the proposed method can achieve the
state-of-the-art performance.",1229,83
['cs.CV'],SpaceMeshLab: Spatial Context Memoization and Meshgrid Atrous Convolution Consensus for Semantic Segmentation,"Semantic segmentation networks adopt transfer learning from image
classification networks which occurs a shortage of spatial context information.
For this reason, we propose Spatial Context Memoization (SpaM), a bypassing
branch for spatial context by retaining the input dimension and constantly
communicating its spatial context and rich semantic information mutually with
the backbone network. Multi-scale context information for semantic segmentation
is crucial for dealing with diverse sizes and shapes of target objects in the
given scene. Conventional multi-scale context scheme adopts multiple effective
receptive fields by multiple dilation rates or pooling operations, but often
suffer from misalignment problem with respect to the target pixel. To this end,
we propose Meshgrid Atrous Convolution Consensus (MetroCon^2) which brings
multi-scale scheme into fine-grained multi-scale object context using
convolutions with meshgrid-like scattered dilation rates. SpaceMeshLab
(ResNet-101 + SpaM + MetroCon^2) achieves 82.0% mIoU in Cityscapes test and
53.5% mIoU on Pascal-Context validation set.",1105,109
['cs.CV'],H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes,"Liver cancer is one of the leading causes of cancer death. To assist doctors
in hepatocellular carcinoma diagnosis and treatment planning, an accurate and
automatic liver and tumor segmentation method is highly demanded in the
clinical practice. Recently, fully convolutional neural networks (FCNs),
including 2D and 3D FCNs, serve as the back-bone in many volumetric image
segmentation. However, 2D convolutions can not fully leverage the spatial
information along the third dimension while 3D convolutions suffer from high
computational cost and GPU memory consumption. To address these issues, we
propose a novel hybrid densely connected UNet (H-DenseUNet), which consists of
a 2D DenseUNet for efficiently extracting intra-slice features and a 3D
counterpart for hierarchically aggregating volumetric contexts under the spirit
of the auto-context algorithm for liver and tumor segmentation. We formulate
the learning process of H-DenseUNet in an end-to-end manner, where the
intra-slice representations and inter-slice features can be jointly optimized
through a hybrid feature fusion (HFF) layer. We extensively evaluated our
method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge
and 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the
segmentation results of tumors and achieved very competitive performance for
liver segmentation even with a single model.",1408,91
['cs.CV'],Auto4D: Learning to Label 4D Objects from Sequential Point Clouds,"In the past few years we have seen great advances in object perception
(particularly in 4D space-time dimensions) thanks to deep learning methods.
However, they typically rely on large amounts of high-quality labels to achieve
good performance, which often require time-consuming and expensive work by
human annotators. To address this we propose an automatic annotation pipeline
that generates accurate object trajectories in 3D space (i.e., 4D labels) from
LiDAR point clouds. The key idea is to decompose the 4D object label into two
parts: the object size in 3D that's fixed through time for rigid objects, and
the motion path describing the evolution of the object's pose through time.
Instead of generating a series of labels in one shot, we adopt an iterative
refinement process where online generated object detections are tracked through
time as the initialization. Given the cheap but noisy input, our model produces
higher quality 4D labels by re-estimating the object size and smoothing the
motion path, where the improvement is achieved by exploiting aggregated
observations and motion cues over the entire trajectory. We validate the
proposed method on a large-scale driving dataset and show a 25% reduction of
human annotation efforts. We also showcase the benefits of our approach in the
annotator-in-the-loop setting.",1334,65
['cs.CV'],Pixel-wise Ear Detection with Convolutional Encoder-Decoder Networks,"Object detection and segmentation represents the basis for many tasks in
computer and machine vision. In biometric recognition systems the detection of
the region-of-interest (ROI) is one of the most crucial steps in the overall
processing pipeline, significantly impacting the performance of the entire
recognition system. Existing approaches to ear detection, for example, are
commonly susceptible to the presence of severe occlusions, ear accessories or
variable illumination conditions and often deteriorate in their performance if
applied on ear images captured in unconstrained settings. To address these
shortcomings, we present in this paper a novel ear detection technique based on
convolutional encoder-decoder networks (CEDs). For our technique, we formulate
the problem of ear detection as a two-class segmentation problem and train a
convolutional encoder-decoder network based on the SegNet architecture to
distinguish between image-pixels belonging to either the ear or the non-ear
class. The output of the network is then post-processed to further refine the
segmentation result and return the final locations of the ears in the input
image. Different from competing techniques from the literature, our approach
does not simply return a bounding box around the detected ear, but provides
detailed, pixel-wise information about the location of the ears in the image.
Our experiments on a dataset gathered from the web (a.k.a. in the wild) show
that the proposed technique ensures good detection results in the presence of
various covariate factors and significantly outperforms the existing
state-of-the-art.",1623,68
"['cs.LG', 'stat.ML']",On Computation and Generalization of Generative Adversarial Imitation Learning,"Generative Adversarial Imitation Learning (GAIL) is a powerful and practical
approach for learning sequential decision-making policies. Different from
Reinforcement Learning (RL), GAIL takes advantage of demonstration data by
experts (e.g., human), and learns both the policy and reward function of the
unknown environment. Despite the significant empirical progresses, the theory
behind GAIL is still largely unknown. The major difficulty comes from the
underlying temporal dependency of the demonstration data and the minimax
computational formulation of GAIL without convex-concave structure. To bridge
such a gap between theory and practice, this paper investigates the theoretical
properties of GAIL. Specifically, we show: (1) For GAIL with general reward
parameterization, the generalization can be guaranteed as long as the class of
the reward functions is properly controlled; (2) For GAIL, where the reward is
parameterized as a reproducing kernel function, GAIL can be efficiently solved
by stochastic first order optimization algorithms, which attain sublinear
convergence to a stationary solution. To the best of our knowledge, these are
the first results on statistical and computational guarantees of imitation
learning with reward/policy function approximation. Numerical experiments are
provided to support our analysis.",1337,78
['cs.CV'],Gray-Level Image Transitions Driven by Tsallis Entropic Index,"The maximum entropy principle is largely used in thresholding and
segmentation of images. Among the several formulations of this principle, the
most effectively applied is that based on Tsallis non-extensive entropy. Here,
we discuss the role of its entropic index in determining the thresholds. When
this index is spanning the interval (0,1), for some images, the values of
thresholds can have large leaps. In this manner, we observe abrupt transitions
in the appearance of corresponding bi-level or multi-level images. These
gray-level image transitions are analogous to order or texture transitions
observed in physical systems, transitions which are driven by the temperature
or by other physical quantities.",712,61
"['cs.LG', 'stat.ML']",Sum-Product Networks for Hybrid Domains,"While all kinds of mixed data -from personal data, over panel and scientific
data, to public and commercial data- are collected and stored, building
probabilistic graphical models for these hybrid domains becomes more difficult.
Users spend significant amounts of time in identifying the parametric form of
the random variables (Gaussian, Poisson, Logit, etc.) involved and learning the
mixed models. To make this difficult task easier, we propose the first
trainable probabilistic deep architecture for hybrid domains that features
tractable queries. It is based on Sum-Product Networks (SPNs) with piecewise
polynomial leave distributions together with novel nonparametric decomposition
and conditioning steps using the Hirschfeld-Gebelein-R\'enyi Maximum
Correlation Coefficient. This relieves the user from deciding a-priori the
parametric form of the random variables but is still expressive enough to
effectively approximate any continuous distribution and permits efficient
learning and inference. Our empirical evidence shows that the architecture,
called Mixed SPNs, can indeed capture complex distributions across a wide range
of hybrid domains.",1155,39
"['cs.LG', 'stat.ML']",Traffic Flow Combination Forecasting Method Based on Improved LSTM and ARIMA,"Traffic flow forecasting is hot spot research of intelligent traffic system
construction. The existing traffic flow prediction methods have problems such
as poor stability, high data requirements, or poor adaptability. In this paper,
we define the traffic data time singularity ratio in the dropout module and
propose a combination prediction method based on the improved long short-term
memory neural network and time series autoregressive integrated moving average
model (SDLSTM-ARIMA), which is derived from the Recurrent Neural Networks (RNN)
model. It compares the traffic data time singularity with the probability value
in the dropout module and combines them at unequal time intervals to achieve an
accurate prediction of traffic flow data. Then, we design an adaptive traffic
flow embedded system that can adapt to Java, Python and other languages and
other interfaces. The experimental results demonstrate that the method based on
the SDLSTM - ARIMA model has higher accuracy than the similar method using only
autoregressive integrated moving average or autoregressive. Our embedded
traffic prediction system integrating computer vision, machine learning and
cloud has the advantages such as high accuracy, high reliability and low cost.
Therefore, it has a wide application prospect.",1295,76
"['cs.LG', 'stat.ML']",A Fast and Easy Regression Technique for k-NN Classification Without Using Negative Pairs,"This paper proposes an inexpensive way to learn an effective dissimilarity
function to be used for $k$-nearest neighbor ($k$-NN) classification. Unlike
Mahalanobis metric learning methods that map both query (unlabeled) objects and
labeled objects to new coordinates by a single transformation, our method
learns a transformation of labeled objects to new points in the feature space
whereas query objects are kept in their original coordinates. This method has
several advantages over existing distance metric learning methods: (i) In
experiments with large document and image datasets, it achieves $k$-NN
classification accuracy better than or at least comparable to the
state-of-the-art metric learning methods. (ii) The transformation can be
learned efficiently by solving a standard ridge regression problem. For
document and image datasets, training is often more than two orders of
magnitude faster than the fastest metric learning methods tested. This speed-up
is also due to the fact that the proposed method eliminates the optimization
over ""negative"" object pairs, i.e., objects whose class labels are different.
(iii) The formulation has a theoretical justification in terms of reducing
hubness in data.",1215,89
['cs.CV'],Stochastic Texture Difference for Scale-Dependent Data Analysis,"This article introduces the Stochastic Texture Difference method for
analyzing data at prescribed spatial and value scales. This method relies on
constrained random walks around each pixel, describing how nearby image values
typically evolve on each side of this pixel. Textures are represented as
probability distributions of such random walks, so a texture difference
operator is statistically defined as a distance between these distributions in
a suitable reproducing kernel Hilbert space. The method is thus not limited to
scalar pixel values: any data type for which a kernel is available may be
considered, from color triplets and multispectral vector data to strings,
graphs, and more. By adjusting the size of the neighborhoods that are compared,
the method is implicitly scale-dependent. It is also able to focus on either
small changes or large gradients. We demonstrate how it can be used to infer
spatial and data value characteristic scales in measured signals and natural
images.",994,63
['cs.CV'],Evolution of Image Segmentation using Deep Convolutional Neural Network: A Survey,"From the autonomous car driving to medical diagnosis, the requirement of the
task of image segmentation is everywhere. Segmentation of an image is one of
the indispensable tasks in computer vision. This task is comparatively
complicated than other vision tasks as it needs low-level spatial information.
Basically, image segmentation can be of two types: semantic segmentation and
instance segmentation. The combined version of these two basic tasks is known
as panoptic segmentation. In the recent era, the success of deep convolutional
neural networks (CNN) has influenced the field of segmentation greatly and gave
us various successful models to date. In this survey, we are going to take a
glance at the evolution of both semantic and instance segmentation work based
on CNN. We have also specified comparative architectural details of some
state-of-the-art models and discuss their training details to present a lucid
understanding of hyper-parameter tuning of those models. We have also drawn a
comparison among the performance of those models on different datasets. Lastly,
we have given a glimpse of some state-of-the-art panoptic segmentation models.",1160,81
['cs.CV'],Dual Encoder-Decoder based Generative Adversarial Networks for Disentangled Facial Representation Learning,"To learn disentangled representations of facial images, we present a Dual
Encoder-Decoder based Generative Adversarial Network (DED-GAN). In the proposed
method, both the generator and discriminator are designed with deep
encoder-decoder architectures as their backbones. To be more specific, the
encoder-decoder structured generator is used to learn a pose disentangled face
representation, and the encoder-decoder structured discriminator is tasked to
perform real/fake classification, face reconstruction, determining identity and
estimating face pose. We further improve the proposed network architecture by
minimising the additional pixel-wise loss defined by the Wasserstein distance
at the output of the discriminator so that the adversarial framework can be
better trained. Additionally, we consider face pose variation to be continuous,
rather than discrete in existing literature, to inject richer pose information
into our model. The pose estimation task is formulated as a regression problem,
which helps to disentangle identity information from pose variations. The
proposed network is evaluated on the tasks of pose-invariant face recognition
(PIFR) and face synthesis across poses. An extensive quantitative and
qualitative evaluation carried out on several controlled and in-the-wild
benchmarking datasets demonstrates the superiority of the proposed DED-GAN
method over the state-of-the-art approaches.",1419,106
"['cs.LG', 'stat.ML']",Regularizing Black-box Models for Improved Interpretability (HILL 2019 Version),"Most of the work on interpretable machine learning has focused on designing
either inherently interpretable models, which typically trade-off accuracy for
interpretability, or post-hoc explanation systems, which lack guarantees about
their explanation quality. We propose an alternative to these approaches by
directly regularizing a black-box model for interpretability at training time.
Our approach explicitly connects three key aspects of interpretable machine
learning: (i) the model's innate explainability, (ii) the explanation system
used at test time, and (iii) the metrics that measure explanation quality. Our
regularization results in substantial improvement in terms of the explanation
fidelity and stability metrics across a range of datasets and black-box
explanation systems while slightly improving accuracy. Further, if the
resulting model is still not sufficiently interpretable, the weight of the
regularization term can be adjusted to achieve the desired trade-off between
accuracy and interpretability. Finally, we justify theoretically that the
benefits of explanation-based regularization generalize to unseen points.",1141,79
['cs.CV'],CroP: Color Constancy Benchmark Dataset Generator,"Implementing color constancy as a pre-processing step in contemporary digital
cameras is of significant importance as it removes the influence of scene
illumination on object colors. Several benchmark color constancy datasets have
been created for the purpose of developing and testing new color constancy
methods. However, they all have numerous drawbacks including a small number of
images, erroneously extracted ground-truth illuminations, long histories of
misuses, violations of their stated assumptions, etc. To overcome such and
similar problems, in this paper a color constancy benchmark dataset generator
is proposed. For a given camera sensor it enables generation of any number of
realistic raw images taken in a subset of the real world, namely images of
printed photographs. Datasets with such images share many positive features
with other existing real-world datasets, while some of the negative features
are completely eliminated. The generated images can be successfully used to
train methods that afterward achieve high accuracy on real-world datasets. This
opens the way for creating large enough datasets for advanced deep learning
techniques. Experimental results are presented and discussed. The source code
is available at http://www.fer.unizg.hr/ipg/resources/color_constancy/.",1301,49
['cs.LG'],POMO: Policy Optimization with Multiple Optima for Reinforcement Learning,"In neural combinatorial optimization (CO), reinforcement learning (RL) can
turn a deep neural net into a fast, powerful heuristic solver of NP-hard
problems. This approach has a great potential in practical applications because
it allows near-optimal solutions to be found without expert guides armed with
substantial domain knowledge. We introduce Policy Optimization with Multiple
Optima (POMO), an end-to-end approach for building such a heuristic solver.
POMO is applicable to a wide range of CO problems. It is designed to exploit
the symmetries in the representation of a CO solution. POMO uses a modified
REINFORCE algorithm that forces diverse rollouts towards all optimal solutions.
Empirically, the low-variance baseline of POMO makes RL training fast and
stable, and it is more resistant to local minima compared to previous
approaches. We also introduce a new augmentation-based inference method, which
accompanies POMO nicely. We demonstrate the effectiveness of POMO by solving
three popular NP-hard problems, namely, traveling salesman (TSP), capacitated
vehicle routing (CVRP), and 0-1 knapsack (KP). For all three, our solver based
on POMO shows a significant improvement in performance over all recent learned
heuristics. In particular, we achieve the optimality gap of 0.14% with TSP100
while reducing inference time by more than an order of magnitude.",1371,73
['cs.CV'],Face De-occlusion using 3D Morphable Model and Generative Adversarial Network,"In recent decades, 3D morphable model (3DMM) has been commonly used in
image-based photorealistic 3D face reconstruction. However, face images are
often corrupted by serious occlusion by non-face objects including eyeglasses,
masks, and hands. Such objects block the correct capture of landmarks and
shading information. Therefore, the reconstructed 3D face model is hardly
reusable. In this paper, a novel method is proposed to restore de-occluded face
images based on inverse use of 3DMM and generative adversarial network. We
utilize the 3DMM prior to the proposed adversarial network and combine a global
and local adversarial convolutional neural network to learn face de-occlusion
model. The 3DMM serves not only as geometric prior but also proposes the face
region for the local discriminator. Experiment results confirm the
effectiveness and robustness of the proposed algorithm in removing challenging
types of occlusions with various head poses and illumination. Furthermore, the
proposed method reconstructs the correct 3D face model with de-occluded
textures.",1071,77
"['cs.LG', 'cs.CV', 'stat.ML', 'I.2.6']",MMGAN: Generative Adversarial Networks for Multi-Modal Distributions,"Over the past years, Generative Adversarial Networks (GANs) have shown a
remarkable generation performance especially in image synthesis. Unfortunately,
they are also known for having an unstable training process and might loose
parts of the data distribution for heterogeneous input data. In this paper, we
propose a novel GAN extension for multi-modal distribution learning (MMGAN). In
our approach, we model the latent space as a Gaussian mixture model with a
number of clusters referring to the number of disconnected data manifolds in
the observation space, and include a clustering network, which relates each
data manifold to one Gaussian cluster. Thus, the training gets more stable.
Moreover, MMGAN allows for clustering real data according to the learned data
manifold in the latent space. By a series of benchmark experiments, we
illustrate that MMGAN outperforms competitive state-of-the-art models in terms
of clustering performance.",946,68
"['cs.CV', 'I.4.6']",Scale-Based Gaussian Coverings: Combining Intra and Inter Mixture Models in Image Segmentation,"By a ""covering"" we mean a Gaussian mixture model fit to observed data.
Approximations of the Bayes factor can be availed of to judge model fit to the
data within a given Gaussian mixture model. Between families of Gaussian
mixture models, we propose the R\'enyi quadratic entropy as an excellent and
tractable model comparison framework. We exemplify this using the segmentation
of an MRI image volume, based (1) on a direct Gaussian mixture model applied to
the marginal distribution function, and (2) Gaussian model fit through k-means
applied to the 4D multivalued image volume furnished by the wavelet transform.
Visual preference for one model over another is not immediate. The R\'enyi
quadratic entropy allows us to show clearly that one of these modelings is
superior to the other.",789,94
"['cs.LG', 'cs.NE', 'stat.ML']",Recurrent Neural Networks for Multivariate Time Series with Missing Values,"Multivariate time series data in practical applications, such as health care,
geoscience, and biology, are characterized by a variety of missing values. In
time series prediction and other related tasks, it has been noted that missing
values and their missing patterns are often correlated with the target labels,
a.k.a., informative missingness. There is very limited work on exploiting the
missing patterns for effective imputation and improving prediction performance.
In this paper, we develop novel deep learning models, namely GRU-D, as one of
the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a
state-of-the-art recurrent neural network. It takes two representations of
missing patterns, i.e., masking and time interval, and effectively incorporates
them into a deep model architecture so that it not only captures the long-term
temporal dependencies in time series, but also utilizes the missing patterns to
achieve better prediction results. Experiments of time series classification
tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic
datasets demonstrate that our models achieve state-of-the-art performance and
provides useful insights for better understanding and utilization of missing
values in time series analysis.",1270,74
['cs.CV'],Kernel Agnostic Real-world Image Super-resolution,"Recently, deep neural network models have achieved impressive results in
various research fields. Come with it, an increasing number of attentions have
been attracted by deep super-resolution (SR) approaches. Many existing methods
attempt to restore high-resolution images from directly down-sampled
low-resolution images or with the assumption of Gaussian degradation kernels
with additive noises for their simplicities. However, in real-world scenarios,
highly complex kernels and non-additive noises may be involved, even though the
distorted images are visually similar to the clear ones. Existing SR models are
facing difficulties to deal with real-world images under such circumstances. In
this paper, we introduce a new kernel agnostic SR framework to deal with
real-world image SR problem. The framework can be hanged seamlessly to multiple
mainstream models. In the proposed framework, the degradation kernels and
noises are adaptively modeled rather than explicitly specified. Moreover, we
also propose an iterative supervision process and frequency-attended objective
from orthogonal perspectives to further boost the performance. The experiments
validate the effectiveness of the proposed framework on multiple real-world
datasets.",1243,49
"['cs.CV', 'eess.IV']",LASSR: Effective Super-Resolution Method for Plant Disease Diagnosis,"The collection of high-resolution training data is crucial in building robust
plant disease diagnosis systems, since such data have a significant impact on
diagnostic performance. However, they are very difficult to obtain and are not
always available in practice. Deep learning-based techniques, and particularly
generative adversarial networks (GANs), can be applied to generate high-quality
super-resolution images, but these methods often produce unexpected artifacts
that can lower the diagnostic performance. In this paper, we propose a novel
artifact-suppression super-resolution method that is specifically designed for
diagnosing leaf disease, called Leaf Artifact-Suppression Super Resolution
(LASSR). Thanks to its own artifact removal module that detects and suppresses
artifacts to a considerable extent, LASSR can generate much more pleasing,
high-quality images compared to the state-of-the-art ESRGAN model. Experiments
based on a five-class cucumber disease (including healthy) discrimination model
show that training with data generated by LASSR significantly boosts the
performance on an unseen test dataset by nearly 22% compared with the baseline,
and that our approach is more than 2% better than a model trained with images
generated by ESRGAN.",1267,68
"['cs.CV', 'cs.AI', 'cs.LG']",Scaling and Benchmarking Self-Supervised Visual Representation Learning,"Self-supervised learning aims to learn representations from the data itself
without explicit manual supervision. Existing efforts ignore a crucial aspect
of self-supervised learning - the ability to scale to large amount of data
because self-supervision requires no manual labels. In this work, we revisit
this principle and scale two popular self-supervised approaches to 100 million
images. We show that by scaling on various axes (including data size and
problem 'hardness'), one can largely match or even exceed the performance of
supervised pre-training on a variety of tasks such as object detection, surface
normal estimation (3D) and visual navigation using reinforcement learning.
Scaling these methods also provides many interesting insights into the
limitations of current self-supervised techniques and evaluations. We conclude
that current self-supervised methods are not 'hard' enough to take full
advantage of large scale data and do not seem to learn effective high level
semantic representations. We also introduce an extensive benchmark across 9
different datasets and tasks. We believe that such a benchmark along with
comparable evaluation settings is necessary to make meaningful progress. Code
is at: https://github.com/facebookresearch/fair_self_supervision_benchmark.",1291,71
"['cs.LG', 'cs.AI', 'cs.RO']",Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations,"Dexterous multi-fingered hands are extremely versatile and provide a generic
way to perform a multitude of tasks in human-centric environments. However,
effectively controlling them remains challenging due to their high
dimensionality and large number of potential contacts. Deep reinforcement
learning (DRL) provides a model-agnostic approach to control complex dynamical
systems, but has not been shown to scale to high-dimensional dexterous
manipulation. Furthermore, deployment of DRL on physical systems remains
challenging due to sample inefficiency. Consequently, the success of DRL in
robotics has thus far been limited to simpler manipulators and tasks. In this
work, we show that model-free DRL can effectively scale up to complex
manipulation tasks with a high-dimensional 24-DoF hand, and solve them from
scratch in simulated experiments. Furthermore, with the use of a small number
of human demonstrations, the sample complexity can be significantly reduced,
which enables learning with sample sizes equivalent to a few hours of robot
experience. The use of demonstrations result in policies that exhibit very
natural movements and, surprisingly, are also substantially more robust.",1195,91
"['cs.LG', 'cs.CR', 'cs.IT', 'math.IT', 'stat.ML']",Differentially Private Distributed Data Summarization under Covariate Shift,"We envision AI marketplaces to be platforms where consumers, with very less
data for a target task, can obtain a relevant model by accessing many private
data sources with vast number of data samples. One of the key challenges is to
construct a training dataset that matches a target task without compromising on
privacy of the data sources. To this end, we consider the following distributed
data summarizataion problem. Given K private source datasets denoted by
$[D_i]_{i\in [K]}$ and a small target validation set $D_v$, which may involve a
considerable covariate shift with respect to the sources, compute a summary
dataset $D_s\subseteq \bigcup_{i\in [K]} D_i$ such that its statistical
distance from the validation dataset $D_v$ is minimized. We use the popular
Maximum Mean Discrepancy as the measure of statistical distance. The
non-private problem has received considerable attention in prior art, for
example in prototype selection (Kim et al., NIPS 2016). Our work is the first
to obtain strong differential privacy guarantees while ensuring the quality
guarantees of the non-private version. We study this problem in a Parsimonious
Curator Privacy Model, where a trusted curator coordinates the summarization
process while minimizing the amount of private information accessed. Our
central result is a novel protocol that (a) ensures the curator accesses at
most $O(K^{\frac{1}{3}}|D_s| + |D_v|)$ points (b) has formal privacy guarantees
on the leakage of information between the data owners and (c) closely matches
the best known non-private greedy algorithm. Our protocol uses two hash
functions, one inspired by the Rahimi-Recht random features method and the
second leverages state of the art differential privacy mechanisms. We introduce
a novel ""noiseless"" differentially private auctioning protocol for winner
notification and demonstrate the efficacy of our protocol using real-world
datasets.",1914,75
"['cs.CV', 'cs.RO']",Dataset and Benchmarking of Real-Time Embedded Object Detection for RoboCup SSL,"When producing a model to object detection in a specific context, the first
obstacle is to have a dataset labeling the desired classes. In RoboCup, some
leagues already have more than one dataset to train and evaluate a model.
However, in the Small Size League (SSL), there is not such dataset available
yet. This paper presents an open-source dataset to be used as a benchmark for
real-time object detection in SSL. This work also presented a pipeline to
train, deploy, and evaluate Convolutional Neural Networks (CNNs) models in a
low-power embedded system. This pipeline was used to evaluate the proposed
dataset with state-of-art optimized models. In this dataset, the MobileNet SSD
v1 achieves 44.88% AP (68.81% AP50) at 94 Frames Per Second (FPS) while running
on an SSL robot.",783,79
"['cs.LG', 'stat.ML']",Representation Learning with Autoencoders for Electronic Health Records: A Comparative Study,"Increasing volume of Electronic Health Records (EHR) in recent years provides
great opportunities for data scientists to collaborate on different aspects of
healthcare research by applying advanced analytics to these EHR clinical data.
A key requirement however is obtaining meaningful insights from high
dimensional, sparse and complex clinical data. Data science approaches
typically address this challenge by performing feature learning in order to
build more reliable and informative feature representations from clinical data
followed by supervised learning. In this paper, we propose a predictive
modeling approach based on deep learning based feature representations and word
embedding techniques. Our method uses different deep architectures (stacked
sparse autoencoders, deep belief network, adversarial autoencoders and
variational autoencoders) for feature representation in higher-level
abstraction to obtain effective and robust features from EHRs, and then build
prediction models on top of them. Our approach is particularly useful when the
unlabeled data is abundant whereas labeled data is scarce. We investigate the
performance of representation learning through a supervised learning approach.
Our focus is to present a comparative study to evaluate the performance of
different deep architectures through supervised learning and provide insights
in the choice of deep feature representation techniques. Our experiments
demonstrate that for small data sets, stacked sparse autoencoder demonstrates a
superior generality performance in prediction due to sparsity regularization
whereas variational autoencoders outperform the competing approaches for large
data sets due to its capability of learning the representation distribution.",1751,92
"['cs.LG', 'cs.DB', 'stat.ML']",Forecasting in multivariate irregularly sampled time series with missing values,"Sparse and irregularly sampled multivariate time series are common in
clinical, climate, financial and many other domains. Most recent approaches
focus on classification, regression or forecasting tasks on such data. In
forecasting, it is necessary to not only forecast the right value but also to
forecast when that value will occur in the irregular time series. In this work,
we present an approach to forecast not only the values but also the time at
which they are expected to occur.",487,79
"['cs.LG', 'cs.AI', 'stat.ML']",Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning,"Designing agents that acquire knowledge autonomously and use it to solve new
tasks efficiently is an important challenge in reinforcement learning.
Knowledge acquired during an unsupervised pre-training phase is often
transferred by fine-tuning neural network weights once rewards are exposed, as
is common practice in supervised domains. Given the nature of the reinforcement
learning problem, we argue that standard fine-tuning strategies alone are not
enough for efficient transfer in challenging domains. We introduce Behavior
Transfer (BT), a technique that leverages pre-trained policies for exploration
and that is complementary to transferring neural network weights. Our
experiments show that, when combined with large-scale pre-training in the
absence of rewards, existing intrinsic motivation objectives can lead to the
emergence of complex behaviors. These pre-trained policies can then be
leveraged by BT to discover better solutions than without pre-training, and
combining BT with standard fine-tuning strategies results in additional
benefits. The largest gains are generally observed in domains requiring
structured exploration, including settings where the behavior of the
pre-trained policies is misaligned with the downstream task.",1251,67
"['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'cs.RO']",Learning Inner-Group Relations on Point Clouds,"The prevalence of relation networks in computer vision is in stark contrast
to underexplored point-based methods. In this paper, we explore the
possibilities of local relation operators and survey their feasibility. We
propose a scalable and efficient module, called group relation aggregator. The
module computes a feature of a group based on the aggregation of the features
of the inner-group points weighted by geometric relations and semantic
relations. We adopt this module to design our RPNet. We further verify the
expandability of RPNet, in terms of both depth and width, on the tasks of
classification and segmentation. Surprisingly, empirical results show that
wider RPNet fits for classification, while deeper RPNet works better on
segmentation. RPNet achieves state-of-the-art for classification and
segmentation on challenging benchmarks. We also compare our local aggregator
with PointNet++, with around 30% parameters and 50% computation saving.
Finally, we conduct experiments to reveal the robustness of RPNet with regard
to rigid transformation and noises.",1074,46
"['cs.LG', 'cs.CV', 'stat.ML']",Self-Supervised GANs via Auxiliary Rotation Loss,"Conditional GANs are at the forefront of natural image synthesis. The main
drawback of such models is the necessity for labeled data. In this work we
exploit two popular unsupervised learning techniques, adversarial training and
self-supervision, and take a step towards bridging the gap between conditional
and unconditional GANs. In particular, we allow the networks to collaborate on
the task of representation learning, while being adversarial with respect to
the classic GAN game. The role of self-supervision is to encourage the
discriminator to learn meaningful feature representations which are not
forgotten during training. We test empirically both the quality of the learned
image representations, and the quality of the synthesized images. Under the
same conditions, the self-supervised GAN attains a similar performance to
state-of-the-art conditional counterparts. Finally, we show that this approach
to fully unsupervised learning can be scaled to attain an FID of 23.4 on
unconditional ImageNet generation.",1022,48
"['cs.LG', 'cs.CV', 'stat.ML']",Fast nonparametric clustering of structured time-series,"In this publication, we combine two Bayesian non-parametric models: the
Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP
model is to introduce a variation on the GP prior which enables us to model
structured time-series data, i.e. data containing groups where we wish to model
inter- and intra-group variability. Our innovation in the DP model is an
implementation of a new fast collapsed variational inference procedure which
enables us to optimize our variationala pproximation significantly faster than
standard VB approaches. In a biological time series application we show how our
model better captures salient features of the data, leading to better
consistency with existing biological classifications, while the associated
inference algorithm provides a twofold speed-up over EM-based variational
inference.",845,55
"['cs.LG', 'stat.ML']",NeuPDE: Neural Network Based Ordinary and Partial Differential Equations for Modeling Time-Dependent Data,"We propose a neural network based approach for extracting models from dynamic
data using ordinary and partial differential equations. In particular, given a
time-series or spatio-temporal dataset, we seek to identify an accurate
governing system which respects the intrinsic differential structure. The
unknown governing model is parameterized by using both (shallow) multilayer
perceptrons and nonlinear differential terms, in order to incorporate relevant
correlations between spatio-temporal samples. We demonstrate the approach on
several examples where the data is sampled from various dynamical systems and
give a comparison to recurrent networks and other data-discovery methods. In
addition, we show that for MNIST and Fashion MNIST, our approach lowers the
parameter cost as compared to other deep neural networks.",823,105
['cs.CV'],Unsupervised Activity Segmentation by Joint Representation Learning and Online Clustering,"We present a novel approach for unsupervised activity segmentation, which
uses video frame clustering as a pretext task and simultaneously performs
representation learning and online clustering. This is in contrast with prior
works where representation learning and clustering are often performed
sequentially. We leverage temporal information in videos by employing temporal
optimal transport and temporal coherence loss. In particular, we incorporate a
temporal regularization term into the standard optimal transport module, which
preserves the temporal order of the activity, yielding the temporal optimal
transport module for computing pseudo-label cluster assignments. Next, the
temporal coherence loss encourages neighboring video frames to be mapped to
nearby points while distant video frames are mapped to farther away points in
the embedding space. The combination of these two components results in
effective representations for unsupervised activity segmentation. Furthermore,
previous methods require storing learned features for the entire dataset before
clustering them in an offline manner, whereas our approach processes one
mini-batch at a time in an online manner. Extensive evaluations on three public
datasets, i.e. 50-Salads, YouTube Instructions, and Breakfast, and our dataset,
i.e., Desktop Assembly, show that our approach performs on par or better than
previous methods for unsupervised activity segmentation, despite having
significantly less memory constraints.",1491,89
"['stat.ML', 'cs.LG']",Faithful Inversion of Generative Models for Effective Amortized Inference,"Inference amortization methods share information across multiple
posterior-inference problems, allowing each to be carried out more efficiently.
Generally, they require the inversion of the dependency structure in the
generative model, as the modeller must learn a mapping from observations to
distributions approximating the posterior. Previous approaches have involved
inverting the dependency structure in a heuristic way that fails to capture
these dependencies correctly, thereby limiting the achievable accuracy of the
resulting approximations. We introduce an algorithm for faithfully, and
minimally, inverting the graphical model structure of any generative model.
Such inverses have two crucial properties: (a) they do not encode any
independence assertions that are absent from the model and; (b) they are local
maxima for the number of true independencies encoded. We prove the correctness
of our approach and empirically show that the resulting minimally faithful
inverses lead to better inference amortization than existing heuristic
approaches.",1058,73
"['cs.CV', 'cs.LG']",AutoTune: Automatically Tuning Convolutional Neural Networks for Improved Transfer Learning,"Transfer learning enables solving a specific task having limited data by
using the pre-trained deep networks trained on large-scale datasets. Typically,
while transferring the learned knowledge from source task to the target task,
the last few layers are fine-tuned (re-trained) over the target dataset.
However, these layers are originally designed for the source task that might
not be suitable for the target task. In this paper, we introduce a mechanism
for automatically tuning the Convolutional Neural Networks (CNN) for improved
transfer learning. The pre-trained CNN layers are tuned with the knowledge from
target data using Bayesian Optimization. First, we train the final layer of the
base CNN model by replacing the number of neurons in the softmax layer with the
number of classes involved in the target task. Next, the pre-trained CNN is
tuned automatically by observing the classification performance on the
validation data (greedy criteria). To evaluate the performance of the proposed
method, experiments are conducted on three benchmark datasets, e.g.,
CalTech-101, CalTech-256, and Stanford Dogs. The classification results
obtained through the proposed AutoTune method outperforms the standard baseline
transfer learning methods over the three datasets by achieving $95.92\%$,
$86.54\%$, and $84.67\%$ accuracy over CalTech-101, CalTech-256, and Stanford
Dogs, respectively. The experimental results obtained in this study depict that
tuning of the pre-trained CNN layers with the knowledge from the target dataset
confesses better transfer learning ability. The source codes are available at
https://github.com/JekyllAndHyde8999/AutoTune_CNN_TransferLearning.",1680,91
"['cs.LG', 'cs.DM', 'math.CO', 'stat.ML']",Graph Homomorphism Convolution,"In this paper, we study the graph classification problem from the graph
homomorphism perspective. We consider the homomorphisms from $F$ to $G$, where
$G$ is a graph of interest (e.g. molecules or social networks) and $F$ belongs
to some family of graphs (e.g. paths or non-isomorphic trees). We show that
graph homomorphism numbers provide a natural invariant (isomorphism invariant
and $\mathcal{F}$-invariant) embedding maps which can be used for graph
classification. Viewing the expressive power of a graph classifier by the
$\mathcal{F}$-indistinguishable concept, we prove the universality property of
graph homomorphism vectors in approximating $\mathcal{F}$-invariant functions.
In practice, by choosing $\mathcal{F}$ whose elements have bounded tree-width,
we show that the homomorphism method is efficient compared with other methods.",845,30
"['cs.LG', 'cs.AI', 'eess.SP']",Short-term Traffic Prediction with Deep Neural Networks: A Survey,"In modern transportation systems, an enormous amount of traffic data is
generated every day. This has led to rapid progress in short-term traffic
prediction (STTP), in which deep learning methods have recently been applied.
In traffic networks with complex spatiotemporal relationships, deep neural
networks (DNNs) often perform well because they are capable of automatically
extracting the most important features and patterns. In this study, we survey
recent STTP studies applying deep networks from four perspectives. 1) We
summarize input data representation methods according to the number and type of
spatial and temporal dependencies involved. 2) We briefly explain a wide range
of DNN techniques from the earliest networks, including Restricted Boltzmann
Machines, to the most recent, including graph-based and meta-learning networks.
3) We summarize previous STTP studies in terms of the type of DNN techniques,
application area, dataset and code availability, and the type of the
represented spatiotemporal dependencies. 4) We compile public traffic datasets
that are popular and can be used as the standard benchmarks. Finally, we
suggest challenging issues and possible future research directions in STTP.",1217,65
['cs.LG'],Anomaly Detection on IT Operation Series via Online Matrix Profile,"Anomaly detection on time series is a fundamental task in monitoring the Key
Performance Indicators (KPIs) of IT systems. Many of the existing approaches in
the literature show good performance while requiring a lot of training
resources. In this paper, the online matrix profile, which requires no
training, is proposed to address this issue. The anomalies are detected by
referring to the past subsequence that is the closest to the current one. The
distance significance is introduced based on the online matrix profile, which
demonstrates a prominent pattern when an anomaly occurs. Another training-free
approach spectral residual is integrated into our approach to further enhance
the detection accuracy. Moreover, the proposed approach is sped up by at least
four times for long time series by the introduced cache strategy. In comparison
to the existing approaches, the online matrix profile makes a good trade-off
between accuracy and efficiency. More importantly, it is generic to various
types of time series in the sense that it works without the constraint from any
trained model.",1093,66
"['stat.ML', 'cs.LG', 'eess.SP', 'math.RT']",Signal Processing on the Permutahedron: Tight Spectral Frames for Ranked Data Analysis,"Ranked data sets, where m judges/voters specify a preference ranking of n
objects/candidates, are increasingly prevalent in contexts such as political
elections, computer vision, recommender systems, and bioinformatics. The vote
counts for each ranking can be viewed as an n! data vector lying on the
permutahedron, which is a Cayley graph of the symmetric group with vertices
labeled by permutations and an edge when two permutations differ by an adjacent
transposition. Leveraging combinatorial representation theory and recent
progress in signal processing on graphs, we investigate a novel, scalable
transform method to interpret and exploit structure in ranked data. We
represent data on the permutahedron using an overcomplete dictionary of atoms,
each of which captures both smoothness information about the data (typically
the focus of spectral graph decomposition methods in graph signal processing)
and structural information about the data (typically the focus of symmetry
decomposition methods from representation theory). These atoms have a more
naturally interpretable structure than any known basis for signals on the
permutahedron, and they form a Parseval frame, ensuring beneficial numerical
properties such as energy preservation. We develop specialized algorithms and
open software that take advantage of the symmetry and structure of the
permutahedron to improve the scalability of the proposed method, making it more
applicable to the high-dimensional ranked data found in applications.",1508,86
['cs.CV'],supervised adptive threshold network for instance segmentation,"Currently, instance segmentation is attracting more and more attention in
machine learning region. However, there exists some defects on the information
propagation in previous Mask R-CNN and other network models. In this paper, we
propose supervised adaptive threshold network for instance segmentation.
Specifically, we adopt the Mask R-CNN method based on adaptive threshold, and
by establishing a layered adaptive network structure, it performs adaptive
binarization on the probability graph generated by Mask RCNN to obtain better
segmentation effect and reduce the error rate. At the same time, an adaptive
feature pool is designed to make the transmission between different layers of
the network more accurate and effective, reduce the loss in the process of
feature transmission, and further improve the mask method. Experiments on
benchmark data sets indicate that the effectiveness of the proposed model",913,62
"['cs.LG', 'stat.ML']",On $\ell_p$-norm Robustness of Ensemble Stumps and Trees,"Recent papers have demonstrated that ensemble stumps and trees could be
vulnerable to small input perturbations, so robustness verification and defense
for those models have become an important research problem. However, due to the
structure of decision trees, where each node makes decision purely based on one
feature value, all the previous works only consider the $\ell_\infty$ norm
perturbation. To study robustness with respect to a general $\ell_p$ norm
perturbation, one has to consider the correlation between perturbations on
different features, which has not been handled by previous algorithms. In this
paper, we study the problem of robustness verification and certified defense
with respect to general $\ell_p$ norm perturbations for ensemble decision
stumps and trees. For robustness verification of ensemble stumps, we prove that
complete verification is NP-complete for $p\in(0, \infty)$ while polynomial
time algorithms exist for $p=0$ or $\infty$. For $p\in(0, \infty)$ we develop
an efficient dynamic programming based algorithm for sound verification of
ensemble stumps. For ensemble trees, we generalize the previous multi-level
robustness verification algorithm to $\ell_p$ norm. We demonstrate the first
certified defense method for training ensemble stumps and trees with respect to
$\ell_p$ norm perturbations, and verify its effectiveness empirically on real
datasets.",1395,56
"['cs.LG', 'eess.SP', 'stat.ML']",Classification of EEG Signal based on non-Gaussian Neutral Vector,"In the design of brain-computer interface systems, classification of
Electroencephalogram (EEG) signals is the essential part and a challenging
task. Recently, as the marginalized discrete wavelet transform (mDWT)
representations can reveal features related to the transient nature of the EEG
signals, the mDWT coefficients have been frequently used in EEG signal
classification. In our previous work, we have proposed a super-Dirichlet
distribution-based classifier, which utilized the nonnegative and sum-to-one
properties of the mDWT coefficients. The proposed classifier performed better
than the state-of-the-art support vector machine-based classifier. In this
paper, we further study the neutrality of the mDWT coefficients. Assuming the
mDWT vector coefficients to be a neutral vector, we transform them non-linearly
into a set of independent scalar coefficients. Feature selection strategy is
proposed on the transformed feature domain. Experimental results show that the
feature selection strategy helps improving the classification accuracy.",1052,65
['cs.CV'],TENet: Triple Excitation Network for Video Salient Object Detection,"In this paper, we propose a simple yet effective approach, named Triple
Excitation Network, to reinforce the training of video salient object detection
(VSOD) from three aspects, spatial, temporal, and online excitations. These
excitation mechanisms are designed following the spirit of curriculum learning
and aim to reduce learning ambiguities at the beginning of training by
selectively exciting feature activations using ground truth. Then we gradually
reduce the weight of ground truth excitations by a curriculum rate and replace
it by a curriculum complementary map for better and faster convergence. In
particular, the spatial excitation strengthens feature activations for clear
object boundaries, while the temporal excitation imposes motions to emphasize
spatio-temporal salient regions. Spatial and temporal excitations can combat
the saliency shifting problem and conflict between spatial and temporal
features of VSOD. Furthermore, our semi-curriculum learning design enables the
first online refinement strategy for VSOD, which allows exciting and boosting
saliency responses during testing without re-training. The proposed triple
excitations can easily plug in different VSOD methods. Extensive experiments
show the effectiveness of all three excitation methods and the proposed method
outperforms state-of-the-art image and video salient object detection methods.",1381,67
"['cs.LG', 'cs.IT', 'cs.RO', 'math.IT', 'stat.ML']",Non-Adversarial Imitation Learning and its Connections to Adversarial Methods,"Many modern methods for imitation learning and inverse reinforcement
learning, such as GAIL or AIRL, are based on an adversarial formulation. These
methods apply GANs to match the expert's distribution over states and actions
with the implicit state-action distribution induced by the agent's policy.
However, by framing imitation learning as a saddle point problem, adversarial
methods can suffer from unstable optimization, and convergence can only be
shown for small policy updates. We address these problems by proposing a
framework for non-adversarial imitation learning. The resulting algorithms are
similar to their adversarial counterparts and, thus, provide insights for
adversarial imitation learning methods. Most notably, we show that AIRL is an
instance of our non-adversarial formulation, which enables us to greatly
simplify its derivations and obtain stronger convergence guarantees. We also
show that our non-adversarial formulation can be used to derive novel
algorithms by presenting a method for offline imitation learning that is
inspired by the recent ValueDice algorithm, but does not rely on small policy
updates for convergence. In our simulated robot experiments, our offline method
for non-adversarial imitation learning seems to perform best when using many
updates for policy and discriminator at each iteration and outperforms
behavioral cloning and ValueDice.",1390,77
['cs.CV'],Nonsmooth Analysis and Subgradient Methods for Averaging in Dynamic Time Warping Spaces,"Time series averaging in dynamic time warping (DTW) spaces has been
successfully applied to improve pattern recognition systems. This article
proposes and analyzes subgradient methods for the problem of finding a sample
mean in DTW spaces. The class of subgradient methods generalizes existing
sample mean algorithms such as DTW Barycenter Averaging (DBA). We show that DBA
is a majorize-minimize algorithm that converges to necessary conditions of
optimality after finitely many iterations. Empirical results show that for
increasing sample sizes the proposed stochastic subgradient (SSG) algorithm is
more stable and finds better solutions in shorter time than the DBA algorithm
on average. Therefore, SSG is useful in online settings and for non-small
sample sizes. The theoretical and empirical results open new paths for devising
sample mean algorithms: nonsmooth optimization methods and modified variants of
pairwise averaging methods.",942,87
['cs.CV'],T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks,"Current methods for single-image depth estimation use training datasets with
real image-depth pairs or stereo pairs, which are not easy to acquire. We
propose a framework, trained on synthetic image-depth pairs and unpaired real
images, that comprises an image translation network for enhancing realism of
input images, followed by a depth prediction network. A key idea is having the
first network act as a wide-spectrum input translator, taking in either
synthetic or real images, and ideally producing minimally modified realistic
images. This is done via a reconstruction loss when the training input is real,
and GAN loss when synthetic, removing the need for heuristic
self-regularization. The second network is trained on a task loss for synthetic
image-depth pairs, with extra GAN loss to unify real and synthetic feature
distributions. Importantly, the framework can be trained end-to-end, leading to
good results, even surpassing early deep-learning methods that use real paired
data.",994,89
['cs.CV'],Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers,"Most polyp segmentation methods use CNNs as their backbone, leading to two
key issues when exchanging information between the encoder and decoder: 1)
taking into account the differences in contribution between different-level
features; and 2) designing effective mechanism for fusing these features.
Different from existing CNN-based methods, we adopt a transformer encoder,
which learns more powerful and robust representations. In addition, considering
the image acquisition influence and elusive properties of polyps, we introduce
three novel modules, including a cascaded fusion module (CFM), a camouflage
identification module (CIM), a and similarity aggregation module (SAM). Among
these, the CFM is used to collect the semantic and location information of
polyps from high-level features, while the CIM is applied to capture polyp
information disguised in low-level features. With the help of the SAM, we
extend the pixel features of the polyp area with high-level semantic position
information to the entire polyp area, thereby effectively fusing cross-level
features. The proposed model, named \ourmodel, effectively suppresses noises in
the features and significantly improves their expressive capabilities.
Extensive experiments on five widely adopted datasets show that the proposed
model is more robust to various challenging situations (e.g., appearance
changes, small objects) than existing methods, and achieves the new
state-of-the-art performance. The proposed model is available at
https://github.com/DengPingFan/Polyp-PVT .",1543,62
['cs.CV'],Geometry-Aware Self-Training for Unsupervised Domain Adaptationon Object Point Clouds,"The point cloud representation of an object can have a large geometric
variation in view of inconsistent data acquisition procedure, which thus leads
to domain discrepancy due to diverse and uncontrollable shape representation
cross datasets. To improve discrimination on unseen distribution of point-based
geometries in a practical and feasible perspective, this paper proposes a new
method of geometry-aware self-training (GAST) for unsupervised domain
adaptation of object point cloud classification. Specifically, this paper aims
to learn a domain-shared representation of semantic categories, via two novel
self-supervised geometric learning tasks as feature regularization. On one
hand, the representation learning is empowered by a linear mixup of point cloud
samples with their self-generated rotation labels, to capture a global
topological configuration of local geometries. On the other hand, a diverse
point distribution across datasets can be normalized with a novel
curvature-aware distortion localization. Experiments on the PointDA-10 dataset
show that our GAST method can significantly outperform the state-of-the-art
methods.",1143,85
"['cs.LG', 'cs.IR', 'stat.ML']",Selective Transfer Learning for Cross Domain Recommendation,"Collaborative filtering (CF) aims to predict users' ratings on items
according to historical user-item preference data. In many real-world
applications, preference data are usually sparse, which would make models
overfit and fail to give accurate predictions. Recently, several research works
show that by transferring knowledge from some manually selected source domains,
the data sparseness problem could be mitigated. However for most cases, parts
of source domain data are not consistent with the observations in the target
domain, which may misguide the target domain model building. In this paper, we
propose a novel criterion based on empirical prediction error and its variance
to better capture the consistency across domains in CF settings. Consequently,
we embed this criterion into a boosting framework to perform selective
knowledge transfer. Comparing to several state-of-the-art methods, we show that
our proposed selective transfer learning framework can significantly improve
the accuracy of rating prediction tasks on several real-world recommendation
tasks.",1076,59
"['cs.LG', 'cs.HC']",Deep Reinforcement Learning for Resource Allocation in Business Processes,"Assigning resources in business processes execution is a repetitive task that
can be effectively automated. However, different automation methods may give
varying results that may not be optimal. Proper resource allocation is crucial
as it may lead to significant cost reductions or increased effectiveness that
results in increased revenues.
  In this work, we first propose a novel representation that allows modeling of
a multi-process environment with different process-based rewards. These
processes can share resources that differ in their eligibility. Then, we use
double deep reinforcement learning to look for optimal resource allocation
policy. We compare those results with two popular strategies that are widely
used in the industry. Learning optimal policy through reinforcement learning
requires frequent interactions with the environment, so we also designed and
developed a simulation engine that can mimic real-world processes.
  The results obtained are promising. Deep reinforcement learning based
resource allocation achieved significantly better results compared to two
commonly used techniques.",1116,73
"['cs.CV', 'cs.RO']",Point-cloud-based place recognition using CNN feature extraction,"This paper proposes a novel point-cloud-based place recognition system that
adopts a deep learning approach for feature extraction. By using a
convolutional neural network pre-trained on color images to extract features
from a range image without fine-tuning on extra range images, significant
improvement has been observed when compared to using hand-crafted features. The
resulting system is illumination invariant, rotation invariant and robust
against moving objects that are unrelated to the place identity. Apart from the
system itself, we also bring to the community a new place recognition dataset
containing both point cloud and grayscale images covering a full $360^\circ$
environmental view. In addition, the dataset is organized in such a way that it
facilitates experimental validation with respect to rotation invariance or
robustness against unrelated moving objects separately.",893,64
"['cs.LG', 'stat.ML']",Fast classification using sparse decision DAGs,"In this paper we propose an algorithm that builds sparse decision DAGs
(directed acyclic graphs) from a list of base classifiers provided by an
external learning method such as AdaBoost. The basic idea is to cast the DAG
design task as a Markov decision process. Each instance can decide to use or to
skip each base classifier, based on the current state of the classifier being
built. The result is a sparse decision DAG where the base classifiers are
selected in a data-dependent way. The method has a single hyperparameter with a
clear semantics of controlling the accuracy/speed trade-off. The algorithm is
competitive with state-of-the-art cascade detectors on three object-detection
benchmarks, and it clearly outperforms them when there is a small number of
base classifiers. Unlike cascades, it is also readily applicable for
multi-class classification. Using the multi-class setup, we show on a benchmark
web page ranking data set that we can significantly improve the decision speed
without harming the performance of the ranker.",1039,46
"['cs.CV', 'cs.AI', 'cs.LG']",Unsupervised Depth Completion with Calibrated Backprojection Layers,"We propose a deep neural network architecture to infer dense depth from an
image and a sparse point cloud. It is trained using a video stream and
corresponding synchronized sparse point cloud, as obtained from a LIDAR or
other range sensor, along with the intrinsic calibration parameters of the
camera. At inference time, the calibration of the camera, which can be
different than the one used for training, is fed as an input to the network
along with the sparse point cloud and a single image. A Calibrated
Backprojection Layer backprojects each pixel in the image to three-dimensional
space using the calibration matrix and a depth feature descriptor. The
resulting 3D positional encoding is concatenated with the image descriptor and
the previous layer output to yield the input to the next layer of the encoder.
A decoder, exploiting skip-connections, produces a dense depth map. The
resulting Calibrated Backprojection Network, or KBNet, is trained without
supervision by minimizing the photometric reprojection error. KBNet imputes
missing depth value based on the training set, rather than on generic
regularization. We test KBNet on public depth completion benchmarks, where it
outperforms the state of the art by 30% indoor and 8% outdoor when the same
camera is used for training and testing. When the test camera is different, the
improvement reaches 62%. Code available at:
https://github.com/alexklwong/calibrated-backprojection-network.",1452,67
"['cs.LG', 'cs.CY', 'stat.AP', 'stat.ML']",Explaining an increase in predicted risk for clinical alerts,"Much work aims to explain a model's prediction on a static input. We consider
explanations in a temporal setting where a stateful dynamical model produces a
sequence of risk estimates given an input at each time step. When the estimated
risk increases, the goal of the explanation is to attribute the increase to a
few relevant inputs from the past. While our formal setup and techniques are
general, we carry out an in-depth case study in a clinical setting. The goal
here is to alert a clinician when a patient's risk of deterioration rises. The
clinician then has to decide whether to intervene and adjust the treatment.
Given a potentially long sequence of new events since she last saw the patient,
a concise explanation helps her to quickly triage the alert. We develop methods
to lift static attribution techniques to the dynamical setting, where we
identify and address challenges specific to dynamics. We then experimentally
assess the utility of different explanations of clinical alerts through expert
evaluation.",1024,60
['cs.CV'],Scribble-Supervised Semantic Segmentation by Uncertainty Reduction on Neural Representation and Self-Supervision on Neural Eigenspace,"Scribble-supervised semantic segmentation has gained much attention recently
for its promising performance without high-quality annotations. Due to the lack
of supervision, confident and consistent predictions are usually hard to
obtain. Typically, people handle these problems to either adopt an auxiliary
task with the well-labeled dataset or incorporate the graphical model with
additional requirements on scribble annotations. Instead, this work aims to
achieve semantic segmentation by scribble annotations directly without extra
information and other limitations. Specifically, we propose holistic
operations, including minimizing entropy and a network embedded random walk on
neural representation to reduce uncertainty. Given the probabilistic transition
matrix of a random walk, we further train the network with self-supervision on
its neural eigenspace to impose consistency on predictions between related
images. Comprehensive experiments and ablation studies verify the proposed
approach, which demonstrates superiority over others; it is even comparable to
some full-label supervised ones and works well when scribbles are randomly
shrunk or dropped.",1164,133
['cs.CV'],Frame-to-Frame Aggregation of Active Regions in Web Videos for Weakly Supervised Semantic Segmentation,"When a deep neural network is trained on data with only image-level labeling,
the regions activated in each image tend to identify only a small region of the
target object. We propose a method of using videos automatically harvested from
the web to identify a larger region of the target object by using temporal
information, which is not present in the static image. The temporal variations
in a video allow different regions of the target object to be activated. We
obtain an activated region in each frame of a video, and then aggregate the
regions from successive frames into a single image, using a warping technique
based on optical flow. The resulting localization maps cover more of the target
object, and can then be used as proxy ground-truth to train a segmentation
network. This simple approach outperforms existing methods under the same level
of supervision, and even approaches relying on extra annotations. Based on
VGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4,
respectively, on PASCAL VOC 2012 test images, which represents a new
state-of-the-art.",1098,102
['cs.CV'],"Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis","Deep person generation has attracted extensive research attention due to its
wide applications in virtual agents, video conferencing, online shopping and
art/movie production. With the advancement of deep learning, visual appearances
(face, pose, cloth) of a person image can be easily generated or manipulated on
demand. In this survey, we first summarize the scope of person generation, and
then systematically review recent progress and technical trends in deep person
generation, covering three major tasks: talking-head generation (face),
pose-guided person generation (pose) and garment-oriented person generation
(cloth). More than two hundred papers are covered for a thorough overview, and
the milestone works are highlighted to witness the major technical
breakthrough. Based on these fundamental tasks, a number of applications are
investigated, e.g., virtual fitting, digital human, generative data
augmentation. We hope this survey could shed some light on the future prospects
of deep person generation, and provide a helpful foundation for full
applications towards digital human.",1095,87
"['stat.ML', 'cs.LG', 'I.2.6; F.m']",An analytic theory of generalization dynamics and transfer learning in deep linear networks,"Much attention has been devoted recently to the generalization puzzle in deep
learning: large, deep networks can generalize well, but existing theories
bounding generalization error are exceedingly loose, and thus cannot explain
this striking performance. Furthermore, a major hope is that knowledge may
transfer across tasks, so that multi-task learning can improve generalization
on individual tasks. However we lack analytic theories that can quantitatively
predict how the degree of knowledge transfer depends on the relationship
between the tasks. We develop an analytic theory of the nonlinear dynamics of
generalization in deep linear networks, both within and across tasks. In
particular, our theory provides analytic solutions to the training and testing
error of deep networks as a function of training time, number of examples,
network size and initialization, and the task structure and SNR. Our theory
reveals that deep networks progressively learn the most important task
structure first, so that generalization error at the early stopping time
primarily depends on task structure and is independent of network size. This
suggests any tight bound on generalization error must take into account task
structure, and explains observations about real data being learned faster than
random data. Intriguingly our theory also reveals the existence of a learning
algorithm that proveably out-performs neural network training through gradient
descent. Finally, for transfer learning, our theory reveals that knowledge
transfer depends sensitively, but computably, on the SNRs and input feature
alignments of pairs of tasks.",1629,91
"['cs.CV', 'cs.LG', 'eess.IV']",Image coding for machines: an end-to-end learned approach,"Over recent years, deep learning-based computer vision systems have been
applied to images at an ever-increasing pace, oftentimes representing the only
type of consumption for those images. Given the dramatic explosion in the
number of images generated per day, a question arises: how much better would an
image codec targeting machine-consumption perform against state-of-the-art
codecs targeting human-consumption? In this paper, we propose an image codec
for machines which is neural network (NN) based and end-to-end learned. In
particular, we propose a set of training strategies that address the delicate
problem of balancing competing loss functions, such as computer vision task
losses, image distortion losses, and rate loss. Our experimental results show
that our NN-based codec outperforms the state-of-the-art Versa-tile Video
Coding (VVC) standard on the object detection and instance segmentation tasks,
achieving -37.87% and -32.90% of BD-rate gain, respectively, while being fast
thanks to its compact size. To the best of our knowledge, this is the first
end-to-end learned machine-targeted image codec.",1120,57
"['cs.LG', 'cs.CV']",Practical Assessment of Generalization Performance Robustness for Deep Networks via Contrastive Examples,"Training images with data transformations have been suggested as contrastive
examples to complement the testing set for generalization performance
evaluation of deep neural networks (DNNs). In this work, we propose a practical
framework ContRE (The word ""contre"" means ""against"" or ""versus"" in French.)
that uses Contrastive examples for DNN geneRalization performance Estimation.
Specifically, ContRE follows the assumption in contrastive learning that robust
DNN models with good generalization performance are capable of extracting a
consistent set of features and making consistent predictions from the same
image under varying data transformations. Incorporating with a set of
randomized strategies for well-designed data transformations over the training
set, ContRE adopts classification errors and Fisher ratios on the generated
contrastive examples to assess and analyze the generalization performance of
deep models in complement with a testing set. To show the effectiveness and the
efficiency of ContRE, extensive experiments have been done using various DNN
models on three open source benchmark datasets with thorough ablation studies
and applicability analyses. Our experiment results confirm that (1) behaviors
of deep models on contrastive examples are strongly correlated to what on the
testing set, and (2) ContRE is a robust measure of generalization performance
complementing to the testing set in various settings.",1436,104
['cs.CV'],Accurate Weakly Supervised Deep Lesion Segmentation on CT Scans: Self-Paced 3D Mask Generation from RECIST,"Volumetric lesion segmentation via medical imaging is a powerful means to
precisely assess multiple time-point lesion/tumor changes. Because manual 3D
segmentation is prohibitively time consuming and requires radiological
experience, current practices rely on an imprecise surrogate called response
evaluation criteria in solid tumors (RECIST). Despite their coarseness, RECIST
marks are commonly found in current hospital picture and archiving systems
(PACS), meaning they can provide a potentially powerful, yet extraordinarily
challenging, source of weak supervision for full 3D segmentation. Toward this
end, we introduce a convolutional neural network based weakly supervised
self-paced segmentation (WSSS) method to 1) generate the initial lesion
segmentation on the axial RECIST-slice; 2) learn the data distribution on
RECIST-slices; 3) adapt to segment the whole volume slice by slice to finally
obtain a volumetric segmentation. In addition, we explore how super-resolution
images (2~5 times beyond the physical CT imaging), generated from a proposed
stacked generative adversarial network, can aid the WSSS performance. We employ
the DeepLesion dataset, a comprehensive CT-image lesion dataset of 32,735
PACS-bookmarked findings, which include lesions, tumors, and lymph nodes of
varying sizes, categories, body regions and surrounding contexts. These are
drawn from 10,594 studies of 4,459 patients. We also validate on a lymph-node
dataset, where 3D ground truth masks are available for all images. For the
DeepLesion dataset, we report mean Dice coefficients of 93% on RECIST-slices
and 76% in 3D lesion volumes. We further validate using a subjective user
study, where an experienced radiologist accepted our WSSS-generated lesion
segmentation results with a high probability of 92.4%.",1800,106
"['cs.LG', 'cs.CV', 'stat.ML']",FedVision: An Online Visual Object Detection Platform Powered by Federated Learning,"Visual object detection is a computer vision-based artificial intelligence
(AI) technique which has many practical applications (e.g., fire hazard
monitoring). However, due to privacy concerns and the high cost of transmitting
video data, it is highly challenging to build object detection models on
centrally stored large training datasets following the current approach.
Federated learning (FL) is a promising approach to resolve this challenge.
Nevertheless, there currently lacks an easy to use tool to enable computer
vision application developers who are not experts in federated learning to
conveniently leverage this technology and apply it in their systems. In this
paper, we report FedVision - a machine learning engineering platform to support
the development of federated learning powered computer vision applications. The
platform has been deployed through a collaboration between WeBank and Extreme
Vision to help customers develop computer vision-based safety monitoring
solutions in smart city applications. Over four months of usage, it has
achieved significant efficiency improvement and cost reduction while removing
the need to transmit sensitive data for three major corporate customers. To the
best of our knowledge, this is the first real application of FL in computer
vision-based tasks.",1311,83
['cs.LG'],Traffic Light Control Using Deep Policy-Gradient and Value-Function Based Reinforcement Learning,"Recent advances in combining deep neural network architectures with
reinforcement learning techniques have shown promising potential results in
solving complex control problems with high dimensional state and action spaces.
Inspired by these successes, in this paper, we build two kinds of reinforcement
learning algorithms: deep policy-gradient and value-function based agents which
can predict the best possible traffic signal for a traffic intersection. At
each time step, these adaptive traffic light control agents receive a snapshot
of the current state of a graphical traffic simulator and produce control
signals. The policy-gradient based agent maps its observation directly to the
control signal, however the value-function based agent first estimates values
for all legal control signals. The agent then selects the optimal control
action with the highest value. Our methods show promising results in a traffic
network simulated in the SUMO traffic simulator, without suffering from
instability issues during the training process.",1041,96
"['cs.LG', 'q-bio.BM']",An End-to-End Framework for Molecular Conformation Generation via Bilevel Programming,"Predicting molecular conformations (or 3D structures) from molecular graphs
is a fundamental problem in many applications. Most existing approaches are
usually divided into two steps by first predicting the distances between atoms
and then generating a 3D structure through optimizing a distance geometry
problem. However, the distances predicted with such two-stage approaches may
not be able to consistently preserve the geometry of local atomic
neighborhoods, making the generated structures unsatisfying. In this paper, we
propose an end-to-end solution for molecular conformation prediction called
ConfVAE based on the conditional variational autoencoder framework.
Specifically, the molecular graph is first encoded in a latent space, and then
the 3D structures are generated by solving a principled bilevel optimization
program. Extensive experiments on several benchmark data sets prove the
effectiveness of our proposed approach over existing state-of-the-art
approaches. Code is available at
\url{https://github.com/MinkaiXu/ConfVAE-ICML21}.",1051,85
"['cs.CV', 'cs.GR', 'cs.HC']",Procams-Based Cybernetics,"Procams-based cybernetics is a unique, emerging research field, which aims at
enhancing and supporting our activities by naturally connecting human and
computers/machines as a cooperative integrated system via projector-camera
systems (procams). It rests on various research domains such as
virtual/augmented reality, computer vision, computer graphics, projection
display, human computer interface, human robot interaction and so on. This
laboratory presentation provides a brief history including recent achievements
of our procams-based cybernetics project.",560,25
"['stat.ML', 'cs.LG']",Topological Constraints on Homeomorphic Auto-Encoding,"When doing representation learning on data that lives on a known non-trivial
manifold embedded in high dimensional space, it is natural to desire the
encoder to be homeomorphic when restricted to the manifold, so that it is
bijective and continuous with a continuous inverse. Using topological
arguments, we show that when the manifold is non-trivial, the encoder must be
globally discontinuous and propose a universal, albeit impractical,
construction. In addition, we derive necessary constraints which need to be
satisfied when designing manifold-specific practical encoders. These are used
to analyse candidates for a homeomorphic encoder for the manifold of 3D
rotations $SO(3)$.",684,53
['cs.CV'],Semantic Segmentation of Remote Sensing Images with Sparse Annotations,"Training Convolutional Neural Networks (CNNs) for very high resolution images
requires a large quantity of high-quality pixel-level annotations, which is
extremely labor- and time-consuming to produce. Moreover, professional photo
interpreters might have to be involved for guaranteeing the correctness of
annotations. To alleviate such a burden, we propose a framework for semantic
segmentation of aerial images based on incomplete annotations, where annotators
are asked to label a few pixels with easy-to-draw scribbles. To exploit these
sparse scribbled annotations, we propose the FEature and Spatial relaTional
regulArization (FESTA) method to complement the supervised task with an
unsupervised learning signal that accounts for neighbourhood structures both in
spatial and feature terms.",795,70
['cs.CV'],EATEN: Entity-aware Attention for Single Shot Visual Text Extraction,"Extracting entity from images is a crucial part of many OCR applications,
such as entity recognition of cards, invoices, and receipts. Most of the
existing works employ classical detection and recognition paradigm. This paper
proposes an Entity-aware Attention Text Extraction Network called EATEN, which
is an end-to-end trainable system to extract the entities without any
post-processing. In the proposed framework, each entity is parsed by its
corresponding entity-aware decoder, respectively. Moreover, we innovatively
introduce a state transition mechanism which further improves the robustness of
entity extraction. In consideration of the absence of public benchmarks, we
construct a dataset of almost 0.6 million images in three real-world scenarios
(train ticket, passport and business card), which is publicly available at
https://github.com/beacandler/EATEN. To the best of our knowledge, EATEN is the
first single shot method to extract entities from images. Extensive experiments
on these benchmarks demonstrate the state-of-the-art performance of EATEN.",1068,68
"['cs.CV', 'eess.IV']",Masked GANs for Unsupervised Depth and Pose Prediction with Scale Consistency,"Previous work has shown that adversarial learning can be used for
unsupervised monocular depth and visual odometry (VO) estimation, in which the
adversarial loss and the geometric image reconstruction loss are utilized as
the mainly supervisory signals to train the whole unsupervised framework.
However, the performance of the adversarial framework and image reconstruction
is usually limited by occlusions and the visual field changes between frames.
This paper proposes a masked generative adversarial network (GAN) for
unsupervised monocular depth and ego-motion estimation.The MaskNet and Boolean
mask scheme are designed in this framework to eliminate the effects of
occlusions and impacts of visual field changes on the reconstruction loss and
adversarial loss, respectively. Furthermore, we also consider the scale
consistency of our pose network by utilizing a new scale-consistency loss, and
therefore, our pose network is capable of providing the full camera trajectory
over a long monocular sequence. Extensive experiments on the KITTI dataset show
that each component proposed in this paper contributes to the performance, and
both our depth and trajectory predictions achieve competitive performance on
the KITTI and Make3D datasets.",1247,77
['cs.CV'],How convolutional neural network see the world - A survey of convolutional neural network visualization methods,"Nowadays, the Convolutional Neural Networks (CNNs) have achieved impressive
performance on many computer vision related tasks, such as object detection,
image recognition, image retrieval, etc. These achievements benefit from the
CNNs outstanding capability to learn the input features with deep layers of
neuron structures and iterative training process. However, these learned
features are hard to identify and interpret from a human vision perspective,
causing a lack of understanding of the CNNs internal working mechanism. To
improve the CNN interpretability, the CNN visualization is well utilized as a
qualitative analysis method, which translates the internal features into
visually perceptible patterns. And many CNN visualization works have been
proposed in the literature to interpret the CNN in perspectives of network
structure, operation, and semantic concept. In this paper, we expect to provide
a comprehensive survey of several representative CNN visualization methods,
including Activation Maximization, Network Inversion, Deconvolutional Neural
Networks (DeconvNet), and Network Dissection based visualization. These methods
are presented in terms of motivations, algorithms, and experiment results.
Based on these visualization methods, we also discuss their practical
applications to demonstrate the significance of the CNN interpretability in
areas of network design, optimization, security enhancement, etc.",1430,111
['cs.CV'],Procedural Generation of Videos to Train Deep Action Recognition Networks,"Deep learning for human action recognition in videos is making significant
progress, but is slowed down by its dependency on expensive manual labeling of
large video collections. In this work, we investigate the generation of
synthetic training data for action recognition, as it has recently shown
promising results for a variety of other computer vision tasks. We propose an
interpretable parametric generative model of human action videos that relies on
procedural generation and other computer graphics techniques of modern game
engines. We generate a diverse, realistic, and physically plausible dataset of
human action videos, called PHAV for ""Procedural Human Action Videos"". It
contains a total of 39,982 videos, with more than 1,000 examples for each
action of 35 categories. Our approach is not limited to existing motion capture
sequences, and we procedurally define 14 synthetic actions. We introduce a deep
multi-task representation learning architecture to mix synthetic and real
videos, even if the action categories differ. Our experiments on the UCF101 and
HMDB51 benchmarks suggest that combining our large set of synthetic videos with
small real-world datasets can boost recognition performance, significantly
outperforming fine-tuning state-of-the-art unsupervised generative models of
videos.",1313,73
['cs.CV'],ApproxDet: Content and Contention-Aware Approximate Object Detection for Mobiles,"Advanced video analytic systems, including scene classification and object
detection, have seen widespread success in various domains such as smart cities
and autonomous transportation. With an ever-growing number of powerful client
devices, there is incentive to move these heavy video analytics workloads from
the cloud to mobile devices to achieve low latency and real-time processing and
to preserve user privacy. However, most video analytic systems are heavyweight
and are trained offline with some pre-defined latency or accuracy requirements.
This makes them unable to adapt at runtime in the face of three types of
dynamism -- the input video characteristics change, the amount of compute
resources available on the node changes due to co-located applications, and the
user's latency-accuracy requirements change. In this paper we introduce
ApproxDet, an adaptive video object detection framework for mobile devices to
meet accuracy-latency requirements in the face of changing content and resource
contention scenarios. To achieve this, we introduce a multi-branch object
detection kernel (layered on Faster R-CNN), which incorporates a data-driven
modeling approach on the performance metrics, and a latency SLA-driven
scheduler to pick the best execution branch at runtime. We couple this kernel
with approximable video object tracking algorithms to create an end-to-end
video object detection system. We evaluate ApproxDet on a large benchmark video
dataset and compare quantitatively to AdaScale and YOLOv3. We find that
ApproxDet is able to adapt to a wide variety of contention and content
characteristics and outshines all baselines, e.g., it achieves 52% lower
latency and 11.1% higher accuracy over YOLOv3.",1725,80
"['cs.LG', 'stat.ML']",A Survey on Self-supervised Pre-training for Sequential Transfer Learning in Neural Networks,"Deep neural networks are typically trained under a supervised learning
framework where a model learns a single task using labeled data. Instead of
relying solely on labeled data, practitioners can harness unlabeled or related
data to improve model performance, which is often more accessible and
ubiquitous. Self-supervised pre-training for transfer learning is becoming an
increasingly popular technique to improve state-of-the-art results using
unlabeled data. It involves first pre-training a model on a large amount of
unlabeled data, then adapting the model to target tasks of interest. In this
review, we survey self-supervised learning methods and their applications
within the sequential transfer learning framework. We provide an overview of
the taxonomy for self-supervised learning and transfer learning, and highlight
some prominent methods for designing pre-training tasks across different
domains. Finally, we discuss recent trends and suggest areas for future
investigation.",989,92
"['cs.LG', 'stat.ML']",Data Augmentation View on Graph Convolutional Network and the Proposal of Monte Carlo Graph Learning,"Today, there are two major understandings for graph convolutional networks,
i.e., in the spectral and spatial domain. But both lack transparency. In this
work, we introduce a new understanding for it -- data augmentation, which is
more transparent than the previous understandings. Inspired by it, we propose a
new graph learning paradigm -- Monte Carlo Graph Learning (MCGL). The core idea
of MCGL contains: (1) Data augmentation: propagate the labels of the training
set through the graph structure and expand the training set; (2) Model
training: use the expanded training set to train traditional classifiers. We
use synthetic datasets to compare the strengths of MCGL and graph convolutional
operation on clean graphs. In addition, we show that MCGL's tolerance to graph
structure noise is weaker than GCN on noisy graphs (four real-world datasets).
Moreover, inspired by MCGL, we re-analyze the reasons why the performance of
GCN becomes worse when deepened too much: rather than the mainstream view of
over-smoothing, we argue that the main reason is the graph structure noise, and
experimentally verify our view. The code is available at
https://github.com/DongHande/MCGL.",1180,100
"['cs.LG', 'q-bio.QM']",bigMap: Big Data Mapping with Parallelized t-SNE,"We introduce an improved unsupervised clustering protocol specially suited
for large-scale structured data. The protocol follows three steps: a
dimensionality reduction of the data, a density estimation over the low
dimensional representation of the data, and a final segmentation of the density
landscape. For the dimensionality reduction step we introduce a parallelized
implementation of the well-known t-Stochastic Neighbouring Embedding (t-SNE)
algorithm that significantly alleviates some inherent limitations, while
improving its suitability for large datasets. We also introduce a new adaptive
Kernel Density Estimation particularly coupled with the t-SNE framework in
order to get accurate density estimates out of the embedded data, and a variant
of the rainfalling watershed algorithm to identify clusters within the density
landscape. The whole mapping protocol is wrapped in the bigMap R package,
together with visualization and analysis tools to ease the qualitative and
quantitative assessment of the clustering.",1027,48
"['cs.CV', 'cs.LG', 'stat.ML']",Understanding Unconventional Preprocessors in Deep Convolutional Neural Networks for Face Identification,"Deep networks have achieved huge successes in application domains like object
and face recognition. The performance gain is attributed to different facets of
the network architecture such as: depth of the convolutional layers, activation
function, pooling, batch normalization, forward and back propagation and many
more. However, very little emphasis is made on the preprocessors. Therefore, in
this paper, the network's preprocessing module is varied across different
preprocessing approaches while keeping constant other facets of the network
architecture, to investigate the contribution preprocessing makes to the
network. Commonly used preprocessors are the data augmentation and
normalization and are termed conventional preprocessors. Others are termed the
unconventional preprocessors, they are: color space converters; HSV, CIE L*a*b*
and YCBCR, grey-level resolution preprocessors; full-based and plane-based
image quantization, illumination normalization and insensitive feature
preprocessing using: histogram equalization (HE), local contrast normalization
(LN) and complete face structural pattern (CFSP). To achieve fixed network
parameters, CNNs with transfer learning is employed. Knowledge from the
high-level feature vectors of the Inception-V3 network is transferred to
offline preprocessed LFW target data; and features trained using the SoftMax
classifier for face identification. The experiments show that the
discriminative capability of the deep networks can be improved by preprocessing
RGB data with HE, full-based and plane-based quantization, rgbGELog, and YCBCR,
preprocessors before feeding it to CNNs. However, for best performance, the
right setup of preprocessed data with augmentation and/or normalization is
required. The plane-based image quantization is found to increase the
homogeneity of neighborhood pixels and utilizes reduced bit depth for better
storage efficiency.",1910,104
['cs.CV'],Fast approximations to structured sparse coding and applications to object classification,"We describe a method for fast approximation of sparse coding. The input space
is subdivided by a binary decision tree, and we simultaneously learn a
dictionary and assignment of allowed dictionary elements for each leaf of the
tree. We store a lookup table with the assignments and the pseudoinverses for
each node, allowing for very fast inference. We give an algorithm for learning
the tree, the dictionary and the dictionary element assignment, and In the
process of describing this algorithm, we discuss the more general problem of
learning the groups in group structured sparse modelling. We show that our
method creates good sparse representations by using it in the object
recognition framework of \cite{lazebnik06,yang-cvpr-09}. Implementing our own
fast version of the SIFT descriptor the whole system runs at 20 frames per
second on $321 \times 481$ sized images on a laptop with a quad-core cpu, while
sacrificing very little accuracy on the Caltech 101 and 15 scenes benchmarks.",990,89
['cs.CV'],A Data-driven Prior on Facet Orientation for Semantic Mesh Labeling,"Mesh labeling is the key problem of classifying the facets of a 3D mesh with
a label among a set of possible ones. State-of-the-art methods model mesh
labeling as a Markov Random Field over the facets. These algorithms map image
segmentations to the mesh by minimizing an energy function that comprises a
data term, a smoothness terms, and class-specific priors. The latter favor a
labeling with respect to another depending on the orientation of the facet
normals. In this paper we propose a novel energy term that acts as a prior, but
does not require any prior knowledge about the scene nor scene-specific
relationship among classes. It bootstraps from a coarse mapping of the 2D
segmentations on the mesh, and it favors the facets to be labeled according to
the statistics of the mesh normals in their neighborhood. We tested our
approach against five different datasets and, even if we do not inject prior
knowledge, our method adapts to the data and overcomes the state-of-the-art.",987,67
['cs.CV'],A Generic Object Re-identification System for Short Videos,"Short video applications like TikTok and Kwai have been a great hit recently.
In order to meet the increasing demands and take full advantage of visual
information in short videos, objects in each short video need to be located and
analyzed as an upstream task. A question is thus raised -- how to improve the
accuracy and robustness of object detection, tracking, and re-identification
across tons of short videos with hundreds of categories and complicated visual
effects (VFX). To this end, a system composed of a detection module, a tracking
module and a generic object re-identification module, is proposed in this
paper, which captures features of major objects from short videos. In
particular, towards the high efficiency demands in practical short video
application, a Temporal Information Fusion Network (TIFN) is proposed in the
object detection module, which shows comparable accuracy and improved time
efficiency to the state-of-the-art video object detector. Furthermore, in order
to mitigate the fragmented issue of tracklets in short videos, a Cross-Layer
Pointwise Siamese Network (CPSN) is proposed in the tracking module to enhance
the robustness of the appearance model. Moreover, in order to evaluate the
proposed system, two challenge datasets containing real-world short videos are
built for video object trajectory extraction and generic object
re-identification respectively. Overall, extensive experiments for each module
and the whole system demonstrate the effectiveness and efficiency of our
system.",1528,58
"['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",A Framework for Reinforcement Learning and Planning,"Sequential decision making, commonly formalized as Markov Decision Process
optimization, is a key challenge in artificial intelligence. Two successful
approaches to MDP optimization are planning and reinforcement learning. Both
research fields largely have their own research communities. However, if both
research fields solve the same problem, then we should be able to disentangle
the common factors in their solution approaches. Therefore, this paper presents
a unifying framework for reinforcement learning and planning (FRAP), which
identifies the underlying dimensions on which any planning or learning
algorithm has to decide. At the end of the paper, we compare - in a single
table - a variety of well-known planning, model-free and model-based RL
algorithms along the dimensions of our framework, illustrating the validity of
the framework. Altogether, FRAP provides deeper insight into the algorithmic
space of planning and reinforcement learning, and also suggests new approaches
to integration of both fields.",1022,51
['cs.CV'],Multi-scale Matching Networks for Semantic Correspondence,"Deep features have been proven powerful in building accurate dense semantic
correspondences in various previous works. However, the multi-scale and
pyramidal hierarchy of convolutional neural networks has not been well studied
to learn discriminative pixel-level features for semantic correspondence. In
this paper, we propose a multi-scale matching network that is sensitive to tiny
semantic differences between neighboring pixels. We follow the coarse-to-fine
matching strategy and build a top-down feature and matching enhancement scheme
that is coupled with the multi-scale hierarchy of deep convolutional neural
networks. During feature enhancement, intra-scale enhancement fuses
same-resolution feature maps from multiple layers together via local
self-attention and cross-scale enhancement hallucinates higher-resolution
feature maps along the top-down hierarchy. Besides, we learn complementary
matching details at different scales thus the overall matching score is refined
by features of different semantic levels gradually. Our multi-scale matching
network can be trained end-to-end easily with few additional learnable
parameters. Experimental results demonstrate that the proposed method achieves
state-of-the-art performance on three popular benchmarks with high
computational efficiency.",1302,57
"['cs.LG', 'cs.CV']",Learning data augmentation policies using augmented random search,"Previous attempts for data augmentation are designed manually, and the
augmentation policies are dataset-specific. Recently, an automatic data
augmentation approach, named AutoAugment, is proposed using reinforcement
learning. AutoAugment searches for the augmentation polices in the discrete
search space, which may lead to a sub-optimal solution. In this paper, we
employ the Augmented Random Search method (ARS) to improve the performance of
AutoAugment. Our key contribution is to change the discrete search space to
continuous space, which will improve the searching performance and maintain the
diversities between sub-policies. With the proposed method, state-of-the-art
accuracies are achieved on CIFAR-10, CIFAR-100, and ImageNet (without
additional data). Our code is available at https://github.com/gmy2013/ARS-Aug.",826,65
"['cs.CV', 'cs.LG', 'eess.IV']",Regularization via deep generative models: an analysis point of view,"This paper proposes a new way of regularizing an inverse problem in imaging
(e.g., deblurring or inpainting) by means of a deep generative neural network.
Compared to end-to-end models, such approaches seem particularly interesting
since the same network can be used for many different problems and experimental
conditions, as soon as the generative model is suited to the data. Previous
works proposed to use a synthesis framework, where the estimation is performed
on the latent vector, the solution being obtained afterwards via the decoder.
Instead, we propose an analysis formulation where we directly optimize the
image itself and penalize the latent vector. We illustrate the interest of such
a formulation by running experiments of inpainting, deblurring and
super-resolution. In many cases our technique achieves a clear improvement of
the performance and seems to be more robust, in particular with respect to
initialization.",935,68
['cs.LG'],Predicting Mood Disorder Symptoms with Remotely Collected Videos Using an Interpretable Multimodal Dynamic Attention Fusion Network,"We developed a novel, interpretable multimodal classification method to
identify symptoms of mood disorders viz. depression, anxiety and anhedonia
using audio, video and text collected from a smartphone application. We used
CNN-based unimodal encoders to learn dynamic embeddings for each modality and
then combined these through a transformer encoder. We applied these methods to
a novel dataset - collected by a smartphone application - on 3002 participants
across up to three recording sessions. Our method demonstrated better
multimodal classification performance compared to existing methods that
employed static embeddings. Lastly, we used SHapley Additive exPlanations
(SHAP) to prioritize important features in our model that could serve as
potential digital markers.",775,131
"['stat.ML', 'cs.LG']",Learning non-Gaussian Time Series using the Box-Cox Gaussian Process,"Gaussian processes (GPs) are Bayesian nonparametric generative models that
provide interpretability of hyperparameters, admit closed-form expressions for
training and inference, and are able to accurately represent uncertainty. To
model general non-Gaussian data with complex correlation structure, GPs can be
paired with an expressive covariance kernel and then fed into a nonlinear
transformation (or warping). However, overparametrising the kernel and the
warping is known to, respectively, hinder gradient-based training and make the
predictions computationally expensive. We remedy this issue by (i) training the
model using derivative-free global-optimisation techniques so as to find
meaningful maxima of the model likelihood, and (ii) proposing a warping
function based on the celebrated Box-Cox transformation that requires minimal
numerical approximations---unlike existing warped GP models. We validate the
proposed approach by first showing that predictions can be computed
analytically, and then on a learning, reconstruction and forecasting experiment
using real-world datasets.",1092,68
"['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE']",Maximin affinity learning of image segmentation,"Images can be segmented by first using a classifier to predict an affinity
graph that reflects the degree to which image pixels must be grouped together
and then partitioning the graph to yield a segmentation. Machine learning has
been applied to the affinity classifier to produce affinity graphs that are
good in the sense of minimizing edge misclassification rates. However, this
error measure is only indirectly related to the quality of segmentations
produced by ultimately partitioning the affinity graph. We present the first
machine learning algorithm for training a classifier to produce affinity graphs
that are good in the sense of producing segmentations that directly minimize
the Rand index, a well known segmentation performance measure. The Rand index
measures segmentation performance by quantifying the classification of the
connectivity of image pixel pairs after segmentation. By using the simple graph
partitioning algorithm of finding the connected components of the thresholded
affinity graph, we are able to train an affinity classifier to directly
minimize the Rand index of segmentations resulting from the graph partitioning.
Our learning algorithm corresponds to the learning of maximin affinities
between image pixel pairs, which are predictive of the pixel-pair connectivity.",1305,47
['cs.CV'],Collaborative Deep Reinforcement Learning for Joint Object Search,"We examine the problem of joint top-down active search of multiple objects
under interaction, e.g., person riding a bicycle, cups held by the table, etc..
Such objects under interaction often can provide contextual cues to each other
to facilitate more efficient search. By treating each detector as an agent, we
present the first collaborative multi-agent deep reinforcement learning
algorithm to learn the optimal policy for joint active object localization,
which effectively exploits such beneficial contextual information. We learn
inter-agent communication through cross connections with gates between the
Q-networks, which is facilitated by a novel multi-agent deep Q-learning
algorithm with joint exploitation sampling. We verify our proposed method on
multiple object detection benchmarks. Not only does our model help to improve
the performance of state-of-the-art active localization models, it also reveals
interesting co-detection patterns that are intuitively interpretable.",988,65
"['cs.CV', 'cs.AI']",Searching for Alignment in Face Recognition,"A standard pipeline of current face recognition frameworks consists of four
individual steps: locating a face with a rough bounding box and several
fiducial landmarks, aligning the face image using a pre-defined template,
extracting representations and comparing. Among them, face detection, landmark
detection and representation learning have long been studied and a lot of works
have been proposed. As an essential step with a significant impact on
recognition performance, the alignment step has attracted little attention. In
this paper, we first explore and highlight the effects of different alignment
templates on face recognition. Then, for the first time, we try to search for
the optimal template automatically. We construct a well-defined searching space
by decomposing the template searching into the crop size and vertical shift,
and propose an efficient method Face Alignment Policy Search (FAPS). Besides, a
well-designed benchmark is proposed to evaluate the searched policy.
Experiments on our proposed benchmark validate the effectiveness of our method
to improve face recognition performance.",1111,43
"['cs.CV', 'cs.LG', 'stat.ML']",LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor Collision Avoidance System,"Collision avoidance is a critical task in many applications, such as ADAS
(advanced driver-assistance systems), industrial automation and robotics. In an
industrial automation setting, certain areas should be off limits to an
automated vehicle for protection of people and high-valued assets. These areas
can be quarantined by mapping (e.g., GPS) or via beacons that delineate a
no-entry area. We propose a delineation method where the industrial vehicle
utilizes a LiDAR {(Light Detection and Ranging)} and a single color camera to
detect passive beacons and model-predictive control to stop the vehicle from
entering a restricted space. The beacons are standard orange traffic cones with
a highly reflective vertical pole attached. The LiDAR can readily detect these
beacons, but suffers from false positives due to other reflective surfaces such
as worker safety vests. Herein, we put forth a method for reducing false
positive detection from the LiDAR by projecting the beacons in the camera
imagery via a deep learning method and validating the detection using a neural
network-learned projection from the camera to the LiDAR space. Experimental
data collected at Mississippi State University's Center for Advanced Vehicular
Systems (CAVS) shows the effectiveness of the proposed system in keeping the
true detection while mitigating false positives.",1355,99
['cs.CV'],DeepWheat: Estimating Phenotypic Traits from Crop Images with Deep Learning,"In this paper, we investigate estimating emergence and biomass traits from
color images and elevation maps of wheat field plots. We employ a
state-of-the-art deconvolutional network for segmentation and convolutional
architectures, with residual and Inception-like layers, to estimate traits via
high dimensional nonlinear regression. Evaluation was performed on two
different species of wheat, grown in field plots for an experimental plant
breeding study. Our framework achieves satisfactory performance with mean and
standard deviation of absolute difference of 1.05 and 1.40 counts for emergence
and 1.45 and 2.05 for biomass estimation. Our results for counting wheat plants
from field images are better than the accuracy reported for the similar, but
arguably less difficult, task of counting leaves from indoor images of rosette
plants. Our results for biomass estimation, even with a very small dataset,
improve upon all previously proposed approaches in the literature.",978,75
"['cs.CV', 'cs.LG', 'cs.NE']",Quad-networks: unsupervised learning to rank for interest point detection,"Several machine learning tasks require to represent the data using only a
sparse set of interest points. An ideal detector is able to find the
corresponding interest points even if the data undergo a transformation typical
for a given domain. Since the task is of high practical interest in computer
vision, many hand-crafted solutions were proposed. In this paper, we ask a
fundamental question: can we learn such detectors from scratch? Since it is
often unclear what points are ""interesting"", human labelling cannot be used to
find a truly unbiased solution. Therefore, the task requires an unsupervised
formulation. We are the first to propose such a formulation: training a neural
network to rank points in a transformation-invariant manner. Interest points
are then extracted from the top/bottom quantiles of this ranking. We validate
our approach on two tasks: standard RGB image interest point detection and
challenging cross-modal interest point detection between RGB and depth images.
We quantitatively show that our unsupervised method performs better or on-par
with baselines.",1088,73
"['cs.LG', 'stat.ML']",Recurrent Value Functions,"Despite recent successes in Reinforcement Learning, value-based methods often
suffer from high variance hindering performance. In this paper, we illustrate
this in a continuous control setting where state of the art methods perform
poorly whenever sensor noise is introduced. To overcome this issue, we
introduce Recurrent Value Functions (RVFs) as an alternative to estimate the
value function of a state. We propose to estimate the value function of the
current state using the value function of past states visited along the
trajectory. Due to the nature of their formulation, RVFs have a natural way of
learning an emphasis function that selectively emphasizes important states.
First, we establish RVF's asymptotic convergence properties in tabular
settings. We then demonstrate their robustness on a partially observable domain
and continuous control tasks. Finally, we provide a qualitative interpretation
of the learned emphasis function.",946,25
"['cs.LG', 'stat.ML']",Classifying Documents within Multiple Hierarchical Datasets using Multi-Task Learning,"Multi-task learning (MTL) is a supervised learning paradigm in which the
prediction models for several related tasks are learned jointly to achieve
better generalization performance. When there are only a few training examples
per task, MTL considerably outperforms the traditional Single task learning
(STL) in terms of prediction accuracy. In this work we develop an MTL based
approach for classifying documents that are archived within dual concept
hierarchies, namely, DMOZ and Wikipedia. We solve the multi-class
classification problem by defining one-versus-rest binary classification tasks
for each of the different classes across the two hierarchical datasets. Instead
of learning a linear discriminant for each of the different tasks
independently, we use a MTL approach with relationships between the different
tasks across the datasets established using the non-parametric, lazy, nearest
neighbor approach. We also develop and evaluate a transfer learning (TL)
approach and compare the MTL (and TL) methods against the standard single task
learning and semi-supervised learning approaches. Our empirical results
demonstrate the strength of our developed methods that show an improvement
especially when there are fewer number of training examples per classification
task.",1282,85
"['cs.LG', 'stat.ML']",Autoencoders and Generative Adversarial Networks for Imbalanced Sequence Classification,"Generative Adversarial Networks (GANs) have been used in many different
applications to generate realistic synthetic data. We introduce a novel GAN
with Autoencoder (GAN-AE) architecture to generate synthetic samples for
variable length, multi-feature sequence datasets. In this model, we develop a
GAN architecture with an additional autoencoder component, where recurrent
neural networks (RNNs) are used for each component of the model in order to
generate synthetic data to improve classification accuracy for a highly
imbalanced medical device dataset. In addition to the medical device dataset,
we also evaluate the GAN-AE performance on two additional datasets and
demonstrate the application of GAN-AE to a sequence-to-sequence task where both
synthetic sequence inputs and sequence outputs must be generated. To evaluate
the quality of the synthetic data, we train encoder-decoder models both with
and without the synthetic data and compare the classification model
performance. We show that a model trained with GAN-AE generated synthetic data
outperforms models trained with synthetic data generated both with standard
oversampling techniques such as SMOTE and Autoencoders as well as with state of
the art GAN-based models.",1234,87
['cs.CV'],"The use of deep learning in image segmentation, classification and detection","Recent years have shown that deep learned neural networks are a valuable tool
in the field of computer vision. This paper addresses the use of two different
kinds of network architectures, namely LeNet and Network in Network (NiN). They
will be compared in terms of both performance and computational efficiency by
addressing the classification and detection problems. In this paper, multiple
databases will be used to test the networks. One of them contains images
depicting burn wounds from pediatric cases, another one contains an extensive
number of art images and other facial databases were used for facial keypoints
detection.",633,76
"['cs.CV', 'eess.IV']",PointAR: Efficient Lighting Estimation for Mobile Augmented Reality,"We propose an efficient lighting estimation pipeline that is suitable to run
on modern mobile devices, with comparable resource complexities to
state-of-the-art mobile deep learning models. Our pipeline, PointAR, takes a
single RGB-D image captured from the mobile camera and a 2D location in that
image, and estimates 2nd order spherical harmonics coefficients. This estimated
spherical harmonics coefficients can be directly utilized by rendering engines
for supporting spatially variant indoor lighting, in the context of augmented
reality. Our key insight is to formulate the lighting estimation as a point
cloud-based learning problem directly from point clouds, which is in part
inspired by the Monte Carlo integration leveraged by real-time spherical
harmonics lighting. While existing approaches estimate lighting information
with complex deep learning pipelines, our method focuses on reducing the
computational complexity. Through both quantitative and qualitative
experiments, we demonstrate that PointAR achieves lower lighting estimation
errors compared to state-of-the-art methods. Further, our method requires an
order of magnitude lower resource, comparable to that of mobile-specific DNNs.",1206,67
"['cs.LG', 'stat.ML']",Visualizing and Understanding Sum-Product Networks,"Sum-Product Networks (SPNs) are recently introduced deep tractable
probabilistic models by which several kinds of inference queries can be
answered exactly and in a tractable time. Up to now, they have been largely
used as black box density estimators, assessed only by comparing their
likelihood scores only. In this paper we explore and exploit the inner
representations learned by SPNs. We do this with a threefold aim: first we want
to get a better understanding of the inner workings of SPNs; secondly, we seek
additional ways to evaluate one SPN model and compare it against other
probabilistic models, providing diagnostic tools to practitioners; lastly, we
want to empirically evaluate how good and meaningful the extracted
representations are, as in a classic Representation Learning framework. In
order to do so we revise their interpretation as deep neural networks and we
propose to exploit several visualization techniques on their node activations
and network outputs under different types of inference queries. To investigate
these models as feature extractors, we plug some SPNs, learned in a greedy
unsupervised fashion on image datasets, in supervised classification learning
tasks. We extract several embedding types from node activations by filtering
nodes by their type, by their associated feature abstraction level and by their
scope. In a thorough empirical comparison we prove them to be competitive
against those generated from popular feature extractors as Restricted Boltzmann
Machines. Finally, we investigate embeddings generated from random
probabilistic marginal queries as means to compare other tractable
probabilistic models on a common ground, extending our experiments to Mixtures
of Trees.",1727,50
"['cs.CV', 'cs.LG']",Graph-Adaptive Pruning for Efficient Inference of Convolutional Neural Networks,"In this work, we propose a graph-adaptive pruning (GAP) method for efficient
inference of convolutional neural networks (CNNs). In this method, the network
is viewed as a computational graph, in which the vertices denote the
computation nodes and edges represent the information flow. Through topology
analysis, GAP is capable of adapting to different network structures,
especially the widely used cross connections and multi-path data flow in recent
novel convolutional models. The models can be adaptively pruned at vertex-level
as well as edge-level without any post-processing, thus GAP can directly get
practical model compression and inference speed-up. Moreover, it does not need
any customized computation library or hardware support. Finetuning is conducted
after pruning to restore the model performance. In the finetuning step, we
adopt a self-taught knowledge distillation (KD) strategy by utilizing
information from the original model, through which, the performance of the
optimized model can be sufficiently improved, without introduction of any other
teacher model. Experimental results show the proposed GAP can achieve promising
result to make inference more efficient, e.g., for ResNeXt-29 on CIFAR10, it
can get 13X model compression and 4.3X practical speed-up with marginal loss of
accuracy.",1314,79
"['cs.CV', 'cs.RO']",PSE-Match: A Viewpoint-free Place Recognition Method with Parallel Semantic Embedding,"Accurate localization on autonomous driving cars is essential for autonomy
and driving safety, especially for complex urban streets and search-and-rescue
subterranean environments where high-accurate GPS is not available. However
current odometry estimation may introduce the drifting problems in long-term
navigation without robust global localization. The main challenges involve
scene divergence under the interference of dynamic environments and effective
perception of observation and object layout variance from different viewpoints.
To tackle these challenges, we present PSE-Match, a viewpoint-free place
recognition method based on parallel semantic analysis of isolated semantic
attributes from 3D point-cloud models. Compared with the original point cloud,
the observed variance of semantic attributes is smaller. PSE-Match incorporates
a divergence place learning network to capture different semantic attributes
parallelly through the spherical harmonics domain. Using both existing
benchmark datasets and two in-field collected datasets, our experiments show
that the proposed method achieves above 70% average recall with top one
retrieval and above 95% average recall with top ten retrieval cases. And
PSE-Match has also demonstrated an obvious generalization ability with a
limited training dataset.",1316,85
['cs.CV'],Multi-Echo LiDAR for 3D Object Detection,"LiDAR sensors can be used to obtain a wide range of measurement signals other
than a simple 3D point cloud, and those signals can be leveraged to improve
perception tasks like 3D object detection. A single laser pulse can be
partially reflected by multiple objects along its path, resulting in multiple
measurements called echoes. Multi-echo measurement can provide information
about object contours and semi-transparent surfaces which can be used to better
identify and locate objects. LiDAR can also measure surface reflectance
(intensity of laser pulse return), as well as ambient light of the scene
(sunlight reflected by objects). These signals are already available in
commercial LiDAR devices but have not been used in most LiDAR-based detection
models. We present a 3D object detection model which leverages the full
spectrum of measurement signals provided by LiDAR. First, we propose a
multi-signal fusion (MSF) module to combine (1) the reflectance and ambient
features extracted with a 2D CNN, and (2) point cloud features extracted using
a 3D graph neural network (GNN). Second, we propose a multi-echo aggregation
(MEA) module to combine the information encoded in different set of echo
points. Compared with traditional single echo point cloud methods, our proposed
Multi-Signal LiDAR Detector (MSLiD) extracts richer context information from a
wider range of sensing measurements and achieves more accurate 3D object
detection. Experiments show that by incorporating the multi-modality of LiDAR,
our method outperforms the state-of-the-art by up to 9.1%.",1570,40
['cs.LG'],Higher Order Linear Transformer,"Following up on the linear transformer part of the article from Katharopoulos
et al., that takes this idea from Shen et al., the trick that produces a linear
complexity for the attention mechanism is re-used and extended to a
second-order approximation of the softmax normalization.",282,31
"['stat.ML', 'cond-mat.stat-mech', 'cs.LG']",Learning the piece-wise constant graph structure of a varying Ising model,"This work focuses on the estimation of multiple change-points in a
time-varying Ising model that evolves piece-wise constantly. The aim is to
identify both the moments at which significant changes occur in the Ising
model, as well as the underlying graph structures. For this purpose, we propose
to estimate the neighborhood of each node by maximizing a penalized version of
its conditional log-likelihood. The objective of the penalization is twofold:
it imposes sparsity in the learned graphs and, thanks to a fused-type penalty,
it also enforces them to evolve piece-wise constantly. Using few assumptions,
we provide two change-points consistency theorems. Those are the first in the
context of unknown number of change-points detection in time-varying Ising
model. Finally, experimental results on several synthetic datasets and a
real-world dataset demonstrate the performance of our method.",897,73
['cs.CV'],Noise Conditional Flow Model for Learning the Super-Resolution Space,"Fundamentally, super-resolution is ill-posed problem because a low-resolution
image can be obtained from many high-resolution images. Recent studies for
super-resolution cannot create diverse super-resolution images. Although SRFlow
tried to account for ill-posed nature of the super-resolution by predicting
multiple high-resolution images given a low-resolution image, there is room to
improve the diversity and visual quality. In this paper, we propose Noise
Conditional flow model for Super-Resolution, NCSR, which increases the visual
quality and diversity of images through noise conditional layer. To learn more
diverse data distribution, we add noise to training data. However, low-quality
images are resulted from adding noise. We propose the noise conditional layer
to overcome this phenomenon. The noise conditional layer makes our model
generate more diverse images with higher visual quality than other works.
Furthermore, we show that this layer can overcome data distribution mismatch, a
problem that arises in normalizing flow models. With these benefits, NCSR
outperforms baseline in diversity and visual quality and achieves better visual
quality than traditional GAN-based models. We also get outperformed scores at
NTIRE 2021 challenge.",1256,68
"['cs.LG', 'cs.CR', 'cs.NI']",A Transfer Learning Approach for Network Intrusion Detection,"Convolution Neural Network (ConvNet) offers a high potential to generalize
input data. It has been widely used in many application areas, such as visual
imagery, where comprehensive learning datasets are available and a ConvNet
model can be well trained and perform the required function effectively.
ConvNet can also be applied to network intrusion detection. However, the
currently available datasets related to the network intrusion are often
inadequate, which makes the ConvNet learning deficient, hence the trained model
is not competent in detecting unknown intrusions. In this paper, we propose a
ConvNet model using transfer learning for network intrusion detection. The
model consists of two concatenated ConvNets and is built on a two-stage
learning process: learning a base dataset and transferring the learned
knowledge to the learning of the target dataset. Our experiments on the NSL-KDD
dataset show that the proposed model can improve the detection accuracy not
only on the test dataset containing mostly known attacks (KDDTest+) but also on
the test dataset featuring many novel attacks (KDDTest-21) -- about 2.68\%
improvement on KDDTest+ and 22.02\% on KDDTest-21 can be achieved, as compared
to the traditional ConvNet model.",1245,60
"['cs.CV', 'cs.CE']",Shape-based defect classification for Non Destructive Testing,"The aim of this work is to classify the aerospace structure defects detected
by eddy current non-destructive testing. The proposed method is based on the
assumption that the defect is bound to the reaction of the probe coil impedance
during the test. Impedance plane analysis is used to extract a feature vector
from the shape of the coil impedance in the complex plane, through the use of
some geometric parameters. Shape recognition is tested with three different
machine-learning based classifiers: decision trees, neural networks and Naive
Bayes. The performance of the proposed detection system are measured in terms
of accuracy, sensitivity, specificity, precision and Matthews correlation
coefficient. Several experiments are performed on dataset of eddy current
signal samples for aircraft structures. The obtained results demonstrate the
usefulness of our approach and the competiveness against existing descriptors.",925,61
['cs.CV'],Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation,"Multiple object tracking and segmentation requires detecting, tracking, and
segmenting objects belonging to a set of given classes. Most approaches only
exploit the temporal dimension to address the association problem, while
relying on single frame predictions for the segmentation mask itself. We
propose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich
spatio-temporal information for online multiple object tracking and
segmentation. PCAN first distills a space-time memory into a set of prototypes
and then employs cross-attention to retrieve rich information from the past
frames. To segment each object, PCAN adopts a prototypical appearance module to
learn a set of contrastive foreground and background prototypes, which are then
propagated over time. Extensive experiments demonstrate that PCAN outperforms
current video instance tracking and segmentation competition winners on both
Youtube-VIS and BDD100K datasets, and shows efficacy to both one-stage and
two-stage segmentation frameworks. Code will be available at
http://vis.xyz/pub/pcan.",1077,83
"['cs.LG', 'cs.AI']",Decomposition Methods with Deep Corrections for Reinforcement Learning,"Decomposition methods have been proposed to approximate solutions to large
sequential decision making problems. In contexts where an agent interacts with
multiple entities, utility decomposition can be used to separate the global
objective into local tasks considering each individual entity independently. An
arbitrator is then responsible for combining the individual utilities and
selecting an action in real time to solve the global problem. Although these
techniques can perform well empirically, they rely on strong assumptions of
independence between the local tasks and sacrifice the optimality of the global
solution. This paper proposes an approach that improves upon such approximate
solutions by learning a correction term represented by a neural network. We
demonstrate this approach on a fisheries management problem where multiple
boats must coordinate to maximize their catch over time as well as on a
pedestrian avoidance problem for autonomous driving. In each problem,
decomposition methods can scale to multiple boats or pedestrians by using
strategies involving one entity. We verify empirically that the proposed
correction method significantly improves the decomposition method and
outperforms a policy trained on the full scale problem without utility
decomposition.",1290,70
"['cs.LG', 'cs.HC']",Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding,"Graph neural networks (GNNs) are a class of powerful machine learning tools
that model node relations for making predictions of nodes or links. GNN
developers rely on quantitative metrics of the predictions to evaluate a GNN,
but similar to many other neural networks, it is difficult for them to
understand if the GNN truly learns characteristics of a graph as expected. We
propose an approach to corresponding an input graph to its node embedding (aka
latent space), a common component of GNNs that is later used for prediction. We
abstract the data and tasks, and develop an interactive multi-view interface
called CorGIE to instantiate the abstraction. As the key function in CorGIE, we
propose the K-hop graph layout to show topological neighbors in hops and their
clustering structure. To evaluate the functionality and usability of CorGIE, we
present how to use CorGIE in two usage scenarios, and conduct a case study with
two GNN experts.",946,85
['cs.CV'],Improved Transformer for High-Resolution GANs,"Attention-based models, exemplified by the Transformer, can effectively model
long range dependency, but suffer from the quadratic complexity of
self-attention operation, making them difficult to be adopted for
high-resolution image generation based on Generative Adversarial Networks
(GANs). In this paper, we introduce two key ingredients to Transformer to
address this challenge. First, in low-resolution stages of the generative
process, standard global self-attention is replaced with the proposed
multi-axis blocked self-attention which allows efficient mixing of local and
global attention. Second, in high-resolution stages, we drop self-attention
while only keeping multi-layer perceptrons reminiscent of the implicit neural
function. To further improve the performance, we introduce an additional
self-modulation component based on cross-attention. The resulting model,
denoted as HiT, has a linear computational complexity with respect to the image
size and thus directly scales to synthesizing high definition images. We show
in the experiments that the proposed HiT achieves state-of-the-art FID scores
of 31.87 and 2.95 on unconditional ImageNet $128 \times 128$ and FFHQ $256
\times 256$, respectively, with a reasonable throughput. We believe the
proposed HiT is an important milestone for generators in GANs which are
completely free of convolutions.",1367,45
['cs.CV'],LID 2020: The Learning from Imperfect Data Challenge Results,"Learning from imperfect data becomes an issue in many industrial applications
after the research community has made profound progress in supervised learning
from perfectly annotated datasets. The purpose of the Learning from Imperfect
Data (LID) workshop is to inspire and facilitate the research in developing
novel approaches that would harness the imperfect data and improve the
data-efficiency during training. A massive amount of user-generated data
nowadays available on multiple internet services. How to leverage those and
improve the machine learning models is a high impact problem. We organize the
challenges in conjunction with the workshop. The goal of these challenges is to
find the state-of-the-art approaches in the weakly supervised learning setting
for object detection, semantic segmentation, and scene parsing. There are three
tracks in the challenge, i.e., weakly supervised semantic segmentation (Track
1), weakly supervised scene parsing (Track 2), and weakly supervised object
localization (Track 3). In Track 1, based on ILSVRC DET, we provide pixel-level
annotations of 15K images from 200 categories for evaluation. In Track 2, we
provide point-based annotations for the training set of ADE20K. In Track 3,
based on ILSVRC CLS-LOC, we provide pixel-level annotations of 44,271 images
for evaluation. Besides, we further introduce a new evaluation metric proposed
by \cite{zhang2020rethinking}, i.e., IoU curve, to measure the quality of the
generated object localization maps. This technical report summarizes the
highlights from the challenge. The challenge submission server and the
leaderboard will continue to open for the researchers who are interested in it.
More details regarding the challenge and the benchmarks are available at
https://lidchallenge.github.io",1796,60
"['cs.CV', 'cs.MM', 'eess.IV']",Deep AutoEncoder-based Lossy Geometry Compression for Point Clouds,"Point cloud is a fundamental 3D representation which is widely used in real
world applications such as autonomous driving. As a newly-developed media
format which is characterized by complexity and irregularity, point cloud
creates a need for compression algorithms which are more flexible than existing
codecs. Recently, autoencoders(AEs) have shown their effectiveness in many
visual analysis tasks as well as image compression, which inspires us to employ
it in point cloud compression. In this paper, we propose a general
autoencoder-based architecture for lossy geometry point cloud compression. To
the best of our knowledge, it is the first autoencoder-based geometry
compression codec that directly takes point clouds as input rather than voxel
grids or collections of images. Compared with handcrafted codecs, this approach
adapts much more quickly to previously unseen media contents and media formats,
meanwhile achieving competitive performance. Our architecture consists of a
pointnet-based encoder, a uniform quantizer, an entropy estimation block and a
nonlinear synthesis transformation module. In lossy geometry compression of
point cloud, results show that the proposed method outperforms the test model
for categories 1 and 3 (TMC13) published by MPEG-3DG group on the 125th
meeting, and on average a 73.15\% BD-rate gain is achieved.",1352,66
"['cs.LG', 'q-fin.CP']",Multi-Scale RCNN Model for Financial Time-series Classification,"Financial time-series classification (FTC) is extremely valuable for
investment management. In past decades, it draws a lot of attention from a wide
extent of research areas, especially Artificial Intelligence (AI). Existing
researches majorly focused on exploring the effects of the Multi-Scale (MS)
property or the Temporal Dependency (TD) within financial time-series.
Unfortunately, most previous researches fail to combine these two properties
effectively and often fall short of accuracy and profitability. To effectively
combine and utilize both properties of financial time-series, we propose a
Multi-Scale Temporal Dependent Recurrent Convolutional Neural Network
(MSTD-RCNN) for FTC. In the proposed method, the MS features are simultaneously
extracted by convolutional units to precisely describe the state of the
financial market. Moreover, the TD and complementary across different scales
are captured through a Recurrent Neural Network. The proposed method is
evaluated on three financial time-series datasets which source from the Chinese
stock market. Extensive experimental results indicate that our model achieves
the state-of-the-art performance in trend classification and simulated trading,
compared with classical and advanced baseline models.",1265,63
['cs.CV'],Exploring and Exploiting Diversity for Image Segmentation,"Semantic image segmentation is an important computer vision task that is
difficult because it consists of both recognition and segmentation. The task is
often cast as a structured output problem on an exponentially large
output-space, which is typically modeled by a discrete probabilistic model. The
best segmentation is found by inferring the Maximum a-Posteriori (MAP) solution
over the output distribution defined by the model. Due to limitations in
optimization, the model cannot be arbitrarily complex. This leads to a
trade-off: devise a more accurate model that incorporates rich high-order
interactions between image elements at the cost of inaccurate and possibly
intractable optimization OR leverage a tractable model which produces less
accurate MAP solutions but may contain high quality solutions as other modes of
its output distribution.
  This thesis investigates the latter and presents a two stage approach to
semantic segmentation. In the first stage a tractable segmentation model
outputs a set of high probability segmentations from the underlying
distribution that are not just minor perturbations of each other. Critically
the output of this stage is a diverse set of plausible solutions and not just a
single one. In the second stage, a discriminatively trained re-ranking model
selects the best segmentation from this set. The re-ranking stage can use much
more complex features than what could be tractably used in the segmentation
model, allowing a better exploration of the solution space than simply
returning the MAP solution. The formulation is agnostic to the underlying
segmentation model (e.g. CRF, CNN, etc.) and optimization algorithm, which
makes it applicable to a wide range of models and inference methods. Evaluation
of the approach on a number of semantic image segmentation benchmark datasets
highlight its superiority over inferring the MAP solution.",1895,57
"['cs.CV', 'cs.RO']",Iterative Hough Forest with Histogram of Control Points for 6 DoF Object Registration from Depth Images,"State-of-the-art techniques proposed for 6D object pose recovery depend on
occlusion-free point clouds to accurately register objects in 3D space. To
reduce this dependency, we introduce a novel architecture called Iterative
Hough Forest with Histogram of Control Points that is capable of estimating
occluded and cluttered objects' 6D pose given a candidate 2D bounding box. Our
Iterative Hough Forest is learnt using patches extracted only from the positive
samples. These patches are represented with Histogram of Control Points (HoCP),
a ""scale-variant"" implicit volumetric description, which we derive from
recently introduced Implicit B-Splines (IBS). The rich discriminative
information provided by this scale-variance is leveraged during inference,
where the initial pose estimation of the object is iteratively refined based on
more discriminative control points by using our Iterative Hough Forest. We
conduct experiments on several test objects of a publicly available dataset to
test our architecture and to compare with the state-of-the-art.",1054,103
['cs.CV'],Defect Detection Techniques for Airbag Production Sewing Stages,"Airbags are subject to strict quality control in order to ensure passengers
safety. The quality of fabric and sewing thread influence the final product and
therefore, sewing defects must be early and accurately detected, in order to
remove the item from production. Airbag seams assembly can take various forms,
using linear and circle primitives, with threads of different colors and length
densities, creating lockstitch or double threads chainstitch. The paper
presents a framework for the automatic detection of defects occurring during
the airbag sewing stage. Types of defects as skipped stitch, missed stitch or
superimposed seam for lockstitch and two threads chainstitch are detected and
marked. Using image processing methods, the proposed framework follows the
seams path and determines if a color pattern of the considered stitches is
valid.",853,63
['cs.CV'],Category Contrast for Unsupervised Domain Adaptation in Visual Tasks,"Instance contrast for unsupervised representation learning has achieved great
success in recent years. In this work, we explore the idea of instance
contrastive learning in unsupervised domain adaptation (UDA) and propose a
novel Category Contrast technique (CaCo) that introduces semantic priors on top
of instance discrimination for visual UDA tasks. By considering instance
contrastive learning as a dictionary look-up operation, we construct a
semantics-aware dictionary with samples from both source and target domains
where each target sample is assigned a (pseudo) category label based on the
category priors of source samples. This allows category contrastive learning
(between target queries and the category-level dictionary) for
category-discriminative yet domain-invariant feature representations: samples
of the same category (from either source or target domain) are pulled closer
while those of different categories are pushed apart simultaneously. Extensive
UDA experiments in multiple visual tasks ($e.g.$, segmentation, classification
and detection) show that the simple implementation of CaCo achieves superior
performance as compared with the highly-optimized state-of-the-art methods.
Analytically and empirically, the experiments also demonstrate that CaCo is
complementary to existing UDA methods and generalizable to other learning
setups such as semi-supervised learning, unsupervised model adaptation, etc.",1432,68
"['cs.CV', 'cs.MM']",Temporal Action Proposal Generation with Transformers,"Transformer networks are effective at modeling long-range contextual
information and have recently demonstrated exemplary performance in the natural
language processing domain. Conventionally, the temporal action proposal
generation (TAPG) task is divided into two main sub-tasks: boundary prediction
and proposal confidence prediction, which rely on the frame-level dependencies
and proposal-level relationships separately. To capture the dependencies at
different levels of granularity, this paper intuitively presents a unified
temporal action proposal generation framework with original Transformers,
called TAPG Transformer, which consists of a Boundary Transformer and a
Proposal Transformer. Specifically, the Boundary Transformer captures long-term
temporal dependencies to predict precise boundary information and the Proposal
Transformer learns the rich inter-proposal relationships for reliable
confidence evaluation. Extensive experiments are conducted on two popular
benchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG
Transformer outperforms state-of-the-art methods. Equipped with the existing
action classifier, our method achieves remarkable performance on the temporal
action localization task. Codes and models will be available.",1276,53
['cs.CV'],Bag of Color Features For Color Constancy,"In this paper, we propose a novel color constancy approach, called Bag of
Color Features (BoCF), building upon Bag-of-Features pooling. The proposed
method substantially reduces the number of parameters needed for illumination
estimation. At the same time, the proposed method is consistent with the color
constancy assumption stating that global spatial information is not relevant
for illumination estimation and local information ( edges, etc.) is sufficient.
Furthermore, BoCF is consistent with color constancy statistical approaches and
can be interpreted as a learning-based generalization of many statistical
approaches. To further improve the illumination estimation accuracy, we propose
a novel attention mechanism for the BoCF model with two variants based on
self-attention. BoCF approach and its variants achieve competitive, compared to
the state of the art, results while requiring much fewer parameters on three
benchmark datasets: ColorChecker RECommended, INTEL-TUT version 2, and NUS8.",1004,41
['cs.CV'],A Survey of Quantization Methods for Efficient Neural Network Inference,"As soon as abstract mathematical computations were adapted to computation on
digital computers, the problem of efficient representation, manipulation, and
communication of the numerical values in those computations arose. Strongly
related to the problem of numerical representation is the problem of
quantization: in what manner should a set of continuous real-valued numbers be
distributed over a fixed discrete set of numbers to minimize the number of bits
required and also to maximize the accuracy of the attendant computations? This
perennial problem of quantization is particularly relevant whenever memory
and/or computational resources are severely restricted, and it has come to the
forefront in recent years due to the remarkable performance of Neural Network
models in computer vision, natural language processing, and related areas.
Moving from floating-point representations to low-precision fixed integer
values represented in four bits or less holds the potential to reduce the
memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x
to 8x are often realized in practice in these applications. Thus, it is not
surprising that quantization has emerged recently as an important and very
active sub-area of research in the efficient implementation of computations
associated with Neural Networks. In this article, we survey approaches to the
problem of quantizing the numerical values in deep Neural Network computations,
covering the advantages/disadvantages of current methods. With this survey and
its organization, we hope to have presented a useful snapshot of the current
research in quantization for Neural Networks and to have given an intelligent
organization to ease the evaluation of future research in this area.",1760,71
['cs.LG'],"Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better","Deep Learning has revolutionized the fields of computer vision, natural
language understanding, speech recognition, information retrieval and more.
However, with the progressive improvements in deep learning models, their
number of parameters, latency, resources required to train, etc. have all have
increased significantly. Consequently, it has become important to pay attention
to these footprint metrics of a model as well, not just its quality. We present
and motivate the problem of efficiency in deep learning, followed by a thorough
survey of the five core areas of model efficiency (spanning modeling
techniques, infrastructure, and hardware) and the seminal work there. We also
present an experiment-based guide along with code, for practitioners to
optimize their model training and deployment. We believe this is the first
comprehensive survey in the efficient deep learning space that covers the
landscape of model efficiency from modeling techniques to hardware support. Our
hope is that this survey would provide the reader with the mental model and the
necessary understanding of the field to apply generic efficiency techniques to
immediately get significant improvements, and also equip them with ideas for
further research and experimentation to achieve additional gains.",1290,92
['cs.CV'],On the Importance of Stereo for Accurate Depth Estimation: An Efficient Semi-Supervised Deep Neural Network Approach,"We revisit the problem of visual depth estimation in the context of
autonomous vehicles. Despite the progress on monocular depth estimation in
recent years, we show that the gap between monocular and stereo depth accuracy
remains large$-$a particularly relevant result due to the prevalent reliance
upon monocular cameras by vehicles that are expected to be self-driving. We
argue that the challenges of removing this gap are significant, owing to
fundamental limitations of monocular vision. As a result, we focus our efforts
on depth estimation by stereo. We propose a novel semi-supervised learning
approach to training a deep stereo neural network, along with a novel
architecture containing a machine-learned argmax layer and a custom runtime
(that will be shared publicly) that enables a smaller version of our stereo DNN
to run on an embedded GPU. Competitive results are shown on the KITTI 2015
stereo dataset. We also evaluate the recent progress of stereo algorithms by
measuring the impact upon accuracy of various design criteria.",1042,116
['cs.CV'],Near Real-time Hippocampus Segmentation Using Patch-based Canonical Neural Network,"Over the past decades, state-of-the-art medical image segmentation has
heavily rested on signal processing paradigms, most notably registration-based
label propagation and pair-wise patch comparison, which are generally slow
despite a high segmentation accuracy. In recent years, deep learning has
revolutionalized computer vision with many practices outperforming prior art,
in particular the convolutional neural network (CNN) studies on image
classification. Deep CNN has also started being applied to medical image
segmentation lately, but generally involves long training and demanding memory
requirements, achieving limited success. We propose a patch-based deep learning
framework based on a revisit to the classic neural network model with
substantial modernization, including the use of Rectified Linear Unit (ReLU)
activation, dropout layers, 2.5D tri-planar patch multi-pathway settings. In a
test application to hippocampus segmentation using 100 brain MR images from the
ADNI database, our approach significantly outperformed prior art in terms of
both segmentation accuracy and speed: scoring a median Dice score up to 90.98%
on a near real-time performance (<1s).",1178,82
['cs.CV'],Graph Contrastive Clustering,"Recently, some contrastive learning methods have been proposed to
simultaneously learn representations and clustering assignments, achieving
significant improvements. However, these methods do not take the category
information and clustering objective into consideration, thus the learned
representations are not optimal for clustering and the performance might be
limited. Towards this issue, we first propose a novel graph contrastive
learning framework, which is then applied to the clustering task and we come up
with the Graph Constrastive Clustering~(GCC) method. Different from basic
contrastive clustering that only assumes an image and its augmentation should
share similar representation and clustering assignments, we lift the
instance-level consistency to the cluster-level consistency with the assumption
that samples in one cluster and their augmentations should all be similar.
Specifically, on the one hand, the graph Laplacian based contrastive loss is
proposed to learn more discriminative and clustering-friendly features. On the
other hand, a novel graph-based contrastive learning strategy is proposed to
learn more compact clustering assignments. Both of them incorporate the latent
category information to reduce the intra-cluster variance while increasing the
inter-cluster variance. Experiments on six commonly used datasets demonstrate
the superiority of our proposed approach over the state-of-the-art methods.",1437,28
"['cs.LG', 'cs.RO', 'cs.SY', 'eess.SY', 'stat.ML']",Training Adversarial Agents to Exploit Weaknesses in Deep Control Policies,"Deep learning has become an increasingly common technique for various control
problems, such as robotic arm manipulation, robot navigation, and autonomous
vehicles. However, the downside of using deep neural networks to learn control
policies is their opaque nature and the difficulties of validating their
safety. As the networks used to obtain state-of-the-art results become
increasingly deep and complex, the rules they have learned and how they operate
become more challenging to understand. This presents an issue, since in
safety-critical applications the safety of the control policy must be ensured
to a high confidence level. In this paper, we propose an automated black box
testing framework based on adversarial reinforcement learning. The technique
uses an adversarial agent, whose goal is to degrade the performance of the
target model under test. We test the approach on an autonomous vehicle problem,
by training an adversarial reinforcement learning agent, which aims to cause a
deep neural network-driven autonomous vehicle to collide. Two neural networks
trained for autonomous driving are compared, and the results from the testing
are used to compare the robustness of their learned control policies. We show
that the proposed framework is able to find weaknesses in both control policies
that were not evident during online testing and therefore, demonstrate a
significant benefit over manual testing methods.",1431,74
"['cs.CV', 'cs.HC']",Investigation of Color Constancy for Ubiquitous Wireless LAN/Camera Positioning: An Initial Outcome,"This paper present our color constancy investigation in the hybridization of
Wireless LAN and Camera positioning in the mobile phone. Five typical color
constancy schemes are analyzed in different location environment. The results
can be used to combine with RF signals from Wireless LAN positioning by using
model fitting approach in order to establish absolute positioning output. There
is no conventional searching algorithm required, thus it is expected to reduce
the complexity of computation. Finally we present our preliminary results to
illustrate the indoor positioning algorithm performance evaluation for an
indoor environment set-up.",645,99
"['cs.LG', 'eess.IV', 'stat.ML']",Recent Developments Combining Ensemble Smoother and Deep Generative Networks for Facies History Matching,"Ensemble smoothers are among the most successful and efficient techniques
currently available for history matching. However, because these methods rely
on Gaussian assumptions, their performance is severely degraded when the prior
geology is described in terms of complex facies distributions. Inspired by the
impressive results obtained by deep generative networks in areas such as image
and video generation, we started an investigation focused on the use of
autoencoders networks to construct a continuous parameterization for facies
models. In our previous publication, we combined a convolutional variational
autoencoder (VAE) with the ensemble smoother with multiple data assimilation
(ES-MDA) for history matching production data in models generated with
multiple-point geostatistics. Despite the good results reported in our previous
publication, a major limitation of the designed parameterization is the fact
that it does not allow applying distance-based localization during the ensemble
smoother update, which limits its application in large-scale problems.
  The present work is a continuation of this research project focusing in two
aspects: firstly, we benchmark seven different formulations, including VAE,
generative adversarial network (GAN), Wasserstein GAN, variational
auto-encoding GAN, principal component analysis (PCA) with cycle GAN, PCA with
transfer style network and VAE with style loss. These formulations are tested
in a synthetic history matching problem with channelized facies. Secondly, we
propose two strategies to allow the use of distance-based localization with the
deep learning parameterizations.",1638,104
"['cs.CV', 'cs.RO']",Object Detection and Classification in Occupancy Grid Maps using Deep Convolutional Networks,"A detailed environment perception is a crucial component of automated
vehicles. However, to deal with the amount of perceived information, we also
require segmentation strategies. Based on a grid map environment
representation, well-suited for sensor fusion, free-space estimation and
machine learning, we detect and classify objects using deep convolutional
neural networks. As input for our networks we use a multi-layer grid map
efficiently encoding 3D range sensor information. The inference output consists
of a list of rotated bounding boxes with associated semantic classes. We
conduct extensive ablation studies, highlight important design considerations
when using grid maps and evaluate our models on the KITTI Bird's Eye View
benchmark. Qualitative and quantitative benchmark results show that we achieve
robust detection and state of the art accuracy solely using top-view grid maps
from range sensor data.",918,92
['cs.CV'],Generative VoxelNet: Learning Energy-Based Models for 3D Shape Synthesis and Analysis,"3D data that contains rich geometry information of objects and scenes is
valuable for understanding 3D physical world. With the recent emergence of
large-scale 3D datasets, it becomes increasingly crucial to have a powerful 3D
generative model for 3D shape synthesis and analysis. This paper proposes a
deep 3D energy-based model to represent volumetric shapes. The maximum
likelihood training of the model follows an ""analysis by synthesis"" scheme. The
benefits of the proposed model are six-fold: first, unlike GANs and VAEs, the
model training does not rely on any auxiliary models; second, the model can
synthesize realistic 3D shapes by Markov chain Monte Carlo (MCMC); third, the
conditional model can be applied to 3D object recovery and super resolution;
fourth, the model can serve as a building block in a multi-grid modeling and
sampling framework for high resolution 3D shape synthesis; fifth, the model can
be used to train a 3D generator via MCMC teaching; sixth, the unsupervisedly
trained model provides a powerful feature extractor for 3D data, which is
useful for 3D object classification. Experiments demonstrate that the proposed
model can generate high-quality 3D shape patterns and can be useful for a wide
variety of 3D shape analysis.",1258,85
['cs.CV'],Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection,"Most of the crowd abnormal event detection methods rely on complex
hand-crafted features to represent the crowd motion and appearance.
Convolutional Neural Networks (CNN) have shown to be a powerful tool with
excellent representational capacities, which can leverage the need for
hand-crafted features. In this paper, we show that keeping track of the changes
in the CNN feature across time can facilitate capturing the local abnormality.
We specifically propose a novel measure-based method which allows measuring the
local abnormality in a video by combining semantic information (inherited from
existing CNN models) with low-level Optical-Flow. One of the advantage of this
method is that it can be used without the fine-tuning costs. The proposed
method is validated on challenging abnormality detection datasets and the
results show the superiority of our method compared to the state-of-the-art
methods.",909,87
['cs.CV'],View Inter-Prediction GAN: Unsupervised Representation Learning for 3D Shapes by Learning Global Shape Memories to Support Local View Predictions,"In this paper we present a novel unsupervised representation learning
approach for 3D shapes, which is an important research challenge as it avoids
the manual effort required for collecting supervised data. Our method trains an
RNN-based neural network architecture to solve multiple view inter-prediction
tasks for each shape. Given several nearby views of a shape, we define view
inter-prediction as the task of predicting the center view between the input
views, and reconstructing the input views in a low-level feature space. The key
idea of our approach is to implement the shape representation as a
shape-specific global memory that is shared between all local view
inter-predictions for each shape. Intuitively, this memory enables the system
to aggregate information that is useful to better solve the view
inter-prediction tasks for each shape, and to leverage the memory as a
view-independent shape representation. Our approach obtains the best results
using a combination of L_2 and adversarial losses for the view inter-prediction
task. We show that VIP-GAN outperforms state-of-the-art methods in unsupervised
3D feature learning on three large scale 3D shape benchmarks.",1185,145
['cs.CV'],Know What Your Neighbors Do: 3D Semantic Segmentation of Point Clouds,"In this paper, we present a deep learning architecture which addresses the
problem of 3D semantic segmentation of unstructured point clouds. Compared to
previous work, we introduce grouping techniques which define point
neighborhoods in the initial world space and the learned feature space.
Neighborhoods are important as they allow to compute local or global point
features depending on the spatial extend of the neighborhood. Additionally, we
incorporate dedicated loss functions to further structure the learned point
feature space: the pairwise distance loss and the centroid loss. We show how to
apply these mechanisms to the task of 3D semantic segmentation of point clouds
and report state-of-the-art performance on indoor and outdoor datasets.",752,69
"['stat.ML', 'cs.AI', 'cs.CE', 'cs.CV', 'cs.LG']",Chemception: A Deep Neural Network with Minimal Chemistry Knowledge Matches the Performance of Expert-developed QSAR/QSPR Models,"In the last few years, we have seen the transformative impact of deep
learning in many applications, particularly in speech recognition and computer
vision. Inspired by Google's Inception-ResNet deep convolutional neural network
(CNN) for image classification, we have developed ""Chemception"", a deep CNN for
the prediction of chemical properties, using just the images of 2D drawings of
molecules. We develop Chemception without providing any additional explicit
chemistry knowledge, such as basic concepts like periodicity, or advanced
features like molecular descriptors and fingerprints. We then show how
Chemception can serve as a general-purpose neural network architecture for
predicting toxicity, activity, and solvation properties when trained on a
modest database of 600 to 40,000 compounds. When compared to multi-layer
perceptron (MLP) deep neural networks trained with ECFP fingerprints,
Chemception slightly outperforms in activity and solvation prediction and
slightly underperforms in toxicity prediction. Having matched the performance
of expert-developed QSAR/QSPR deep learning models, our work demonstrates the
plausibility of using deep neural networks to assist in computational chemistry
research, where the feature engineering process is performed primarily by a
deep learning algorithm.",1311,128
"['cs.LG', 'math.CO']",Wasserstein Learning of Determinantal Point Processes,"Determinantal point processes (DPPs) have received significant attention as
an elegant probabilistic model for discrete subset selection. Most prior work
on DPP learning focuses on maximum likelihood estimation (MLE). While efficient
and scalable, MLE approaches do not leverage any subset similarity information
and may fail to recover the true generative distribution of discrete data. In
this work, by deriving a differentiable relaxation of a DPP sampling algorithm,
we present a novel approach for learning DPPs that minimizes the Wasserstein
distance between the model and data composed of observed subsets. Through an
evaluation on a real-world dataset, we show that our Wasserstein learning
approach provides significantly improved predictive performance on a generative
task compared to DPPs trained using MLE.",819,53
"['cs.LG', 'math.OC', 'stat.ML']",Efficient Local Planning with Linear Function Approximation,"We study query and computationally efficient planning algorithms with linear
function approximation and a simulator. We assume that the agent only has local
access to the simulator, meaning that the agent can only query the simulator at
states that have been visited before. This setting is more practical than many
prior works on reinforcement learning with a generative model. We propose an
algorithm named confident Monte Carlo least square policy iteration (Confident
MC-LSPI) for this setting. Under the assumption that the Q-functions of all
deterministic policies are linear in known features of the state-action pairs,
we show that our algorithm has polynomial query and computational complexities
in the dimension of the features, the effective planning horizon and the
targeted sub-optimality, while these complexities are independent of the size
of the state space. One technical contribution of our work is the introduction
of a novel proof technique that makes use of a virtual policy iteration
algorithm. We use this method to leverage existing results on
$\ell_\infty$-bounded approximate policy iteration to show that our algorithm
can learn the optimal policy for the given initial state even only with local
access to the simulator. We believe that this technique can be extended to
broader settings beyond this work.",1335,59
['cs.CV'],Multiple Exemplars-based Hallucinationfor Face Super-resolution and Editing,"Given a really low-resolution input image of a face (say 16x16 or 8x8
pixels), the goal of this paper is to reconstruct a high-resolution version
thereof. This, by itself, is an ill-posed problem, as the high-frequency
information is missing in the low-resolution input and needs to be
hallucinated, based on prior knowledge about the image content. Rather than
relying on a generic face prior, in this paper, we explore the use of a set of
exemplars, i.e. other high-resolution images of the same person. These guide
the neural network as we condition the output on them. Multiple exemplars work
better than a single one. To combine the information from multiple exemplars
effectively, we introduce a pixel-wise weight generation module. Besides
standard face super-resolution, our method allows to perform subtle face
editing simply by replacing the exemplars with another set with different
facial features. A user study is conducted and shows the super-resolved images
can hardly be distinguished from real images on the CelebA dataset. A
qualitative comparison indicates our model outperforms methods proposed in the
literature on the CelebA and WebFace dataset.",1167,75
['cs.CV'],Using Human Psychophysics to Evaluate Generalization in Scene Text Recognition Models,"Scene text recognition models have advanced greatly in recent years. Inspired
by human reading we characterize two important scene text recognition models by
measuring their domains i.e. the range of stimulus images that they can read.
The domain specifies the ability of readers to generalize to different word
lengths, fonts, and amounts of occlusion. These metrics identify strengths and
weaknesses of existing models. Relative to the attention-based (Attn) model, we
discover that the connectionist temporal classification (CTC) model is more
robust to noise and occlusion, and better at generalizing to different word
lengths. Further, we show that in both models, adding noise to training images
yields better generalization to occlusion. These results demonstrate the value
of testing models till they break, complementing the traditional data science
focus on optimizing performance.",891,85
"['cs.CV', 'cs.NE']",Learning What and Where to Draw,"Generative Adversarial Networks (GANs) have recently demonstrated the
capability to synthesize compelling real-world images, such as room interiors,
album covers, manga, faces, birds, and flowers. While existing models can
synthesize images based on global constraints such as a class label or caption,
they do not provide control over pose or object location. We propose a new
model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes
images given instructions describing what content to draw in which location. We
show high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset,
conditioned on both informal text descriptions and also object location. Our
system exposes control over both the bounding box around the bird and its
constituent parts. By modeling the conditional distributions over part
locations, our system also enables conditioning on arbitrary subsets of parts
(e.g. only the beak and tail), yielding an efficient interface for picking part
locations. We also show preliminary results on the more challenging domain of
text- and location-controllable synthesis of images of human actions on the
MPII Human Pose dataset.",1174,31
"['cs.LG', 'cs.DC']",AutoScale: Optimizing Energy Efficiency of End-to-End Edge Inference under Stochastic Variance,"Deep learning inference is increasingly run at the edge. As the programming
and system stack support becomes mature, it enables acceleration opportunities
within a mobile system, where the system performance envelope is scaled up with
a plethora of programmable co-processors. Thus, intelligent services designed
for mobile users can choose between running inference on the CPU or any of the
co-processors on the mobile system, or exploiting connected systems, such as
the cloud or a nearby, locally connected system. By doing so, the services can
scale out the performance and increase the energy efficiency of edge mobile
systems. This gives rise to a new challenge - deciding when inference should
run where. Such execution scaling decision becomes more complicated with the
stochastic nature of mobile-cloud execution, where signal strength variations
of the wireless networks and resource interference can significantly affect
real-time inference performance and system energy efficiency. To enable
accurate, energy-efficient deep learning inference at the edge, this paper
proposes AutoScale. AutoScale is an adaptive and light-weight execution scaling
engine built upon the custom-designed reinforcement learning algorithm. It
continuously learns and selects the most energy-efficient inference execution
target by taking into account characteristics of neural networks and available
systems in the collaborative cloud-edge execution environment while adapting to
the stochastic runtime variance. Real system implementation and evaluation,
considering realistic execution scenarios, demonstrate an average of 9.8 and
1.6 times energy efficiency improvement for DNN edge inference over the
baseline mobile CPU and cloud offloading, while meeting the real-time
performance and accuracy requirement.",1803,94
"['cs.LG', 'stat.ML']",Simulated Data Experiments for Time Series Classification Part 1: Accuracy Comparison with Default Settings,"There are now a broad range of time series classification (TSC) algorithms
designed to exploit different representations of the data. These have been
evaluated on a range of problems hosted at the UCR-UEA TSC Archive
(www.timeseriesclassification.com), and there have been extensive comparative
studies. However, our understanding of why one algorithm outperforms another is
still anecdotal at best. This series of experiments is meant to help provide
insights into what sort of discriminatory features in the data lead one set of
algorithms that exploit a particular representation to be better than other
algorithms. We categorise five different feature spaces exploited by TSC
algorithms then design data simulators to generate randomised data from each
representation. We describe what results we expected from each class of
algorithm and data representation, then observe whether these prior beliefs are
supported by the experimental evidence. We provide an open source
implementation of all the simulators to allow for the controlled testing of
hypotheses relating to classifier performance on different data
representations. We identify many surprising results that confounded our
expectations, and use these results to highlight how an over simplified view of
classifier structure can often lead to erroneous prior beliefs. We believe
ensembling can often overcome prior bias, and our results support the belief by
showing that the ensemble approach adopted by the Hierarchical Collective of
Transform based Ensembles (HIVE-COTE) is significantly better than the
alternatives when the data representation is unknown, and is significantly
better than, or not significantly significantly better than, or not
significantly worse than, the best other approach on three out of five of the
individual simulators.",1814,107
"['cs.CV', '62H30', 'I.4.6']",Kullback-Leibler Divergence-Based Fuzzy $C$-Means Clustering Incorporating Morphological Reconstruction and Wavelet Frames for Image Segmentation,"Although spatial information of images usually enhance the robustness of the
Fuzzy C-Means (FCM) algorithm, it greatly increases the computational costs for
image segmentation. To achieve a sound trade-off between the segmentation
performance and the speed of clustering, we come up with a Kullback-Leibler
(KL) divergence-based FCM algorithm by incorporating a tight wavelet frame
transform and a morphological reconstruction operation. To enhance FCM's
robustness, an observed image is first filtered by using the morphological
reconstruction. A tight wavelet frame system is employed to decompose the
observed and filtered images so as to form their feature sets. Considering
these feature sets as data of clustering, an modified FCM algorithm is
proposed, which introduces a KL divergence term in the partition matrix into
its objective function. The KL divergence term aims to make membership degrees
of each image pixel closer to those of its neighbors, which brings that the
membership partition becomes more suitable and the parameter setting of FCM
becomes simplified. On the basis of the obtained partition matrix and
prototypes, the segmented feature set is reconstructed by minimizing the
inverse process of the modified objective function. To modify abnormal features
produced in the reconstruction process, each reconstructed feature is
reassigned to the closest prototype. As a result, the segmentation accuracy of
KL divergence-based FCM is further improved. What's more, the segmented image
is reconstructed by using a tight wavelet frame reconstruction operation.
Finally, supporting experiments coping with synthetic, medical and color images
are reported. Experimental results exhibit that the proposed algorithm works
well and comes with better segmentation performance than other comparative
algorithms. Moreover, the proposed algorithm requires less time than most of
the FCM-related algorithms.",1918,145
"['cs.LG', 'cs.SI', 'stat.ML']",Ego-based Entropy Measures for Structural Representations,"In complex networks, nodes that share similar structural characteristics
often exhibit similar roles (e.g type of users in a social network or the
hierarchical position of employees in a company). In order to leverage this
relationship, a growing literature proposed latent representations that
identify structurally equivalent nodes. However, most of the existing methods
require high time and space complexity. In this paper, we propose VNEstruct, a
simple approach for generating low-dimensional structural node embeddings, that
is both time efficient and robust to perturbations of the graph structure. The
proposed approach focuses on the local neighborhood of each node and employs
the Von Neumann entropy, an information-theoretic tool, to extract features
that capture the neighborhood's topology. Moreover, on graph classification
tasks, we suggest the utilization of the generated structural embeddings for
the transformation of an attributed graph structure into a set of augmented
node attributes. Empirically, we observe that the proposed approach exhibits
robustness on structural role identification tasks and state-of-the-art
performance on graph classification tasks, while maintaining very high
computational speed.",1233,57
['cs.CV'],Top-down Visual Saliency Guided by Captions,"Neural image/video captioning models can generate accurate descriptions, but
their internal process of mapping regions to words is a black box and therefore
difficult to explain. Top-down neural saliency methods can find important
regions given a high-level semantic task such as object classification, but
cannot use a natural language sentence as the top-down input for the task. In
this paper, we propose Caption-Guided Visual Saliency to expose the
region-to-word mapping in modern encoder-decoder networks and demonstrate that
it is learned implicitly from caption training data, without any pixel-level
annotations. Our approach can produce spatial or spatiotemporal heatmaps for
both predicted captions, and for arbitrary query sentences. It recovers
saliency without the overhead of introducing explicit attention layers, and can
be used to analyze a variety of existing model architectures and improve their
design. Evaluation on large-scale video and image datasets demonstrates that
our approach achieves comparable captioning performance with existing methods
while providing more accurate saliency heatmaps. Our code is available at
visionlearninggroup.github.io/caption-guided-saliency/.",1201,43
['cs.CV'],Multi-modality Sensor Data Classification with Selective Attention,"Multimodal wearable sensor data classification plays an important role in
ubiquitous computing and has a wide range of applications in scenarios from
healthcare to entertainment. However, most existing work in this field employs
domain-specific approaches and is thus ineffective in complex sit- uations
where multi-modality sensor data are col- lected. Moreover, the wearable sensor
data are less informative than the conventional data such as texts or images.
In this paper, to improve the adapt- ability of such classification methods
across differ- ent application domains, we turn this classification task into a
game and apply a deep reinforcement learning scheme to deal with complex
situations dynamically. Additionally, we introduce a selective attention
mechanism into the reinforcement learn- ing scheme to focus on the crucial
dimensions of the data. This mechanism helps to capture extra information from
the signal and thus it is able to significantly improve the discriminative
power of the classifier. We carry out several experiments on three wearable
sensor datasets and demonstrate the competitive performance of the proposed
approach compared to several state-of-the-art baselines.",1201,66
"['cs.LG', 'cs.NE']",A Novel Genetic Algorithm with Hierarchical Evaluation Strategy for Hyperparameter Optimisation of Graph Neural Networks,"Graph representation of structured data can facilitate the extraction of
stereoscopic features, and it has demonstrated excellent ability when working
with deep learning systems, the so-called Graph Neural Networks (GNNs).
Choosing a promising architecture for constructing GNNs can be transferred to a
hyperparameter optimisation problem, a very challenging task due to the size of
the underlying search space and high computational cost for evaluating
candidate GNNs. To address this issue, this research presents a novel genetic
algorithm with a hierarchical evaluation strategy (HESGA), which combines the
full evaluation of GNNs with a fast evaluation approach. By using full
evaluation, a GNN is represented by a set of hyperparameter values and trained
on a specified dataset, and root mean square error (RMSE) will be used to
measure the quality of the GNN represented by the set of hyperparameter values
(for regression problems). While in the proposed fast evaluation process, the
training will be interrupted at an early stage, the difference of RMSE values
between the starting and interrupted epochs will be used as a fast score, which
implies the potential of the GNN being considered. To coordinate both types of
evaluations, the proposed hierarchical strategy uses the fast evaluation in a
lower level for recommending candidates to a higher level, where the full
evaluation will act as a final assessor to maintain a group of elite
individuals. To validate the effectiveness of HESGA, we apply it to optimise
two types of deep graph neural networks. The experimental results on three
benchmark datasets demonstrate its advantages compared to Bayesian
hyperparameter optimization.",1696,120
"['cs.CV', 'eess.IV']",LEAD: LiDAR Extender for Autonomous Driving,"3D perception using sensors under vehicle industrial standard is the rigid
demand in autonomous driving. MEMS LiDAR emerges with irresistible trend due to
its lower cost, more robust, and meeting the mass-production standards.
However, it suffers small field of view (FoV), slowing down the step of its
population. In this paper, we propose LEAD, i.e., LiDAR Extender for Autonomous
Driving, to extend the MEMS LiDAR by coupled image w.r.t both FoV and range. We
propose a multi-stage propagation strategy based on depth distributions and
uncertainty map, which shows effective propagation ability. Moreover, our depth
outpainting/propagation network follows a teacher-student training fashion,
which transfers depth estimation ability to depth completion network without
any scale error passed. To validate the LiDAR extension quality, we utilize a
high-precise laser scanner to generate a ground-truth dataset. Quantitative and
qualitative evaluations show that our scheme outperforms SOTAs with a large
margin. We believe the proposed LEAD along with the dataset would benefit the
community w.r.t depth researches.",1117,43
"['cs.LG', 'cs.AI', 'cs.NE']",Model Primitive Hierarchical Lifelong Reinforcement Learning,"Learning interpretable and transferable subpolicies and performing task
decomposition from a single, complex task is difficult. Some traditional
hierarchical reinforcement learning techniques enforce this decomposition in a
top-down manner, while meta-learning techniques require a task distribution at
hand to learn such decompositions. This paper presents a framework for using
diverse suboptimal world models to decompose complex task solutions into
simpler modular subpolicies. This framework performs automatic decomposition of
a single source task in a bottom up manner, concurrently learning the required
modular subpolicies as well as a controller to coordinate them. We perform a
series of experiments on high dimensional continuous action control tasks to
demonstrate the effectiveness of this approach at both complex single task
learning and lifelong learning. Finally, we perform ablation studies to
understand the importance and robustness of different elements in the framework
and limitations to this approach.",1026,60
"['cs.LG', 'stat.ML']",A deep-learning view of chemical space designed to facilitate drug discovery,"Drug discovery projects entail cycles of design, synthesis, and testing that
yield a series of chemically related small molecules whose properties, such as
binding affinity to a given target protein, are progressively tailored to a
particular drug discovery goal. The use of deep learning technologies could
augment the typical practice of using human intuition in the design cycle, and
thereby expedite drug discovery projects. Here we present DESMILES, a deep
neural network model that advances the state of the art in machine learning
approaches to molecular design. We applied DESMILES to a previously published
benchmark that assesses the ability of a method to modify input molecules to
inhibit the dopamine receptor D2, and DESMILES yielded a 77% lower failure rate
compared to state-of-the-art models. To explain the ability of DESMILES to hone
molecular properties, we visualize a layer of the DESMILES network, and further
demonstrate this ability by using DESMILES to tailor the same molecules used in
the D2 benchmark test to dock more potently against seven different receptors.",1091,76
"['cs.LG', 'cs.CV', 'stat.ML']",Simultaneous multi-view instance detection with learned geometric soft-constraints,"We propose to jointly learn multi-view geometry and warping between views of
the same object instances for robust cross-view object detection. What makes
multi-view object instance detection difficult are strong changes in viewpoint,
lighting conditions, high similarity of neighbouring objects, and strong
variability in scale. By turning object detection and instance
re-identification in different views into a joint learning task, we are able to
incorporate both image appearance and geometric soft constraints into a single,
multi-view detection process that is learnable end-to-end. We validate our
method on a new, large data set of street-level panoramas of urban objects and
show superior performance compared to various baselines. Our contribution is
threefold: a large-scale, publicly available data set for multi-view instance
detection and re-identification; an annotation tool custom-tailored for
multi-view instance detection; and a novel, holistic multi-view instance
detection and re-identification method that jointly models geometry and
appearance across views.",1080,82
"['cs.CV', 'cs.CR']",Revisiting the Threat Space for Vision-based Keystroke Inference Attacks,"A vision-based keystroke inference attack is a side-channel attack in which
an attacker uses an optical device to record users on their mobile devices and
infer their keystrokes. The threat space for these attacks has been studied in
the past, but we argue that the defining characteristics for this threat space,
namely the strength of the attacker, are outdated. Previous works do not study
adversaries with vision systems that have been trained with deep neural
networks because these models require large amounts of training data and
curating such a dataset is expensive. To address this, we create a large-scale
synthetic dataset to simulate the attack scenario for a keystroke inference
attack. We show that first pre-training on synthetic data, followed by adopting
transfer learning techniques on real-life data, increases the performance of
our deep learning models. This indicates that these models are able to learn
rich, meaningful representations from our synthetic data and that training on
the synthetic data can help overcome the issue of having small, real-life
datasets for vision-based key stroke inference attacks. For this work, we focus
on single keypress classification where the input is a frame of a keypress and
the output is a predicted key. We are able to get an accuracy of 95.6% after
pre-training a CNN on our synthetic data and training on a small set of
real-life data in an adversarial domain adaptation framework. Source Code for
Simulator:
https://github.com/jlim13/keystroke-inference-attack-synthetic-dataset-generator-",1557,72
"['cs.LG', 'stat.ML']",CatGCN: Graph Convolutional Networks with Categorical Node Features,"Recent studies on Graph Convolutional Networks (GCNs) reveal that the initial
node representations (i.e., the node representations before the first-time
graph convolution) largely affect the final model performance. However, when
learning the initial representation for a node, most existing work linearly
combines the embeddings of node features, without considering the interactions
among the features (or feature embeddings). We argue that when the node
features are categorical, e.g., in many real-world applications like user
profiling and recommender system, feature interactions usually carry important
signals for predictive analytics. Ignoring them will result in suboptimal
initial node representation and thus weaken the effectiveness of the follow-up
graph convolution. In this paper, we propose a new GCN model named CatGCN,
which is tailored for graph learning when the node features are categorical.
Specifically, we integrate two ways of explicit interaction modeling into the
learning of initial node representation, i.e., local interaction modeling on
each pair of node features and global interaction modeling on an artificial
feature graph. We then refine the enhanced initial node representations with
the neighborhood aggregation-based graph convolution. We train CatGCN in an
end-to-end fashion and demonstrate it on semi-supervised node classification.
Extensive experiments on three tasks of user profiling (the prediction of user
age, city, and purchase level) from Tencent and Alibaba datasets validate the
effectiveness of CatGCN, especially the positive effect of performing feature
interaction modeling before graph convolution.",1658,67
"['cs.CV', 'I.5.4; I.4.9; J.2']",Geometric Cross-Modal Comparison of Heterogeneous Sensor Data,"In this work, we address the problem of cross-modal comparison of aerial data
streams. A variety of simulated automobile trajectories are sensed using two
different modalities: full-motion video, and radio-frequency (RF) signals
received by detectors at various locations. The information represented by the
two modalities is compared using self-similarity matrices (SSMs) corresponding
to time-ordered point clouds in feature spaces of each of these data sources;
we note that these feature spaces can be of entirely different scale and
dimensionality. Several metrics for comparing SSMs are explored, including a
cutting-edge time-warping technique that can simultaneously handle local time
warping and partial matches, while also controlling for the change in geometry
between feature spaces of the two modalities. We note that this technique is
quite general, and does not depend on the choice of modalities. In this
particular setting, we demonstrate that the cross-modal distance between SSMs
corresponding to the same trajectory type is smaller than the cross-modal
distance between SSMs corresponding to distinct trajectory types, and we
formalize this observation via precision-recall metrics in experiments.
Finally, we comment on promising implications of these ideas for future
integration into multiple-hypothesis tracking systems.",1344,61
"['cs.CV', 'cs.LG', 'I.2.6; I.2.10; I.4.9; I.5.1; I.5.4']",Vision-Based Layout Detection from Scientific Literature using Recurrent Convolutional Neural Networks,"We present an approach for adapting convolutional neural networks for object
recognition and classification to scientific literature layout detection
(SLLD), a shared subtask of several information extraction problems. Scientific
publications contain multiple types of information sought by researchers in
various disciplines, organized into an abstract, bibliography, and sections
documenting related work, experimental methods, and results; however, there is
no effective way to extract this information due to their diverse layout. In
this paper, we present a novel approach to developing an end-to-end learning
framework to segment and classify major regions of a scientific document. We
consider scientific document layout analysis as an object detection task over
digital images, without any additional text features that need to be added into
the network during the training process. Our technical objective is to
implement transfer learning via fine-tuning of pre-trained networks and thereby
demonstrate that this deep learning architecture is suitable for tasks that
lack very large document corpora for training ab initio. As part of the
experimental test bed for empirical evaluation of this approach, we created a
merged multi-corpus data set for scientific publication layout detection tasks.
Our results show good improvement with fine-tuning of a pre-trained base
network using this merged data set, compared to the baseline convolutional
neural network architecture.",1483,102
"['cs.CV', 'eess.IV']",Anomaly Detection by One Class Latent Regularized Networks,"Anomaly detection is a fundamental problem in computer vision area with many
real-world applications. Given a wide range of images belonging to the normal
class, emerging from some distribution, the objective of this task is to
construct the model to detect out-of-distribution images belonging to abnormal
instances. Semi-supervised Generative Adversarial Networks (GAN)-based methods
have been gaining popularity in anomaly detection task recently. However, the
training process of GAN is still unstable and challenging. To solve these
issues, a novel adversarial dual autoencoder network is proposed, in which the
underlying structure of training data is not only captured in latent feature
space, but also can be further restricted in the space of latent representation
in a discriminant manner, leading to a more accurate detector. In addition, the
auxiliary autoencoder regarded as a discriminator could obtain an more stable
training process. Experiments show that our model achieves the state-of-the-art
results on MNIST and CIFAR10 datasets as well as GTSRB stop signs dataset.",1086,58
"['cs.CV', 'cs.AI', 'cs.LG']",Out-of-distribution detection and generalization to enhance fairness in age prediction,"Deep learning-based facial recognition systems have experienced increased
media attention due to exhibiting unfair behavior. Large enterprises, such as
IBM, shut down their facial recognition and age prediction systems as a
consequence. Age prediction is an especially difficult application with the
issue of fairness remaining an open research problem (e.g. predicting age for
different ethnicity equally accurate). One of the main causes of unfair
behavior in age prediction methods lies in the distribution and diversity of
the training data. In this work, we present two novel approaches for dataset
curation and data augmentation in order to increase fairness through
distribution aware curation and increase diversity through distribution aware
augmentation. To achieve this, we created an out-of-distribution technique
which is used to select the data most relevant to the deep neural network's
(DNN) task when balancing the data among age, ethnicity, and gender. Our
approach shows promising results. Our best-trained DNN model outperformed all
academic and industrial baselines in terms of fairness by up to 4.92 times.
When it comes to generalization, the increase in diversity also enhanced the
DNN's performance, outperforming state-of-the-art approaches of prior research
on the Age Estimation Benchmark dataset AFAD by 30.40% and the Amazon AWS and
Microsoft Azure public cloud systems by 31.88% and 10.95%, respectively.",1435,86
"['stat.ML', 'cs.LG', 'stat.ME']",Computing Valid p-value for Optimal Changepoint by Selective Inference using Dynamic Programming,"There is a vast body of literature related to methods for detecting
changepoints (CP). However, less attention has been paid to assessing the
statistical reliability of the detected CPs. In this paper, we introduce a
novel method to perform statistical inference on the significance of the CPs,
estimated by a Dynamic Programming (DP)-based optimal CP detection algorithm.
Based on the selective inference (SI) framework, we propose an exact
(non-asymptotic) approach to compute valid p-values for testing the
significance of the CPs. Although it is well-known that SI has low statistical
power because of over-conditioning, we address this disadvantage by introducing
parametric programming techniques. Then, we propose an efficient method to
conduct SI with the minimum amount of conditioning, leading to high statistical
power. We conduct experiments on both synthetic and real-world datasets,
through which we offer evidence that our proposed method is more powerful than
existing methods, has decent performance in terms of computational efficiency,
and provides good results in many practical applications.",1112,96
"['cs.LG', 'cs.AI', 'stat.ML']",Understanding and Improving Recurrent Networks for Human Activity Recognition by Continuous Attention,"Deep neural networks, including recurrent networks, have been successfully
applied to human activity recognition. Unfortunately, the final representation
learned by recurrent networks might encode some noise (irrelevant signal
components, unimportant sensor modalities, etc.). Besides, it is difficult to
interpret the recurrent networks to gain insight into the models' behavior. To
address these issues, we propose two attention models for human activity
recognition: temporal attention and sensor attention. These two mechanisms
adaptively focus on important signals and sensor modalities. To further improve
the understandability and mean F1 score, we add continuity constraints,
considering that continuous sensor signals are more robust than discrete ones.
We evaluate the approaches on three datasets and obtain state-of-the-art
results. Furthermore, qualitative analysis shows that the attention learned by
the models agree well with human intuition.",958,101
['cs.CV'],Static object detection and segmentation in videos based on dual foregrounds difference with noise filtering,"This paper presents static object detection and segmentation method in videos
from cluttered scenes. Robust static object detection is still challenging task
due to presence of moving objects in many surveillance applications. The level
of difficulty is extremely influenced by on how you label the object to be
identified as static that do not establish the original background but appeared
in the video at different time. In this context, background subtraction
technique based on the frame difference concept is applied to the
identification of static objects. Firstly, we estimate a frame differencing
foreground mask image by computing the difference of each frame with respect to
a static reference frame. The Mixture of Gaussian MOG method is applied to
detect the moving particles and then outcome foreground mask is subtracted from
frame differencing foreground mask. Pre-processing techniques, illumination
equalization and de-hazing methods are applied to handle low contrast and to
reduce the noise from scattered materials in the air e.g. water droplets and
dust particles. Finally, a set of mathematical morphological operation and
largest connected-component analysis is applied to segment the object and
suppress the noise. The proposed method was built for rock breaker station
application and effectively validated with real, synthetic and two public data
sets. The results demonstrate the proposed approach can robustly detect,
segmented the static objects without any prior information of tracking.",1518,108
"['cs.LG', 'cs.CV', 'stat.ML']",Optimal transport maps for distribution preserving operations on latent spaces of Generative Models,"Generative models such as Variational Auto Encoders (VAEs) and Generative
Adversarial Networks (GANs) are typically trained for a fixed prior
distribution in the latent space, such as uniform or Gaussian. After a trained
model is obtained, one can sample the Generator in various forms for
exploration and understanding, such as interpolating between two samples,
sampling in the vicinity of a sample or exploring differences between a pair of
samples applied to a third sample. In this paper, we show that the latent space
operations used in the literature so far induce a distribution mismatch between
the resulting outputs and the prior distribution the model was trained on. To
address this, we propose to use distribution matching transport maps to ensure
that such latent space operations preserve the prior distribution, while
minimally modifying the original operation. Our experimental results validate
that the proposed operations give higher quality samples compared to the
original operations.",1005,99
"['cs.LG', 'cs.AI', 'stat.ML']",Deep Echo State Network (DeepESN): A Brief Survey,"The study of deep recurrent neural networks (RNNs) and, in particular, of
deep Reservoir Computing (RC) is gaining an increasing research attention in
the neural networks community. The recently introduced Deep Echo State Network
(DeepESN) model opened the way to an extremely efficient approach for designing
deep neural networks for temporal data. At the same time, the study of DeepESNs
allowed to shed light on the intrinsic properties of state dynamics developed
by hierarchical compositions of recurrent layers, i.e. on the bias of depth in
RNNs architectural design. In this paper, we summarize the advancements in the
development, analysis and applications of DeepESNs.",677,49
"['cs.LG', 'cs.CE', 'eess.SP', 'physics.comp-ph']",Accelerating Training in Artificial Neural Networks with Dynamic Mode Decomposition,"Training of deep neural networks (DNNs) frequently involves optimizing
several millions or even billions of parameters. Even with modern computing
architectures, the computational expense of DNN training can inhibit, for
instance, network architecture design optimization, hyper-parameter studies,
and integration into scientific research cycles. The key factor limiting
performance is that both the feed-forward evaluation and the back-propagation
rule are needed for each weight during optimization in the update rule. In this
work, we propose a method to decouple the evaluation of the update rule at each
weight. At first, Proper Orthogonal Decomposition (POD) is used to identify a
current estimate of the principal directions of evolution of weights per layer
during training based on the evolution observed with a few backpropagation
steps. Then, Dynamic Mode Decomposition (DMD) is used to learn the dynamics of
the evolution of the weights in each layer according to these principal
directions. The DMD model is used to evaluate an approximate converged state
when training the ANN. Afterward, some number of backpropagation steps are
performed, starting from the DMD estimates, leading to an update to the
principal directions and DMD model. This iterative process is repeated until
convergence. By fine-tuning the number of backpropagation steps used for each
DMD model estimation, a significant reduction in the number of operations
required to train the neural networks can be achieved. In this paper, the DMD
acceleration method will be explained in detail, along with the theoretical
justification for the acceleration provided by DMD. This method is illustrated
using a regression problem of key interest for the scientific machine learning
community: the prediction of a pollutant concentration field in a diffusion,
advection, reaction problem.",1862,83
"['cs.LG', 'cs.CC', 'math.OC']",Primal-Dual $π$ Learning: Sample Complexity and Sublinear Run Time for Ergodic Markov Decision Problems,"Consider the problem of approximating the optimal policy of a Markov decision
process (MDP) by sampling state transitions. In contrast to existing
reinforcement learning methods that are based on successive approximations to
the nonlinear Bellman equation, we propose a Primal-Dual $\pi$ Learning method
in light of the linear duality between the value and policy. The $\pi$ learning
method is model-free and makes primal-dual updates to the policy and value
vectors as new data are revealed. For infinite-horizon undiscounted Markov
decision process with finite state space $S$ and finite action space $A$, the
$\pi$ learning method finds an $\epsilon$-optimal policy using the following
number of sample transitions $$ \tilde{O}( \frac{(\tau\cdot t^*_{mix})^2 |S|
|A| }{\epsilon^2} ),$$ where $t^*_{mix}$ is an upper bound of mixing times
across all policies and $\tau$ is a parameter characterizing the range of
stationary distributions across policies. The $\pi$ learning method also
applies to the computational problem of MDP where the transition probabilities
and rewards are explicitly given as the input. In the case where each state
transition can be sampled in $\tilde{O}(1)$ time, the $\pi$ learning method
gives a sublinear-time algorithm for solving the averaged-reward MDP.",1288,103
"['cs.CV', 'cs.RO']",A Novel Multi-layer Framework for Tiny Obstacle Discovery,"For tiny obstacle discovery in a monocular image, edge is a fundamental
visual element. Nevertheless, because of various reasons, e.g., noise and
similar color distribution with background, it is still difficult to detect the
edges of tiny obstacles at long distance. In this paper, we propose an
obstacle-aware discovery method to recover the missing contours of these
obstacles, which helps to obtain obstacle proposals as much as possible. First,
by using visual cues in monocular images, several multi-layer regions are
elaborately inferred to reveal the distances from the camera. Second, several
novel obstacle-aware occlusion edge maps are constructed to well capture the
contours of tiny obstacles, which combines cues from each layer. Third, to
ensure the existence of the tiny obstacle proposals, the maps from all layers
are used for proposals extraction. Finally, based on these proposals containing
tiny obstacles, a novel obstacle-aware regressor is proposed to generate an
obstacle occupied probability map with high confidence. The convincing
experimental results with comparisons on the Lost and Found dataset demonstrate
the effectiveness of our approach, achieving around 9.5% improvement on the
accuracy than FPHT and PHT, it even gets comparable performance to MergeNet.
Moreover, our method outperforms the state-of-the-art algorithms and
significantly improves the discovery ability for tiny obstacles at long
distance.",1442,57
['cs.CV'],Double Weighted Truncated Nuclear Norm Regularization for Low-Rank Matrix Completion,"Matrix completion focuses on recovering a matrix from a small subset of its
observed elements, and has already gained cumulative attention in computer
vision. Many previous approaches formulate this issue as a low-rank matrix
approximation problem. Recently, a truncated nuclear norm has been presented as
a surrogate of traditional nuclear norm, for better estimation to the rank of a
matrix. The truncated nuclear norm regularization (TNNR) method is applicable
in real-world scenarios. However, it is sensitive to the selection of the
number of truncated singular values and requires numerous iterations to
converge. Hereby, this paper proposes a revised approach called the double
weighted truncated nuclear norm regularization (DW-TNNR), which assigns
different weights to the rows and columns of a matrix separately, to accelerate
the convergence with acceptable performance. The DW-TNNR is more robust to the
number of truncated singular values than the TNNR. Instead of the iterative
updating scheme in the second step of TNNR, this paper devises an efficient
strategy that uses a gradient descent manner in a concise form, with a
theoretical guarantee in optimization. Sufficient experiments conducted on real
visual data prove that DW-TNNR has promising performance and holds the
superiority in both speed and accuracy for matrix completion.",1351,84
"['cs.CV', 'cs.AI', 'cs.LG']",Colorization Transformer,"We present the Colorization Transformer, a novel approach for diverse high
fidelity image colorization based on self-attention. Given a grayscale image,
the colorization proceeds in three steps. We first use a conditional
autoregressive transformer to produce a low resolution coarse coloring of the
grayscale image. Our architecture adopts conditional transformer layers to
effectively condition grayscale input. Two subsequent fully parallel networks
upsample the coarse colored low resolution image into a finely colored high
resolution image. Sampling from the Colorization Transformer produces diverse
colorings whose fidelity outperforms the previous state-of-the-art on
colorising ImageNet based on FID results and based on a human evaluation in a
Mechanical Turk test. Remarkably, in more than 60% of cases human evaluators
prefer the highest rated among three generated colorings over the ground truth.
The code and pre-trained checkpoints for Colorization Transformer are publicly
available at
https://github.com/google-research/google-research/tree/master/coltran",1074,24
"['cs.CV', 'cs.LG']",Monocular Differentiable Rendering for Self-Supervised 3D Object Detection,"3D object detection from monocular images is an ill-posed problem due to the
projective entanglement of depth and scale. To overcome this ambiguity, we
present a novel self-supervised method for textured 3D shape reconstruction and
pose estimation of rigid objects with the help of strong shape priors and 2D
instance masks. Our method predicts the 3D location and meshes of each object
in an image using differentiable rendering and a self-supervised objective
derived from a pretrained monocular depth estimation network. We use the KITTI
3D object detection dataset to evaluate the accuracy of the method. Experiments
demonstrate that we can effectively use noisy monocular depth and
differentiable rendering as an alternative to expensive 3D ground-truth labels
or LiDAR information.",787,74
['stat.ML'],Graphical Exponential Screening,"In high dimensions we propose and analyze an aggregation estimator of the
precision matrix for Gaussian graphical models. This estimator, called
graphical Exponential Screening (gES), linearly combines a suitable set of
individual estimators with different underlying graphs, and balances the
estimation error and sparsity. We study the risk of this aggregation estimator
and show that it is comparable to that of the best estimator based on a single
graph, chosen by an oracle. Numerical performance of our method is investigated
using both simulated and real datasets, in comparison with some state-of-art
estimation procedures.",630,31
['cs.CV'],Ensemble Super-Resolution with A Reference Dataset,"By developing sophisticated image priors or designing deep(er) architectures,
a variety of image Super-Resolution (SR) approaches have been proposed recently
and achieved very promising performance. A natural question that arises is
whether these methods can be reformulated into a unifying framework and whether
this framework assists in SR reconstruction? In this paper, we present a simple
but effective single image SR method based on ensemble learning, which can
produce a better performance than that could be obtained from any of SR methods
to be ensembled (or called component super-resolvers). Based on the assumption
that better component super-resolver should have larger ensemble weight when
performing SR reconstruction, we present a Maximum A Posteriori (MAP)
estimation framework for the inference of optimal ensemble weights. Specially,
we introduce a reference dataset, which is composed of High-Resolution (HR) and
Low-Resolution (LR) image pairs, to measure the super-resolution abilities
(prior knowledge) of different component super-resolvers. To obtain the optimal
ensemble weights, we propose to incorporate the reconstruction constraint,
which states that the degenerated HR image should be equal to the LR
observation one, as well as the prior knowledge of ensemble weights into the
MAP estimation framework. Moreover, the proposed optimization problem can be
solved by an analytical solution. We study the performance of the proposed
method by comparing with different competitive approaches, including four
state-of-the-art non-deep learning based methods, four latest deep learning
based methods and one ensemble learning based method, and prove its
effectiveness and superiority on three public datasets.",1734,50
['cs.CV'],Interpretability-Driven Sample Selection Using Self Supervised Learning For Disease Classification And Segmentation,"In supervised learning for medical image analysis, sample selection
methodologies are fundamental to attain optimum system performance promptly and
with minimal expert interactions (e.g. label querying in an active learning
setup). In this paper we propose a novel sample selection methodology based on
deep features leveraging information contained in interpretability saliency
maps. In the absence of ground truth labels for informative samples, we use a
novel self supervised learning based approach for training a classifier that
learns to identify the most informative sample in a given batch of images. We
demonstrate the benefits of the proposed approach, termed
Interpretability-Driven Sample Selection (IDEAL), in an active learning setup
aimed at lung disease classification and histopathology image segmentation. We
analyze three different approaches to determine sample informativeness from
interpretability saliency maps: (i) an observational model stemming from
findings on previous uncertainty-based sample selection approaches, (ii) a
radiomics-based model, and (iii) a novel data-driven self-supervised approach.
We compare IDEAL to other baselines using the publicly available NIH chest
X-ray dataset for lung disease classification, and a public histopathology
segmentation dataset (GLaS), demonstrating the potential of using
interpretability information for sample selection in active learning systems.
Results show our proposed self supervised approach outperforms other approaches
in selecting informative samples leading to state of the art performance with
fewer samples.",1596,115
['cs.CV'],Generative Adversarial Talking Head: Bringing Portraits to Life with a Weakly Supervised Neural Network,"This paper presents Generative Adversarial Talking Head (GATH), a novel deep
generative neural network that enables fully automatic facial expression
synthesis of an arbitrary portrait with continuous action unit (AU)
coefficients. Specifically, our model directly manipulates image pixels to make
the unseen subject in the still photo express various emotions controlled by
values of facial AU coefficients, while maintaining her personal
characteristics, such as facial geometry, skin color and hair style, as well as
the original surrounding background. In contrast to prior work, GATH is purely
data-driven and it requires neither a statistical face model nor image
processing tricks to enact facial deformations. Additionally, our model is
trained from unpaired data, where the input image, with its auxiliary identity
label taken from abundance of still photos in the wild, and the target frame
are from different persons. In order to effectively learn such model, we
propose a novel weakly supervised adversarial learning framework that consists
of a generator, a discriminator, a classifier and an action unit estimator. Our
work gives rise to template-and-target-free expression editing, where still
faces can be effortlessly animated with arbitrary AU coefficients provided by
the user.",1296,103
['cs.CV'],Single-Camera Basketball Tracker through Pose and Semantic Feature Fusion,"Tracking sports players is a widely challenging scenario, specially in
single-feed videos recorded in tight courts, where cluttering and occlusions
cannot be avoided. This paper presents an analysis of several geometric and
semantic visual features to detect and track basketball players. An ablation
study is carried out and then used to remark that a robust tracker can be built
with Deep Learning features, without the need of extracting contextual ones,
such as proximity or color similarity, nor applying camera stabilization
techniques. The presented tracker consists of: (1) a detection step, which uses
a pretrained deep learning model to estimate the players pose, followed by (2)
a tracking step, which leverages pose and semantic information from the output
of a convolutional layer in a VGG network. Its performance is analyzed in terms
of MOTA over a basketball dataset with more than 10k instances.",912,73
['cs.CV'],Zero-Shot Object Detection by Hybrid Region Embedding,"Object detection is considered as one of the most challenging problems in
computer vision, since it requires correct prediction of both classes and
locations of objects in images. In this study, we define a more difficult
scenario, namely zero-shot object detection (ZSD) where no visual training data
is available for some of the target object classes. We present a novel approach
to tackle this ZSD problem, where a convex combination of embeddings are used
in conjunction with a detection framework. For evaluation of ZSD methods, we
propose a simple dataset constructed from Fashion-MNIST images and also a
custom zero-shot split for the Pascal VOC detection challenge. The experimental
results suggest that our method yields promising results for ZSD.",756,53
"['cs.LG', 'cs.AI']",Spectral Temporal Graph Neural Network for Multivariate Time-series Forecasting,"Multivariate time-series forecasting plays a crucial role in many real-world
applications. It is a challenging problem as one needs to consider both
intra-series temporal correlations and inter-series correlations
simultaneously. Recently, there have been multiple works trying to capture both
correlations, but most, if not all of them only capture temporal correlations
in the time domain and resort to pre-defined priors as inter-series
relationships.
  In this paper, we propose Spectral Temporal Graph Neural Network (StemGNN) to
further improve the accuracy of multivariate time-series forecasting. StemGNN
captures inter-series correlations and temporal dependencies \textit{jointly}
in the \textit{spectral domain}. It combines Graph Fourier Transform (GFT)
which models inter-series correlations and Discrete Fourier Transform (DFT)
which models temporal dependencies in an end-to-end framework. After passing
through GFT and DFT, the spectral representations hold clear patterns and can
be predicted effectively by convolution and sequential learning modules.
Moreover, StemGNN learns inter-series correlations automatically from the data
without using pre-defined priors. We conduct extensive experiments on ten
real-world datasets to demonstrate the effectiveness of StemGNN. Code is
available at https://github.com/microsoft/StemGNN/",1346,79
"['cs.LG', 'stat.ML']",Semi-Implicit Graph Variational Auto-Encoders,"Semi-implicit graph variational auto-encoder (SIG-VAE) is proposed to expand
the flexibility of variational graph auto-encoders (VGAE) to model graph data.
SIG-VAE employs a hierarchical variational framework to enable neighboring node
sharing for better generative modeling of graph dependency structure, together
with a Bernoulli-Poisson link decoder. Not only does this hierarchical
construction provide a more flexible generative graph model to better capture
real-world graph properties, but also does SIG-VAE naturally lead to
semi-implicit hierarchical variational inference that allows faithful modeling
of implicit posteriors of given graph data, which may exhibit heavy tails,
multiple modes, skewness, and rich dependency structures. Compared to VGAE, the
derived graph latent representations by SIG-VAE are more interpretable, due to
more expressive generative model and more faithful inference enabled by the
flexible semi-implicit construction. Extensive experiments with a variety of
graph data show that SIG-VAE significantly outperforms state-of-the-art methods
on several different graph analytic tasks.",1121,45
"['cs.CV', 'cs.LG']",Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving,"We present an end-to-end method for object detection and trajectory
prediction utilizing multi-view representations of LiDAR returns. Our method
builds on a state-of-the-art Bird's-Eye View (BEV) network that fuses voxelized
features from a sequence of historical LiDAR data as well as rasterized
high-definition map to perform detection and prediction tasks. We extend the
BEV network with additional LiDAR Range-View (RV) features that use the raw
LiDAR information in its native, non-quantized representation. The RV feature
map is projected into BEV and fused with the BEV features computed from LiDAR
and high-definition map. The fused features are then further processed to
output the final detections and trajectories, within a single end-to-end
trainable network. In addition, using this framework the RV fusion of LiDAR and
camera is performed in a straightforward and computational efficient manner.
The proposed approach improves the state-of-the-art on proprietary large-scale
real-world data collected by a fleet of self-driving vehicles, as well as on
the public nuScenes data set.",1095,93
['cs.CV'],Seuillage par hystérésis pour le test de photo-consistance des voxels dans le cadre de la reconstruction 3D,"Voxel coloring is a popular method of reconstructing a three-dimensional
surface model from a set of calibrated 2D images. However, the reconstruction
quality is largely dependent on a thresholding procedure allowing the authors
to decide, for each voxel, whether it is photo-consistent or not. Even so, this
method is widely used because of its simplicity and low computational cost. We
have returned to this method in order to propose an improvement in the
thresholding step which will be fully automated. Indeed, the geometrical
information is implicitly integrated using an hysteresis thresholding which
takes into account the spatial coherence of color voxels. Moreover, the
ambiguity of choosing the thresholds is extremely minimized by defining a fuzzy
degree of membership of each voxel into the class of consistent voxels. Also,
there is no need for preset thresholds since the hysteresis ones are defined
automatically and adaptively depending on the number of images that the voxel
isprojected onto. Preliminary results are very promising and demonstrate that
the proposed method performs automatically precise and smooth volumetric scene
reconstruction.",1165,107
"['cs.CV', 'cs.DB', 'cs.MM']",Changing Fashion Cultures,"The paper presents a novel concept that analyzes and visualizes worldwide
fashion trends. Our goal is to reveal cutting-edge fashion trends without
displaying an ordinary fashion style. To achieve the fashion-based analysis, we
created a new fashion culture database (FCDB), which consists of 76 million
geo-tagged images in 16 cosmopolitan cities. By grasping a fashion trend of
mixed fashion styles,the paper also proposes an unsupervised fashion trend
descriptor (FTD) using a fashion descriptor, a codeword vetor, and temporal
analysis. To unveil fashion trends in the FCDB, the temporal analysis in FTD
effectively emphasizes consecutive features between two different times. In
experiments, we clearly show the analysis of fashion trends and fashion-based
city similarity. As the result of large-scale data collection and an
unsupervised analyzer, the proposed approach achieves world-level fashion
visualization in a time series. The code, model, and FCDB will be publicly
available after the construction of the project page.",1033,25
['cs.CV'],Privacy-Preserving Deep Action Recognition: An Adversarial Learning Framework and A New Dataset,"We investigate privacy-preserving, video-based action recognition in deep
learning, a problem with growing importance in smart camera applications. A
novel adversarial training framework is formulated to learn an anonymization
transform for input videos such that the trade-off between target utility task
performance and the associated privacy budgets is explicitly optimized on the
anonymized videos. Notably, the privacy budget, often defined and measured in
task-driven contexts, cannot be reliably indicated using any single model
performance because strong protection of privacy should sustain against any
malicious model that tries to steal private information. To tackle this
problem, we propose two new optimization strategies of model restarting and
model ensemble to achieve stronger universal privacy protection against any
attacker models. Extensive experiments have been carried out and analyzed. On
the other hand, given few public datasets available with both utility and
privacy labels, the data-driven (supervised) learning cannot exert its full
power on this task. We first discuss an innovative heuristic of cross-dataset
training and evaluation, enabling the use of multiple single-task datasets (one
with target task labels and the other with privacy labels) in our problem. To
further address this dataset challenge, we have constructed a new dataset,
termed PA-HMDB51, with both target task labels (action) and selected privacy
attributes (skin color, face, gender, nudity, and relationship) annotated on a
per-frame basis. This first-of-its-kind video dataset and evaluation protocol
can greatly facilitate visual privacy research and open up other opportunities.
Our codes, models, and the PA-HMDB51 dataset are available at
https://github.com/VITA-Group/PA-HMDB51.",1791,95
"['cs.CV', 'cs.IR', 'math.ST', 'stat.CO', 'stat.TH']",Accurate shape and phase averaging of time series through Dynamic Time Warping,"We propose a novel time series averaging method based on Dynamic Time Warping
(DTW). In contrast to previous methods, our algorithm preserves durational
information and the distinctive durational features of the sequences due to a
simple conversion of the output of DTW into a time sequence and an innovative
iterative averaging process. We show that it accurately estimates the ground
truth mean sequences and mean temporal location of landmarks in synthetic and
real-world datasets and outperforms state-of-the-art methods.",525,78
['cs.CV'],UP-DETR: Unsupervised Pre-training for Object Detection with Transformers,"Object detection with transformers (DETR) reaches competitive performance
with Faster R-CNN via a transformer encoder-decoder architecture. Inspired by
the great success of pre-training transformers in natural language processing,
we propose a pretext task named random query patch detection to Unsupervisedly
Pre-train DETR (UP-DETR) for object detection. Specifically, we randomly crop
patches from the given image and then feed them as queries to the decoder. The
model is pre-trained to detect these query patches from the original image.
During the pre-training, we address two critical issues: multi-task learning
and multi-query localization. (1) To trade off classification and localization
preferences in the pretext task, we freeze the CNN backbone and propose a patch
feature reconstruction branch which is jointly optimized with patch detection.
(2) To perform multi-query localization, we introduce UP-DETR from single-query
patch and extend it to multi-query patches with object query shuffle and
attention mask. In our experiments, UP-DETR significantly boosts the
performance of DETR with faster convergence and higher average precision on
object detection, one-shot detection and panoptic segmentation. Code and
pre-training models: https://github.com/dddzg/up-detr.",1283,73
['cs.CV'],Felzenszwalb-Baum-Welch: Event Detection by Changing Appearance,"We propose a method which can detect events in videos by modeling the change
in appearance of the event participants over time. This method makes it
possible to detect events which are characterized not by motion, but by the
changing state of the people or objects involved. This is accomplished by using
object detectors as output models for the states of a hidden Markov model
(HMM). The method allows an HMM to model the sequence of poses of the event
participants over time, and is effective for poses of humans and inanimate
objects. The ability to use existing object-detection methods as part of an
event model makes it possible to leverage ongoing work in the object-detection
community. A novel training method uses an EM loop to simultaneously learn the
temporal structure and object models automatically, without the need to specify
either the individual poses to be modeled or the frames in which they occur.
The E-step estimates the latent assignment of video frames to HMM states, while
the M-step estimates both the HMM transition probabilities and state output
models, including the object detectors, which are trained on the weighted
subset of frames assigned to their state. A new dataset was gathered because
little work has been done on events characterized by changing object pose, and
suitable datasets are not available. Our method produced results superior to
that of comparison systems on this dataset.",1427,63
['cs.CV'],Multimodal Semantic Scene Graphs for Holistic Modeling of Surgical Procedures,"From a computer science viewpoint, a surgical domain model needs to be a
conceptual one incorporating both behavior and data. It should therefore model
actors, devices, tools, their complex interactions and data flow. To capture
and model these, we take advantage of the latest computer vision methodologies
for generating 3D scene graphs from camera views. We then introduce the
Multimodal Semantic Scene Graph (MSSG) which aims at providing a unified
symbolic, spatiotemporal and semantic representation of surgical procedures.
This methodology aims at modeling the relationship between different components
in surgical domain including medical staff, imaging systems, and surgical
devices, opening the path towards holistic understanding and modeling of
surgical procedures. We then use MSSG to introduce a dynamically generated
graphical user interface tool for surgical procedure analysis which could be
used for many applications including process optimization, OR design and
automatic report generation. We finally demonstrate that the proposed MSSGs
could also be used for synchronizing different complex surgical procedures.
While the system still needs to be integrated into real operating rooms before
getting validated, this conference paper aims mainly at providing the community
with the basic principles of this novel concept through a first prototypal
partial realization based on MVOR dataset.",1410,77
"['cs.CV', 'cs.LG']",Semi-weakly Supervised Contrastive Representation Learning for Retinal Fundus Images,"We explore the value of weak labels in learning transferable representations
for medical images. Compared to hand-labeled datasets, weak or inexact labels
can be acquired in large quantities at significantly lower cost and can provide
useful training signals for data-hungry models such as deep neural networks. We
consider weak labels in the form of pseudo-labels and propose a semi-weakly
supervised contrastive learning (SWCL) framework for representation learning
using semi-weakly annotated images. Specifically, we train a semi-supervised
model to propagate labels from a small dataset consisting of diverse
image-level annotations to a large unlabeled dataset. Using the propagated
labels, we generate a patch-level dataset for pretraining and formulate a
multi-label contrastive learning objective to capture position-specific
features encoded in each patch. We empirically validate the transfer learning
performance of SWCL on seven public retinal fundus datasets, covering three
disease classification tasks and two anatomical structure segmentation tasks.
Our experiment results suggest that, under very low data regime, large-scale
ImageNet pretraining on improved architecture remains a very strong baseline,
and recently proposed self-supervised methods falter in segmentation tasks,
possibly due to the strong invariant constraint imposed. Our method surpasses
all prior self-supervised methods and standard cross-entropy training, while
closing the gaps with ImageNet pretraining.",1496,84
"['cs.LG', 'stat.ML']",Diversity-Driven Extensible Hierarchical Reinforcement Learning,"Hierarchical reinforcement learning (HRL) has recently shown promising
advances on speeding up learning, improving the exploration, and discovering
intertask transferable skills. Most recent works focus on HRL with two levels,
i.e., a master policy manipulates subpolicies, which in turn manipulate
primitive actions. However, HRL with multiple levels is usually needed in many
real-world scenarios, whose ultimate goals are highly abstract, while their
actions are very primitive. Therefore, in this paper, we propose a
diversity-driven extensible HRL (DEHRL), where an extensible and scalable
framework is built and learned levelwise to realize HRL with multiple levels.
DEHRL follows a popular assumption: diverse subpolicies are useful, i.e.,
subpolicies are believed to be more useful if they are more diverse. However,
existing implementations of this diversity assumption usually have their own
drawbacks, which makes them inapplicable to HRL with multiple levels.
Consequently, we further propose a novel diversity-driven solution to achieve
this assumption in DEHRL. Experimental studies evaluate DEHRL with five
baselines from four perspectives in two domains; the results show that DEHRL
outperforms the state-of-the-art baselines in all four aspects.",1262,63
['cs.CV'],Guaranteed Parameter Estimation for Discrete Energy Minimization,"Structural learning, a method to estimate the parameters for discrete energy
minimization, has been proven to be effective in solving computer vision
problems, especially in 3D scene parsing. As the complexity of the models
increases, structural learning algorithms turn to approximate inference to
retain tractability. Unfortunately, such methods often fail because the
approximation can be arbitrarily poor. In this work, we propose a method to
overcome this limitation through exploiting the properties of the joint problem
of training time inference and learning. With the help of the learning
framework, we transform the inapproximable inference problem into a polynomial
time solvable one, thereby enabling tractable exact inference while still
allowing an arbitrary graph structure and full potential interactions. Our
learning algorithm is guaranteed to return a solution with a bounded error to
the global optimal within the feasible parameter space. We demonstrate the
effectiveness of this method on two point cloud scene parsing datasets. Our
approach runs much faster and solves a problem that is intractable for
previous, well-known approaches.",1158,64
['cs.LG'],RL for Latent MDPs: Regret Guarantees and a Lower Bound,"In this work, we consider the regret minimization problem for reinforcement
learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is
randomly drawn from a set of $M$ possible MDPs at the beginning of the
interaction, but the identity of the chosen MDP is not revealed to the agent.
We first show that a general instance of LMDPs requires at least
$\Omega((SA)^M)$ episodes to even approximate the optimal policy. Then, we
consider sufficient assumptions under which learning good policies requires
polynomial number of episodes. We show that the key link is a notion of
separation between the MDP system dynamics. With sufficient separation, we
provide an efficient algorithm with local guarantee, {\it i.e.,} providing a
sublinear regret guarantee when we are given a good initialization. Finally, if
we are given standard statistical sufficiency assumptions common in the
Predictive State Representation (PSR) literature (e.g., Boots et al.) and a
reachability assumption, we show that the need for initialization can be
removed.",1049,55
"['cs.LG', 'stat.ML']",CAUSE: Learning Granger Causality from Event Sequences using Attribution Methods,"We study the problem of learning Granger causality between event types from
asynchronous, interdependent, multi-type event sequences. Existing work suffers
from either limited model flexibility or poor model explainability and thus
fails to uncover Granger causality across a wide variety of event sequences
with diverse event interdependency. To address these weaknesses, we propose
CAUSE (Causality from AttribUtions on Sequence of Events), a novel framework
for the studied task. The key idea of CAUSE is to first implicitly capture the
underlying event interdependency by fitting a neural point process, and then
extract from the process a Granger causality statistic using an axiomatic
attribution method. Across multiple datasets riddled with diverse event
interdependency, we demonstrate that CAUSE achieves superior performance on
correctly inferring the inter-type Granger causality over a range of
state-of-the-art methods.",933,80
"['cs.LG', 'cs.AI']",Learning Memory-Dependent Continuous Control from Demonstrations,"Efficient exploration has presented a long-standing challenge in
reinforcement learning, especially when rewards are sparse. A developmental
system can overcome this difficulty by learning from both demonstrations and
self-exploration. However, existing methods are not applicable to most
real-world robotic controlling problems because they assume that environments
follow Markov decision processes (MDP); thus, they do not extend to partially
observable environments where historical observations are necessary for
decision making. This paper builds on the idea of replaying demonstrations for
memory-dependent continuous control, by proposing a novel algorithm, Recurrent
Actor-Critic with Demonstration and Experience Replay (READER). Experiments
involving several memory-crucial continuous control tasks reveal significantly
reduce interactions with the environment using our method with a reasonably
small number of demonstration samples. The algorithm also shows better sample
efficiency and learning capabilities than a baseline reinforcement learning
algorithm for memory-based control from demonstrations.",1115,64
"['cs.LG', 'cs.AI', 'cs.LO']",Scaling Guarantees for Nearest Counterfactual Explanations,"Counterfactual explanations (CFE) are being widely used to explain
algorithmic decisions, especially in consequential decision-making contexts
(e.g., loan approval or pretrial bail). In this context, CFEs aim to provide
individuals affected by an algorithmic decision with the most similar
individual (i.e., nearest individual) with a different outcome. However, while
an increasing number of works propose algorithms to compute CFEs, such
approaches either lack in optimality of distance (i.e., they do not return the
nearest individual) and perfect coverage (i.e., they do not provide a CFE for
all individuals); or they cannot handle complex models, such as neural
networks. In this work, we provide a framework based on Mixed-Integer
Programming (MIP) to compute nearest counterfactual explanations with provable
guarantees and with runtimes comparable to gradient-based approaches. Our
experiments on the Adult, COMPAS, and Credit datasets show that, in contrast
with previous methods, our approach allows for efficiently computing diverse
CFEs with both distance guarantees and perfect coverage.",1101,58
['cs.CV'],TiVGAN: Text to Image to Video Generation with Step-by-Step Evolutionary Generator,"Advances in technology have led to the development of methods that can create
desired visual multimedia. In particular, image generation using deep learning
has been extensively studied across diverse fields. In comparison, video
generation, especially on conditional inputs, remains a challenging and less
explored area. To narrow this gap, we aim to train our model to produce a video
corresponding to a given text description. We propose a novel training
framework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN),
which evolves frame-by-frame and finally produces a full-length video. In the
first phase, we focus on creating a high-quality single video frame while
learning the relationship between the text and an image. As the steps proceed,
our model is trained gradually on more number of consecutive frames.This
step-by-step learning process helps stabilize the training and enables the
creation of high-resolution video based on conditional text descriptions.
Qualitative and quantitative experimental results on various datasets
demonstrate the effectiveness of the proposed method.",1109,82
"['cs.LG', 'cs.FL']",Mapping Monotonic Restrictions in Inductive Inference,"In language learning in the limit we investigate computable devices
(learners) learning formal languages. Through the years, many natural
restrictions have been imposed on the studied learners. As such, monotonic
restrictions always enjoyed particular attention as, although being a natural
requirement, monotonic learners show significantly diverse behaviour when
studied in different settings. A recent study thoroughly analysed the learning
capabilities of strongly monotone learners imposed with memory restrictions and
various additional requirements. The unveiled differences between explanatory
and behaviourally correct such learners motivate our studies of monotone
learners dealing with the same restrictions.
  We reveal differences and similarities between monotone learners and their
strongly monotone counterpart when studied with various additional
restrictions. In particular, we show that explanatory monotone learners,
although known to be strictly stronger, do (almost) preserve the pairwise
relation as seen in strongly monotone learning. Contrasting this similarity, we
find substantial differences when studying behaviourally correct monotone
learners. Most notably, we show that monotone learners, as opposed to their
strongly monotone counterpart, do heavily rely on the order the information is
given in, an unusual result for behaviourally correct learners.",1383,53
"['cs.LG', 'cs.AI']",Robust Reinforcement Learning under model misspecification,"Reinforcement learning has achieved remarkable performance in a wide range of
tasks these days. Nevertheless, some unsolved problems limit its applications
in real-world control. One of them is model misspecification, a situation where
an agent is trained and deployed in environments with different transition
dynamics. We propose an novel framework that utilize history trajectory and
Partial Observable Markov Decision Process Modeling to deal with this dilemma.
Additionally, we put forward an efficient adversarial attack method to assist
robust training. Our experiments in four gym domains validate the effectiveness
of our framework.",641,58
['cs.CV'],Vision-Based Fall Event Detection in Complex Background Using Attention Guided Bi-directional LSTM,"Fall event detection, as one of the greatest risks to the elderly, has been a
hot research issue in the solitary scene in recent years. Nevertheless, there
are few researches on the fall event detection in complex background. Different
from most conventional background subtraction methods which depend on
background modeling, Mask R-CNN method based on deep learning technique can
clearly extract the moving object in noise background. We further propose an
attention guided Bi-directional LSTM model for the final fall event detection.
To demonstrate the efficiency, the proposed method is verified in the public
dataset and self-build dataset. Evaluation of the algorithm performances in
comparison with other state-of-the-art methods indicates that the proposed
design is accurate and robust, which means it is suitable for the task of fall
event detection in complex situation.",882,98
['cs.CV'],Iris-GAN: Learning to Generate Realistic Iris Images Using Convolutional GAN,"Generating iris images which look realistic is both an interesting and
challenging problem. Most of the classical statistical models are not powerful
enough to capture the complicated texture representation in iris images, and
therefore fail to generate iris images which look realistic. In this work, we
present a machine learning framework based on generative adversarial network
(GAN), which is able to generate iris images sampled from a prior distribution
(learned from a set of training images). We apply this framework to two popular
iris databases, and generate images which look very realistic, and similar to
the image distribution in those databases. Through experimental results, we
show that the generated iris images have a good diversity, and are able to
capture different part of the prior distribution.",819,76
"['cs.LG', 'stat.ML']",Graph Convolution: A High-Order and Adaptive Approach,"In this paper, we presented a novel convolutional neural network framework
for graph modeling, with the introduction of two new modules specially designed
for graph-structured data: the $k$-th order convolution operator and the
adaptive filtering module. Importantly, our framework of High-order and
Adaptive Graph Convolutional Network (HA-GCN) is a general-purposed
architecture that fits various applications on both node and graph centrics, as
well as graph generative models. We conducted extensive experiments on
demonstrating the advantages of our framework. Particularly, our HA-GCN
outperforms the state-of-the-art models on node classification and molecule
property prediction tasks. It also generates 32% more real molecules on the
molecule generation task, both of which will significantly benefit real-world
applications such as material design and drug screening.",877,53
['cs.CV'],Learning to Discover Novel Visual Categories via Deep Transfer Clustering,"We consider the problem of discovering novel object categories in an image
collection. While these images are unlabelled, we also assume prior knowledge
of related but different image classes. We use such prior knowledge to reduce
the ambiguity of clustering, and improve the quality of the newly discovered
classes. Our contributions are twofold. The first contribution is to extend
Deep Embedded Clustering to a transfer learning setting; we also improve the
algorithm by introducing a representation bottleneck, temporal ensembling, and
consistency. The second contribution is a method to estimate the number of
classes in the unlabelled data. This also transfers knowledge from the known
classes, using them as probes to diagnose different choices for the number of
classes in the unlabelled subset. We thoroughly evaluate our method,
substantially outperforming state-of-the-art techniques in a large number of
benchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN.",988,73
['cs.CV'],Coarse-to-Fine Salient Object Detection with Low-Rank Matrix Recovery,"Low-Rank Matrix Recovery (LRMR) has recently been applied to saliency
detection by decomposing image features into a low-rank component associated
with background and a sparse component associated with visual salient regions.
Despite its great potential, existing LRMR-based saliency detection methods
seldom consider the inter-relationship among elements within these two
components, thus are prone to generating scattered or incomplete saliency maps.
In this paper, we introduce a novel and efficient LRMR-based saliency detection
model under a coarse-to-fine framework to circumvent this limitation. First, we
roughly measure the saliency of image regions with a baseline LRMR model that
integrates a $\ell_1$-norm sparsity constraint and a Laplacian regularization
smooth term. Given samples from the coarse saliency map, we then learn a
projection that maps image features to refined saliency values, to
significantly sharpen the object boundaries and to preserve the object
entirety. We evaluate our framework against existing LRMR-based methods on
three benchmark datasets. Experimental results validate the superiority of our
method as well as the effectiveness of our suggested coarse-to-fine framework,
especially for images containing multiple objects.",1263,69
['cs.CV'],P2B: Point-to-Box Network for 3D Object Tracking in Point Clouds,"Towards 3D object tracking in point clouds, a novel point-to-box network
termed P2B is proposed in an end-to-end learning manner. Our main idea is to
first localize potential target centers in 3D search area embedded with target
information. Then point-driven 3D target proposal and verification are executed
jointly. In this way, the time-consuming 3D exhaustive search can be avoided.
Specifically, we first sample seeds from the point clouds in template and
search area respectively. Then, we execute permutation-invariant feature
augmentation to embed target clues from template into search area seeds and
represent them with target-specific features. Consequently, the augmented
search area seeds regress the potential target centers via Hough voting. The
centers are further strengthened with seed-wise targetness scores. Finally,
each center clusters its neighbors to leverage the ensemble power for joint 3D
target proposal and verification. We apply PointNet++ as our backbone and
experiments on KITTI tracking dataset demonstrate P2B's superiority (~10%'s
improvement over state-of-the-art). Note that P2B can run with 40FPS on a
single NVIDIA 1080Ti GPU. Our code and model are available at
https://github.com/HaozheQi/P2B.",1234,64
"['cs.LG', 'cs.SY', 'eess.SY', 'stat.ML']",Tightening Exploration in Upper Confidence Reinforcement Learning,"The upper confidence reinforcement learning (UCRL2) algorithm introduced in
(Jaksch et al., 2010) is a popular method to perform regret minimization in
unknown discrete Markov Decision Processes under the average-reward criterion.
Despite its nice and generic theoretical regret guarantees, this algorithm and
its variants have remained until now mostly theoretical as numerical
experiments in simple environments exhibit long burn-in phases before the
learning takes place. In pursuit of practical efficiency, we present UCRL3,
following the lines of UCRL2, but with two key modifications: First, it uses
state-of-the-art time-uniform concentration inequalities to compute confidence
sets on the reward and (component-wise) transition distributions for each
state-action pair. Furthermore, to tighten exploration, it uses an adaptive
computation of the support of each transition distribution, which in turn
enables us to revisit the extended value iteration procedure of UCRL2 to
optimize over distributions with reduced support by disregarding low
probability transitions, while still ensuring near-optimism. We demonstrate,
through numerical experiments in standard environments, that reducing
exploration this way yields a substantial numerical improvement compared to
UCRL2 and its variants. On the theoretical side, these key modifications enable
us to derive a regret bound for UCRL3 improving on UCRL2, that for the first
time makes appear notions of local diameter and local effective support, thanks
to variance-aware concentration bounds.",1550,65
['cs.CV'],Fingerprint Spoof Buster,"The primary purpose of a fingerprint recognition system is to ensure a
reliable and accurate user authentication, but the security of the recognition
system itself can be jeopardized by spoof attacks. This study addresses the
problem of developing accurate, generalizable, and efficient algorithms for
detecting fingerprint spoof attacks. Specifically, we propose a deep
convolutional neural network based approach utilizing local patches centered
and aligned using fingerprint minutiae. Experimental results on three
public-domain LivDet datasets (2011, 2013, and 2015) show that the proposed
approach provides state-of-the-art accuracies in fingerprint spoof detection
for intra-sensor, cross-material, cross-sensor, as well as cross-dataset
testing scenarios. For example, in LivDet 2015, the proposed approach achieves
99.03% average accuracy over all sensors compared to 95.51% achieved by the
LivDet 2015 competition winners. Additionally, two new fingerprint presentation
attack datasets containing more than 20,000 images, using two different
fingerprint readers, and over 12 different spoof fabrication materials are
collected. We also present a graphical user interface, called Fingerprint Spoof
Buster, that allows the operator to visually examine the local regions of the
fingerprint highlighted as live or spoof, instead of relying on only a single
score as output by the traditional approaches.",1408,24
"['cs.LG', 'stat.ML']",Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings,"In this paper, we propose an end-to-end graph learning framework, namely
Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning
graph structure and graph embedding. The key rationale of IDGL is to learn a
better graph structure based on better node embeddings, and vice versa (i.e.,
better node embeddings based on a better graph structure). Our iterative method
dynamically stops when the learned graph structure approaches close enough to
the graph optimized for the downstream prediction task. In addition, we cast
the graph learning problem as a similarity metric learning problem and leverage
adaptive graph regularization for controlling the quality of the learned graph.
Finally, combining the anchor-based approximation technique, we further propose
a scalable version of IDGL, namely IDGL-Anch, which significantly reduces the
time and space complexity of IDGL without compromising the performance. Our
extensive experiments on nine benchmarks show that our proposed IDGL models can
consistently outperform or match the state-of-the-art baselines. Furthermore,
IDGL can be more robust to adversarial graphs and cope with both transductive
and inductive learning.",1191,90
"['cs.CV', 'eess.IV']",Handheld Multi-Frame Super-Resolution,"Compared to DSLR cameras, smartphone cameras have smaller sensors, which
limits their spatial resolution; smaller apertures, which limits their light
gathering ability; and smaller pixels, which reduces their signal-to noise
ratio. The use of color filter arrays (CFAs) requires demosaicing, which
further degrades resolution. In this paper, we supplant the use of traditional
demosaicing in single-frame and burst photography pipelines with a multiframe
super-resolution algorithm that creates a complete RGB image directly from a
burst of CFA raw images. We harness natural hand tremor, typical in handheld
photography, to acquire a burst of raw frames with small offsets. These frames
are then aligned and merged to form a single image with red, green, and blue
values at every pixel site. This approach, which includes no explicit
demosaicing step, serves to both increase image resolution and boost signal to
noise ratio. Our algorithm is robust to challenging scene conditions: local
motion, occlusion, or scene changes. It runs at 100 milliseconds per
12-megapixel RAW input burst frame on mass-produced mobile phones.
Specifically, the algorithm is the basis of the Super-Res Zoom feature, as well
as the default merge method in Night Sight mode (whether zooming or not) on
Google's flagship phone.",1306,37
"['cs.CV', 'cs.AI', 'cs.LG']",3D for Free: Crossmodal Transfer Learning using HD Maps,"3D object detection is a core perceptual challenge for robotics and
autonomous driving. However, the class-taxonomies in modern autonomous driving
datasets are significantly smaller than many influential 2D detection datasets.
In this work, we address the long-tail problem by leveraging both the large
class-taxonomies of modern 2D datasets and the robustness of state-of-the-art
2D detection methods. We proceed to mine a large, unlabeled dataset of images
and LiDAR, and estimate 3D object bounding cuboids, seeded from an
off-the-shelf 2D instance segmentation model. Critically, we constrain this
ill-posed 2D-to-3D mapping by using high-definition maps and object size
priors. The result of the mining process is 3D cuboids with varying confidence.
This mining process is itself a 3D object detector, although not especially
accurate when evaluated as such. However, we then train a 3D object detection
model on these cuboids, consistent with other recent observations in the deep
learning literature, we find that the resulting model is fairly robust to the
noisy supervision that our mining process provides. We mine a collection of
1151 unlabeled, multimodal driving logs from an autonomous vehicle and use the
discovered objects to train a LiDAR-based object detector. We show that
detector performance increases as we mine more unlabeled data. With our full,
unlabeled dataset, our method performs competitively with fully supervised
methods, even exceeding the performance for certain object categories, without
any human 3D annotations.",1549,55
"['cs.CV', 'cs.IR', 'cs.SI']",HARRISON: A Benchmark on HAshtag Recommendation for Real-world Images in Social Networks,"Simple, short, and compact hashtags cover a wide range of information on
social networks. Although many works in the field of natural language
processing (NLP) have demonstrated the importance of hashtag recommendation,
hashtag recommendation for images has barely been studied. In this paper, we
introduce the HARRISON dataset, a benchmark on hashtag recommendation for real
world images in social networks. The HARRISON dataset is a realistic dataset,
composed of 57,383 photos from Instagram and an average of 4.5 associated
hashtags for each photo. To evaluate our dataset, we design a baseline
framework consisting of visual feature extractor based on convolutional neural
network (CNN) and multi-label classifier based on neural network. Based on this
framework, two single feature-based models, object-based and scene-based model,
and an integrated model of them are evaluated on the HARRISON dataset. Our
dataset shows that hashtag recommendation task requires a wide and contextual
understanding of the situation conveyed in the image. As far as we know, this
work is the first vision-only attempt at hashtag recommendation for real world
images in social networks. We expect this benchmark to accelerate the
advancement of hashtag recommendation.",1256,88
['cs.CV'],Effect of Super Resolution on High Dimensional Features for Unsupervised Face Recognition in the Wild,"Majority of the face recognition algorithms use query faces captured from
uncontrolled, in the wild, environment. Often caused by the cameras limited
capabilities, it is common for these captured facial images to be blurred or
low resolution. Super resolution algorithms are therefore crucial in improving
the resolution of such images especially when the image size is small requiring
enlargement. This paper aims to demonstrate the effect of one of the
state-of-the-art algorithms in the field of image super resolution. To
demonstrate the functionality of the algorithm, various before and after 3D
face alignment cases are provided using the images from the Labeled Faces in
the Wild (lfw). Resulting images are subject to testing on a closed set face
recognition protocol using unsupervised algorithms with high dimension
extracted features. The inclusion of super resolution algorithm resulted in
significant improved recognition rate over recently reported results obtained
from unsupervised algorithms.",1010,101
['cs.CV'],Deformable kernel networks for guided depth map upsampling,"We address the problem of upsampling a low-resolution (LR) depth map using a
registered high-resolution (HR) color image of the same scene. Previous methods
based on convolutional neural networks (CNNs) combine nonlinear activations of
spatially-invariant kernels to estimate structural details from LR depth and HR
color images, and regress upsampling results directly from the networks. In
this paper, we revisit the weighted averaging process that has been widely used
to transfer structural details from hand-crafted visual features to LR depth
maps. We instead learn explicitly sparse and spatially-variant kernels for this
task. To this end, we propose a CNN architecture and its efficient
implementation, called the deformable kernel network (DKN), that outputs sparse
sets of neighbors and the corresponding weights adaptively for each pixel. We
also propose a fast version of DKN (FDKN) that runs about 17 times faster (0.01
seconds for a HR image of size 640 x 480). Experimental results on standard
benchmarks demonstrate the effectiveness of our approach. In particular, we
show that the weighted averaging process with 3 x 3 kernels (i.e., aggregating
9 samples sparsely chosen) outperforms the state of the art by a significant
margin.",1249,58
['cs.CV'],Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision,"In this paper, we teach machines to understand visuals and natural language
by learning the mapping between sentences and noisy video snippets without
explicit annotations. Firstly, we define a self-supervised learning framework
that captures the cross-modal information. A novel adversarial learning module
is then introduced to explicitly handle the noises in the natural videos, where
the subtitle sentences are not guaranteed to be strongly corresponded to the
video snippets. For training and evaluation, we contribute a new dataset
`ApartmenTour' that contains a large number of online videos and subtitles. We
carry out experiments on the bidirectional retrieval tasks between sentences
and videos, and the results demonstrate that our proposed model achieves the
state-of-the-art performance on both retrieval tasks and exceeds several strong
baselines. The dataset can be downloaded at https://github.com/zyj-13/WAL.",925,83
"['cs.CV', 'cs.LG']",Geometrically Principled Connections in Graph Neural Networks,"Graph convolution operators bring the advantages of deep learning to a
variety of graph and mesh processing tasks previously deemed out of reach. With
their continued success comes the desire to design more powerful architectures,
often by adapting existing deep learning techniques to non-Euclidean data. In
this paper, we argue geometry should remain the primary driving force behind
innovation in the emerging field of geometric deep learning. We relate graph
neural networks to widely successful computer graphics and data approximation
models: radial basis functions (RBFs). We conjecture that, like RBFs, graph
convolution layers would benefit from the addition of simple functions to the
powerful convolution kernels. We introduce affine skip connections, a novel
building block formed by combining a fully connected layer with any graph
convolution operator. We experimentally demonstrate the effectiveness of our
technique and show the improved performance is the consequence of more than the
increased number of parameters. Operators equipped with the affine skip
connection markedly outperform their base performance on every task we
evaluated, i.e., shape reconstruction, dense shape correspondence, and graph
classification. We hope our simple and effective approach will serve as a solid
baseline and help ease future research in graph neural networks.",1366,61
"['cs.LG', 'stat.ML']",Investigating Estimated Kolmogorov Complexity as a Means of Regularization for Link Prediction,"Link prediction in graphs is an important task in the fields of network
science and machine learning. We investigate a flexible means of regularization
for link prediction based on an approximation of the Kolmogorov complexity of
graphs that is differentiable and compatible with recent advances in link
prediction algorithms. Informally, the Kolmogorov complexity of an object is
the length of the shortest computer program that produces the object. Complex
networks are often generated, in part, by simple mechanisms; for example, many
citation networks and social networks are approximately scale-free and can be
explained by preferential attachment. A preference for predicting graphs with
simpler generating mechanisms motivates our choice of Kolmogorov complexity as
a regularization term. In our experiments the regularization method shows good
performance on many diverse real-world networks, however we determine that this
is likely due to an aggregation method rather than any actual estimation of
Kolmogorov complexity.",1030,94
"['cs.CV', 'cs.LG', 'cs.MM']",Self Paced Adversarial Training for Multimodal Few-shot Learning,"State-of-the-art deep learning algorithms yield remarkable results in many
visual recognition tasks. However, they still fail to provide satisfactory
results in scarce data regimes. To a certain extent this lack of data can be
compensated by multimodal information. Missing information in one modality of a
single data point (e.g. an image) can be made up for in another modality (e.g.
a textual description). Therefore, we design a few-shot learning task that is
multimodal during training (i.e. image and text) and single-modal during test
time (i.e. image). In this regard, we propose a self-paced class-discriminative
generative adversarial network incorporating multimodality in the context of
few-shot learning. The proposed approach builds upon the idea of cross-modal
data generation in order to alleviate the data sparsity problem. We improve
few-shot learning accuracies on the finegrained CUB and Oxford-102 datasets.",928,64
"['cs.LG', 'cs.IR', 'stat.ML', '68T05']",Sequential Variational Autoencoders for Collaborative Filtering,"Variational autoencoders were proven successful in domains such as computer
vision and speech processing. Their adoption for modeling user preferences is
still unexplored, although recently it is starting to gain attention in the
current literature. In this work, we propose a model which extends variational
autoencoders by exploiting the rich information present in the past preference
history. We introduce a recurrent version of the VAE, where instead of passing
a subset of the whole history regardless of temporal dependencies, we rather
pass the consumption sequence subset through a recurrent neural network. At
each time-step of the RNN, the sequence is fed through a series of
fully-connected layers, the output of which models the probability distribution
of the most likely future preferences. We show that handling temporal
information is crucial for improving the accuracy of the VAE: In fact, our
model beats the current state-of-the-art by valuable margins because of its
ability to capture temporal dependencies among the user-consumption sequence
using the recurrent encoder still keeping the fundamentals of variational
autoencoders intact.",1159,63
"['cs.LG', 'stat.ML']",Efficient logic architecture in training gradient boosting decision tree for high-performance and edge computing,"This study proposes a logic architecture for the high-speed and power
efficiently training of a gradient boosting decision tree model of binary
classification. We implemented the proposed logic architecture on an FPGA and
compared training time and power efficiency with three general GBDT software
libraries using CPU and GPU. The training speed of the logic architecture on
the FPGA was 26-259 times faster than the software libraries. The power
efficiency of the logic architecture was 90-1,104 times higher than the
software libraries. The results show that the logic architecture suits for
high-performance and edge computing.",631,112
['cs.LG'],High-Performance FPGA-based Accelerator for Bayesian Recurrent Neural Networks,"Neural networks have demonstrated their great performance in a wide range of
tasks. Especially in time-series analysis, recurrent architectures based on
long-short term memory (LSTM) cells have manifested excellent capability to
model time dependencies in real-world data. However, standard recurrent
architectures cannot estimate their uncertainty which is essential for
safety-critical applications such as in medicine. In contrast, Bayesian
recurrent neural networks (RNNs) are able to provide uncertainty estimation
with improved accuracy. Nonetheless, Bayesian RNNs are computationally and
memory demanding, which limits their practicality despite their advantages. To
address this issue, we propose an FPGA-based hardware design to accelerate
Bayesian LSTM-based RNNs. To further improve the overall algorithmic-hardware
performance, a co-design framework is proposed to explore the most optimal
algorithmic-hardware configurations for Bayesian RNNs. We conduct extensive
experiments on health-related tasks to demonstrate the improvement of our
design and the effectiveness of our framework. Compared with GPU
implementation, our FPGA-based design can achieve up to 10 times speedup with
nearly 106 times higher energy efficiency. To the best of our knowledge, this
is the first work targeting the acceleration of Bayesian RNNs on FPGAs.",1344,78
['cs.CV'],Generating superpixels using deep image representations,"Superpixel algorithms are a common pre-processing step for computer vision
algorithms such as segmentation, object tracking and localization. Many
superpixel methods only rely on colors features for segmentation, limiting
performance in low-contrast regions and applicability to infrared or medical
images where object boundaries have wide appearance variability. We study the
inclusion of deep image features in the SLIC superpixel algorithm to exploit
higher-level image representations. In addition, we devise a trainable
superpixel algorithm, yielding an intermediate domain-specific image
representation that can be applied to different tasks. A clustering-based
superpixel algorithm is transformed into a pixel-wise classification task and
superpixel training data is derived from semantic segmentation datasets. Our
results demonstrate that this approach is able to improve superpixel quality
consistently.",913,55
"['cs.CV', 'cs.IR']",Symmetrical Synthesis for Deep Metric Learning,"Deep metric learning aims to learn embeddings that contain semantic
similarity information among data points. To learn better embeddings, methods
to generate synthetic hard samples have been proposed. Existing methods of
synthetic hard sample generation are adopting autoencoders or generative
adversarial networks, but this leads to more hyper-parameters, harder
optimization, and slower training speed. In this paper, we address these
problems by proposing a novel method of synthetic hard sample generation called
symmetrical synthesis. Given two original feature points from the same class,
the proposed method firstly generates synthetic points with each other as an
axis of symmetry. Secondly, it performs hard negative pair mining within the
original and synthetic points to select a more informative negative pair for
computing the metric learning loss. Our proposed method is hyper-parameter free
and plug-and-play for existing metric learning losses without network
modification. We demonstrate the superiority of our proposed method over
existing methods for a variety of loss functions on clustering and image
retrieval tasks. Our implementations is publicly available.",1181,46
['cs.CV'],Face recognition using color local binary pattern from mutually independent color channels,"In this paper, a high performance face recognition system based on local
binary pattern (LBP) using the probability distribution functions (PDF) of
pixels in different mutually independent color channels which are robust to
frontal homogenous illumination and planer rotation is proposed. The
illumination of faces is enhanced by using the state-of-the-art technique which
is using discrete wavelet transform (DWT) and singular value decomposition
(SVD). After equalization, face images are segmented by use of local Successive
Mean Quantization Transform (SMQT) followed by skin color based face detection
system. Kullback-Leibler Distance (KLD) between the concatenated PDFs of a
given face obtained by LBP and the concatenated PDFs of each face in the
database is used as a metric in the recognition process. Various decision
fusion techniques have been used in order to improve the recognition rate. The
proposed system has been tested on the FERET, HP, and Bosphorus face databases.
The proposed system is compared with conventional and thestate-of-the-art
techniques. The recognition rates obtained using FVF approach for FERET
database is 99.78% compared with 79.60% and 68.80% for conventional gray scale
LBP and Principle Component Analysis (PCA) based face recognition techniques
respectively.",1303,90
"['cs.CV', 'cs.GR']",Learning to Generate Diverse Dance Motions with Transformer,"With the ongoing pandemic, virtual concerts and live events using digitized
performances of musicians are getting traction on massive multiplayer online
worlds. However, well choreographed dance movements are extremely complex to
animate and would involve an expensive and tedious production process. In
addition to the use of complex motion capture systems, it typically requires a
collaborative effort between animators, dancers, and choreographers. We
introduce a complete system for dance motion synthesis, which can generate
complex and highly diverse dance sequences given an input music sequence. As
motion capture data is limited for the range of dance motions and styles, we
introduce a massive dance motion data set that is created from YouTube videos.
We also present a novel two-stream motion transformer generative model, which
can generate motion sequences with high flexibility. We also introduce new
evaluation metrics for the quality of synthesized dance motions, and
demonstrate that our system can outperform state-of-the-art methods. Our system
provides high-quality animations suitable for large crowds for virtual concerts
and can also be used as reference for professional animation pipelines. Most
importantly, we show that vast online videos can be effective in training dance
motion models.",1316,59
['cs.CV'],RepPoints V2: Verification Meets Regression for Object Detection,"Verification and regression are two general methodologies for prediction in
neural networks. Each has its own strengths: verification can be easier to
infer accurately, and regression is more efficient and applicable to continuous
target variables. Hence, it is often beneficial to carefully combine them to
take advantage of their benefits. In this paper, we take this philosophy to
improve state-of-the-art object detection, specifically by RepPoints. Though
RepPoints provides high performance, we find that its heavy reliance on
regression for object localization leaves room for improvement. We introduce
verification tasks into the localization prediction of RepPoints, producing
RepPoints v2, which provides consistent improvements of about 2.0 mAP over the
original RepPoints on the COCO object detection benchmark using different
backbones and training methods. RepPoints v2 also achieves 52.1 mAP on COCO
\texttt{test-dev} by a single model. Moreover, we show that the proposed
approach can more generally elevate other object detection frameworks as well
as applications such as instance segmentation. The code is available at
https://github.com/Scalsol/RepPointsV2.",1177,64
['cs.CV'],LED2Net: Deep Illumination-aware Dehazing with Low-light and Detail Enhancement,"We present a novel dehazing and low-light enhancement method based on an
illumination map that is accurately estimated by a convolutional neural network
(CNN). In this paper, the illumination map is used as a component for three
different tasks, namely, atmospheric light estimation, transmission map
estimation, and low-light enhancement. To train CNNs for dehazing and low-light
enhancement simultaneously based on the retinex theory, we synthesize numerous
low-light and hazy images from normal hazy images from the FADE data set. In
addition, we further improve the network using detail enhancement. Experimental
results demonstrate that our method surpasses recent state-of-theart algorithms
quantitatively and qualitatively. In particular, our haze-free images present
vivid colors and enhance visibility without a halo effect or color distortion.",853,79
"['cs.CV', 'cs.SD', 'eess.AS', 'eess.IV']",A Realistic Face-to-Face Conversation System based on Deep Neural Networks,"To improve the experiences of face-to-face conversation with avatar, this
paper presents a novel conversation system. It is composed of two
sequence-to-sequence models respectively for listening and speaking and a
Generative Adversarial Network (GAN) based realistic avatar synthesizer. The
models exploit the facial action and head pose to learn natural human
reactions. Based on the models' output, the synthesizer uses the Pixel2Pixel
model to generate realistic facial images. To show the improvement of our
system, we use a 3D model based avatar driving scheme as a reference. We train
and evaluate our neural networks with the data from ESPN shows. Experimental
results show that our conversation system can generate natural facial reactions
and realistic facial images.",776,74
"['cs.CV', 'eess.IV']",Double-Uncertainty Assisted Spatial and Temporal Regularization Weighting for Learning-based Registration,"In order to tackle the difficulty associated with the ill-posed nature of the
image registration problem, researchers use regularization to constrain the
solution space. For most learning-based registration approaches, the
regularization usually has a fixed weight and only constrains the spatial
transformation. Such convention has two limitations: (1) The regularization
strength of a specific image pair should be associated with the content of the
images, thus the ``one value fits all'' scheme is not ideal; (2) Only spatially
regularizing the transformation (but overlooking the temporal consistency of
different estimations) may not be the best strategy to cope with the
ill-posedness. In this study, we propose a mean-teacher based registration
framework. This framework incorporates an additional \textit{temporal
regularization} term by encouraging the teacher model's temporal ensemble
prediction to be consistent with that of the student model. At each training
step, it also automatically adjusts the weights of the \textit{spatial
regularization} and the \textit{temporal regularization} by taking account of
the transformation uncertainty and appearance uncertainty derived from the
perturbed teacher model. We perform experiments on multi- and uni-modal
registration tasks, and the results show that our strategy outperforms the
traditional and learning-based benchmark methods.",1394,105
['stat.ML'],Learning the Number of Autoregressive Mixtures in Time Series Using the Gap Statistics,"Using a proper model to characterize a time series is crucial in making
accurate predictions. In this work we use time-varying autoregressive process
(TVAR) to describe non-stationary time series and model it as a mixture of
multiple stable autoregressive (AR) processes. We introduce a new model
selection technique based on Gap statistics to learn the appropriate number of
AR filters needed to model a time series. We define a new distance measure
between stable AR filters and draw a reference curve that is used to measure
how much adding a new AR filter improves the performance of the model, and then
choose the number of AR filters that has the maximum gap with the reference
curve. To that end, we propose a new method in order to generate uniform random
stable AR filters in root domain. Numerical results are provided demonstrating
the performance of the proposed approach.",884,86
['cs.CV'],Self-Supervised Multi-View Synchronization Learning for 3D Pose Estimation,"Current state-of-the-art methods cast monocular 3D human pose estimation as a
learning problem by training neural networks on large data sets of images and
corresponding skeleton poses. In contrast, we propose an approach that can
exploit small annotated data sets by fine-tuning networks pre-trained via
self-supervised learning on (large) unlabeled data sets. To drive such networks
towards supporting 3D pose estimation during the pre-training step, we
introduce a novel self-supervised feature learning task designed to focus on
the 3D structure in an image. We exploit images extracted from videos captured
with a multi-view camera system. The task is to classify whether two images
depict two views of the same scene up to a rigid transformation. In a
multi-view data set, where objects deform in a non-rigid manner, a rigid
transformation occurs only between two views taken at the exact same time,
i.e., when they are synchronized. We demonstrate the effectiveness of the
synchronization task on the Human3.6M data set and achieve state-of-the-art
results in 3D human pose estimation.",1092,74
"['cs.LG', 'stat.ML']",Block-wise Dynamic Sparseness,"Neural networks have achieved state of the art performance across a wide
variety of machine learning tasks, often with large and computation-heavy
models. Inducing sparseness as a way to reduce the memory and computation
footprint of these models has seen significant research attention in recent
years. In this paper, we present a new method for \emph{dynamic sparseness},
whereby part of the computations are omitted dynamically, based on the input.
For efficiency, we combined the idea of dynamic sparseness with block-wise
matrix-vector multiplications. In contrast to static sparseness, which
permanently zeroes out selected positions in weight matrices, our method
preserves the full network capabilities by potentially accessing any trained
weights. Yet, matrix vector multiplications are accelerated by omitting a
pre-defined fraction of weight blocks from the matrix, based on the input.
Experimental results on the task of language modeling, using recurrent and
quasi-recurrent models, show that the proposed method can outperform a
magnitude-based static sparseness baseline. In addition, our method achieves
similar language modeling perplexities as the dense baseline, at half the
computational cost at inference time.",1231,29
['cs.CV'],"Exploiting feature representations through similarity learning, post-ranking and ranking aggregation for person re-identification","Person re-identification has received special attention by the human analysis
community in the last few years. To address the challenges in this field, many
researchers have proposed different strategies, which basically exploit either
cross-view invariant features or cross-view robust metrics. In this work, we
propose to exploit a post-ranking approach and combine different feature
representations through ranking aggregation. Spatial information, which
potentially benefits the person matching, is represented using a 2D body model,
from which color and texture information are extracted and combined. We also
consider background/foreground information, automatically extracted via Deep
Decompositional Network, and the usage of Convolutional Neural Network (CNN)
features. To describe the matching between images we use the polynomial feature
map, also taking into account local and global information. The Discriminant
Context Information Analysis based post-ranking approach is used to improve
initial ranking lists. Finally, the Stuart ranking aggregation method is
employed to combine complementary ranking lists obtained from different feature
representations. Experimental results demonstrated that we improve the
state-of-the-art on VIPeR and PRID450s datasets, achieving 67.21% and 75.64% on
top-1 rank recognition rate, respectively, as well as obtaining competitive
results on CUHK01 dataset.",1408,129
"['cs.CV', 'cs.AI', 'cs.HC']",GazeGAN - Unpaired Adversarial Image Generation for Gaze Estimation,"Recent research has demonstrated the ability to estimate gaze on mobile
devices by performing inference on the image from the phone's front-facing
camera, and without requiring specialized hardware. While this offers wide
potential applications such as in human-computer interaction, medical diagnosis
and accessibility (e.g., hands free gaze as input for patients with motor
disorders), current methods are limited as they rely on collecting data from
real users, which is a tedious and expensive process that is hard to scale
across devices. There have been some attempts to synthesize eye region data
using 3D models that can simulate various head poses and camera settings,
however these lack in realism.
  In this paper, we improve upon a recently suggested method, and propose a
generative adversarial framework to generate a large dataset of high resolution
colorful images with high diversity (e.g., in subjects, head pose, camera
settings) and realism, while simultaneously preserving the accuracy of gaze
labels. The proposed approach operates on extended regions of the eye, and even
completes missing parts of the image. Using this rich synthesized dataset, and
without using any additional training data from real users, we demonstrate
improvements over state-of-the-art for estimating 2D gaze position on mobile
devices. We further demonstrate cross-device generalization of model
performance, as well as improved robustness to diverse head pose, blur and
distance.",1479,67
"['cs.LG', 'cs.CR', 'stat.ML']",Secure Social Recommendation based on Secret Sharing,"Nowadays, privacy preserving machine learning has been drawing much attention
in both industry and academy. Meanwhile, recommender systems have been
extensively adopted by many commercial platforms (e.g. Amazon) and they are
mainly built based on user-item interactions. Besides, social platforms (e.g.
Facebook) have rich resources of user social information. It is well known that
social information, which is rich on social platforms such as Facebook, are
useful to recommender systems. It is anticipated to combine the social
information with the user-item ratings to improve the overall recommendation
performance. Most existing recommendation models are built based on the
assumptions that the social information are available. However, different
platforms are usually reluctant to (or cannot) share their data due to certain
concerns. In this paper, we first propose a SEcure SOcial RECommendation
(SeSoRec) framework which can (1) collaboratively mine knowledge from social
platform to improve the recommendation performance of the rating platform, and
(2) securely keep the raw data of both platforms. We then propose a Secret
Sharing based Matrix Multiplication (SSMM) protocol to optimize SeSoRec and
prove its correctness and security theoretically. By applying minibatch
gradient descent, SeSoRec has linear time complexities in terms of both
computation and communication. The comprehensive experimental results on three
real-world datasets demonstrate the effectiveness of our proposed SeSoRec and
SSMM.",1518,52
['cs.CV'],Attend More Times for Image Captioning,"Most attention-based image captioning models attend to the image once per
word. However, attending once per word is rigid and is easy to miss some
information. Attending more times can adjust the attention position, find the
missing information back and avoid generating the wrong word. In this paper, we
show that attending more times per word can gain improvements in the image
captioning task, without increasing the number of parameters. We propose a
flexible two-LSTM merge model to make it convenient to encode more attentions
than words. Our captioning model uses two LSTMs to encode the word sequence and
the attention sequence respectively. The information of the two LSTMs and the
image feature are combined to predict the next word. Experiments on the MSCOCO
caption dataset show that our method outperforms the state-of-the-art. Using
bottom up features and self-critical training method, our method gets BLEU-4,
METEOR, ROUGE-L, CIDEr and SPICE scores of 0.381, 0.283, 0.580, 1.261 and 0.220
on the Karpathy test split.",1032,38
"['cs.LG', 'eess.SP', 'stat.ML']",Enhanced Adversarial Strategically-Timed Attacks against Deep Reinforcement Learning,"Recent deep neural networks based techniques, especially those equipped with
the ability of self-adaptation in the system level such as deep reinforcement
learning (DRL), are shown to possess many advantages of optimizing robot
learning systems (e.g., autonomous navigation and continuous robot arm
control.) However, the learning-based systems and the associated models may be
threatened by the risks of intentionally adaptive (e.g., noisy sensor
confusion) and adversarial perturbations from real-world scenarios. In this
paper, we introduce timing-based adversarial strategies against a DRL-based
navigation system by jamming in physical noise patterns on the selected time
frames. To study the vulnerability of learning-based navigation systems, we
propose two adversarial agent models: one refers to online learning; another
one is based on evolutionary learning. Besides, three open-source robot
learning and navigation control environments are employed to study the
vulnerability under adversarial timing attacks. Our experimental results show
that the adversarial timing attacks can lead to a significant performance drop,
and also suggest the necessity of enhancing the robustness of robot learning
systems.",1216,84
"['cs.CV', 'cs.LG', 'cs.NE']",Deep Learning-Based Classification Of the Defective Pistachios Via Deep Autoencoder Neural Networks,"Pistachio nut is mainly consumed as raw, salted or roasted because of its
high nutritional properties and favorable taste. Pistachio nuts with shell and
kernel defects, besides not being acceptable for a consumer, are also prone to
insects damage, mold decay, and aflatoxin contamination. In this research, a
deep learning-based imaging algorithm was developed to improve the sorting of
nuts with shell and kernel defects that indicate the risk of aflatoxin
contamination, such as dark stains, oily stains, adhering hull, fungal decay
and Aspergillus molds. This paper presents an unsupervised learning method to
classify defective and unpleasant pistachios based on deep Auto-encoder neural
networks. The testing of the designed neural network on a validation dataset
showed that nuts having dark stain, oily stain or adhering hull with an
accuracy of 80.3% can be distinguished from normal nuts. Due to the limited
memory available in the HPC of university, the results are reasonable and
justifiable.",1003,99
"['cs.LG', 'cs.AI']",Counterfactual Fairness with Disentangled Causal Effect Variational Autoencoder,"The problem of fair classification can be mollified if we develop a method to
remove the embedded sensitive information from the classification features.
This line of separating the sensitive information is developed through the
causal inference, and the causal inference enables the counterfactual
generations to contrast the what-if case of the opposite sensitive attribute.
Along with this separation with the causality, a frequent assumption in the
deep latent causal model defines a single latent variable to absorb the entire
exogenous uncertainty of the causal graph. However, we claim that such
structure cannot distinguish the 1) information caused by the intervention
(i.e., sensitive variable) and 2) information correlated with the intervention
from the data. Therefore, this paper proposes Disentangled Causal Effect
Variational Autoencoder (DCEVAE) to resolve this limitation by disentangling
the exogenous uncertainty into two latent variables: either 1) independent to
interventions or 2) correlated to interventions without causality.
Particularly, our disentangling approach preserves the latent variable
correlated to interventions in generating counterfactual examples. We show that
our method estimates the total effect and the counterfactual effect without a
complete causal graph. By adding a fairness regularization, DCEVAE generates a
counterfactual fair dataset while losing less original information. Also,
DCEVAE generates natural counterfactual images by only flipping sensitive
information. Additionally, we theoretically show the differences in the
covariance structures of DCEVAE and prior works from the perspective of the
latent disentanglement.",1679,79
"['cs.LG', 'cs.CV']",MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering,"We present Mixture of Contrastive Experts (MiCE), a unified probabilistic
clustering framework that simultaneously exploits the discriminative
representations learned by contrastive learning and the semantic structures
captured by a latent mixture model. Motivated by the mixture of experts, MiCE
employs a gating function to partition an unlabeled dataset into subsets
according to the latent semantics and multiple experts to discriminate distinct
subsets of instances assigned to them in a contrastive learning manner. To
solve the nontrivial inference and learning problems caused by the latent
variables, we further develop a scalable variant of the
Expectation-Maximization (EM) algorithm for MiCE and provide proof of the
convergence. Empirically, we evaluate the clustering performance of MiCE on
four widely adopted natural image datasets. MiCE achieves significantly better
results than various previous methods and a strong contrastive learning
baseline.",965,70
"['cs.CV', 'eess.IV']",EnlightenGAN: Deep Light Enhancement without Paired Supervision,"Deep learning-based methods have achieved remarkable success in image
restoration and enhancement, but are they still competitive when there is a
lack of paired training data? As one such example, this paper explores the
low-light image enhancement problem, where in practice it is extremely
challenging to simultaneously take a low-light and a normal-light photo of the
same visual scene. We propose a highly effective unsupervised generative
adversarial network, dubbed EnlightenGAN, that can be trained without
low/normal-light image pairs, yet proves to generalize very well on various
real-world test images. Instead of supervising the learning using ground truth
data, we propose to regularize the unpaired training using the information
extracted from the input itself, and benchmark a series of innovations for the
low-light image enhancement problem, including a global-local discriminator
structure, a self-regularized perceptual loss fusion, and attention mechanism.
Through extensive experiments, our proposed approach outperforms recent methods
under a variety of metrics in terms of visual quality and subjective user
study. Thanks to the great flexibility brought by unpaired training,
EnlightenGAN is demonstrated to be easily adaptable to enhancing real-world
images from various domains. The code is available at
\url{https://github.com/yueruchen/EnlightenGAN}",1378,63
"['cs.LG', 'stat.ML']",Fuzzy c-Means Clustering for Persistence Diagrams,"Persistence diagrams concisely represent the topology of a point cloud whilst
having strong theoretical guarantees, but the question of how to best integrate
this information into machine learning workflows remains open. In this paper we
extend the ubiquitous Fuzzy c-Means (FCM) clustering algorithm to the space of
persistence diagrams, enabling unsupervised learning that automatically
captures the topological structure of data without the topological prior
knowledge or additional processing of persistence diagrams that many other
techniques require. We give theoretical convergence guarantees that correspond
to the Euclidean case, and empirically demonstrate the capability of our
algorithm to capture topological information via the fuzzy RAND index. We end
with experiments on two datasets that utilise both the topological and fuzzy
nature of our algorithm: pre-trained model selection in machine learning and
lattices structures from materials science. As pre-trained models can perform
well on multiple tasks, selecting the best model is a naturally fuzzy problem;
we show that fuzzy clustering persistence diagrams allows for model selection
using the topology of decision boundaries. In materials science, we classify
transformed lattice structure datasets for the first time, whilst the
probabilistic membership values let us rank candidate lattices in a scenario
where further investigation requires expensive laboratory time and expertise.",1457,49
['cs.CV'],"SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving","Object detection is a crucial task for autonomous driving. In addition to
requiring high accuracy to ensure safety, object detection for autonomous
driving also requires real-time inference speed to guarantee prompt vehicle
control, as well as small model size and energy efficiency to enable embedded
system deployment.
  In this work, we propose SqueezeDet, a fully convolutional neural network for
object detection that aims to simultaneously satisfy all of the above
constraints. In our network, we use convolutional layers not only to extract
feature maps but also as the output layer to compute bounding boxes and class
probabilities. The detection pipeline of our model only contains a single
forward pass of a neural network, thus it is extremely fast. Our model is
fully-convolutional, which leads to a small model size and better energy
efficiency. While achieving the same accuracy as previous baselines, our model
is 30.4x smaller, 19.7x faster, and consumes 35.2x lower energy. The code is
open-sourced at \url{https://github.com/BichenWuUCB/squeezeDet}.",1067,127
"['cs.CV', 'cs.CR', 'eess.IV']",Privacy-Preserving Image Sharing via Sparsifying Layers on Convolutional Groups,"We propose a practical framework to address the problem of privacy-aware
image sharing in large-scale setups. We argue that, while compactness is always
desired at scale, this need is more severe when trying to furthermore protect
the privacy-sensitive content. We therefore encode images, such that, from one
hand, representations are stored in the public domain without paying the huge
cost of privacy protection, but ambiguated and hence leaking no discernible
content from the images, unless a combinatorially-expensive guessing mechanism
is available for the attacker. From the other hand, authorized users are
provided with very compact keys that can easily be kept secure. This can be
used to disambiguate and reconstruct faithfully the corresponding
access-granted images. We achieve this with a convolutional autoencoder of our
design, where feature maps are passed independently through sparsifying
transformations, providing multiple compact codes, each responsible for
reconstructing different attributes of the image. The framework is tested on a
large-scale database of images with public implementation available.",1128,79
"['cs.LG', 'cs.AI']",Motif Learning in Knowledge Graphs Using Trajectories Of Differential Equations,"Knowledge Graph Embeddings (KGEs) have shown promising performance on link
prediction tasks by mapping the entities and relations from a knowledge graph
into a geometric space (usually a vector space). Ultimately, the plausibility
of the predicted links is measured by using a scoring function over the learned
embeddings (vectors). Therefore, the capability in preserving graph
characteristics including structural aspects and semantics highly depends on
the design of the KGE, as well as the inherited abilities from the underlying
geometry. Many KGEs use the flat geometry which renders them incapable of
preserving complex structures and consequently causes wrong inferences by the
models. To address this problem, we propose a neuro differential KGE that
embeds nodes of a KG on the trajectories of Ordinary Differential Equations
(ODEs). To this end, we represent each relation (edge) in a KG as a vector
field on a smooth Riemannian manifold. We specifically parameterize ODEs by a
neural network to represent various complex shape manifolds and more
importantly complex shape vector fields on the manifold. Therefore, the
underlying embedding space is capable of getting various geometric forms to
encode complexity in subgraph structures with different motifs. Experiments on
synthetic and benchmark dataset as well as social network KGs justify the ODE
trajectories as a means to structure preservation and consequently avoiding
wrong inferences over state-of-the-art KGE models.",1489,79
['cs.CV'],Small Drone Field Experiment: Data Collection & Processing,"Following an initiative formalized in April 2016 formally known as ARL West
between the U.S. Army Research Laboratory (ARL) and University of Southern
California's Institute for Creative Technologies (USC ICT), a field experiment
was coordinated and executed in the summer of 2016 by ARL, USC ICT, and
Headwall Photonics. The purpose was to image part of the USC main campus in Los
Angeles, USA, using two portable COTS (commercial off the shelf) aerial drone
solutions for data acquisition, for photogrammetry (3D reconstruction from
images), and fusion of hyperspectral data with the recovered set of 3D point
clouds representing the target area. The research aims for determining the
viability of having a machine capable of segmenting the target area into key
material classes (e.g., manmade structures, live vegetation, water) for use in
multiple purposes, to include providing the user with a more accurate scene
understanding and enabling the unsupervised automatic sampling of meaningful
material classes from the target area for adaptive semi-supervised machine
learning. In the latter, a target set library may be used for automatic machine
training with data of local material classes, as an example, to increase the
prediction chances of machines recognizing targets. The field experiment and
associated data post processing approach to correct for reflectance,
geo-rectify, recover the area's dense point clouds from images, register
spectral with elevation properties of scene surfaces from the independently
collected datasets, and generate the desired scene segmented maps are
discussed. Lessons learned from the experience are also highlighted throughout
the paper.",1682,58
['cs.CV'],ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image Translation,"Recently, there has been an increasing interest in image editing methods that
employ pre-trained unconditional image generators (e.g., StyleGAN). However,
applying these methods to translate images to multiple visual domains remains
challenging. Existing works do not often preserve the domain-invariant part of
the image (e.g., the identity in human face translations), they do not usually
handle multiple domains, or do not allow for multi-modal translations. This
work proposes an implicit style function (ISF) to straightforwardly achieve
multi-modal and multi-domain image-to-image translation from pre-trained
unconditional generators. The ISF manipulates the semantics of an input latent
code to make the image generated from it lying in the desired visual domain.
Our results in human face and animal manipulations show significantly improved
results over the baselines. Our model enables cost-effective multi-modal
unsupervised image-to-image translations at high resolution using pre-trained
unconditional GANs. The code and data are available at:
\url{https://github.com/yhlleo/stylegan-mmuit}.",1105,82
"['cs.LG', 'stat.ML']",A Neural Network-Based On-device Learning Anomaly Detector for Edge Devices,"Semi-supervised anomaly detection is an approach to identify anomalies by
learning the distribution of normal data. Backpropagation neural networks
(i.e., BP-NNs) based approaches have recently drawn attention because of their
good generalization capability. In a typical situation, BP-NN-based models are
iteratively optimized in server machines with input data gathered from edge
devices. However, (1) the iterative optimization often requires significant
efforts to follow changes in the distribution of normal data (i.e., concept
drift), and (2) data transfers between edge and server impose additional
latency and energy consumption. To address these issues, we propose ONLAD and
its IP core, named ONLAD Core. ONLAD is highly optimized to perform fast
sequential learning to follow concept drift in less than one millisecond. ONLAD
Core realizes on-device learning for edge devices at low power consumption,
which realizes standalone execution where data transfers between edge and
server are not required. Experiments show that ONLAD has favorable anomaly
detection capability in an environment that simulates concept drift.
Evaluations of ONLAD Core confirm that the training latency is 1.95x~6.58x
faster than the other software implementations. Also, the runtime power
consumption of ONLAD Core implemented on PYNQ-Z1 board, a small FPGA/CPU SoC
platform, is 5.0x~25.4x lower than them.",1396,75
"['cs.LG', 'cs.CL', 'stat.ML']",A Capsule Network-based Model for Learning Node Embeddings,"In this paper, we focus on learning low-dimensional embeddings for nodes in
graph-structured data. To achieve this, we propose Caps2NE -- a new
unsupervised embedding model leveraging a network of two capsule layers.
Caps2NE induces a routing process to aggregate feature vectors of context
neighbors of a given target node at the first capsule layer, then feed these
features into the second capsule layer to infer a plausible embedding for the
target node. Experimental results show that our proposed Caps2NE obtains
state-of-the-art performances on benchmark datasets for the node classification
task. Our code is available at: \url{https://github.com/daiquocnguyen/Caps2NE}.",678,58
"['stat.ML', 'cs.LG']",S-LIME: Stabilized-LIME for Model Explanation,"An increasing number of machine learning models have been deployed in domains
with high stakes such as finance and healthcare. Despite their superior
performances, many models are black boxes in nature which are hard to explain.
There are growing efforts for researchers to develop methods to interpret these
black-box models. Post hoc explanations based on perturbations, such as LIME,
are widely used approaches to interpret a machine learning model after it has
been built. This class of methods has been shown to exhibit large instability,
posing serious challenges to the effectiveness of the method itself and harming
user trust. In this paper, we propose S-LIME, which utilizes a hypothesis
testing framework based on central limit theorem for determining the number of
perturbation points needed to guarantee stability of the resulting explanation.
Experiments on both simulated and real world data sets are provided to
demonstrate the effectiveness of our method.",972,45
['cs.CV'],Unsupervised Learning of Video Representations via Dense Trajectory Clustering,"This paper addresses the task of unsupervised learning of representations for
action recognition in videos. Previous works proposed to utilize future
prediction, or other domain-specific objectives to train a network, but
achieved only limited success. In contrast, in the relevant field of image
representation learning, simpler, discrimination-based methods have recently
bridged the gap to fully-supervised performance. We first propose to adapt two
top performing objectives in this class - instance recognition and local
aggregation, to the video domain. In particular, the latter approach iterates
between clustering the videos in the feature space of a network and updating it
to respect the cluster with a non-parametric classification loss. We observe
promising performance, but qualitative analysis shows that the learned
representations fail to capture motion patterns, grouping the videos based on
appearance. To mitigate this issue, we turn to the heuristic-based IDT
descriptors, that were manually designed to encode motion patterns in videos.
We form the clusters in the IDT space, using these descriptors as a an
unsupervised prior in the iterative local aggregation algorithm. Our
experiments demonstrates that this approach outperform prior work on UCF101 and
HMDB51 action recognition benchmarks. We also qualitatively analyze the learned
representations and show that they successfully capture video dynamics.",1430,78
"['stat.ML', 'cs.LG']",Interactive slice visualization for exploring machine learning models,"Machine learning models fit complex algorithms to arbitrarily large datasets.
These algorithms are well-known to be high on performance and low on
interpretability. We use interactive visualization of slices of predictor space
to address the interpretability deficit; in effect opening up the black-box of
machine learning algorithms, for the purpose of interrogating, explaining,
validating and comparing model fits. Slices are specified directly through
interaction, or using various touring algorithms designed to visit
high-occupancy sections or regions where the model fits have interesting
properties. The methods presented here are implemented in the R package
\pkg{condvis2}.",683,69
"['cs.CV', 'cs.HC', 'cs.LG', 'eess.IV']",A Neuro-AI Interface for Evaluating Generative Adversarial Networks,"Generative adversarial networks (GANs) are increasingly attracting attention
in the computer vision, natural language processing, speech synthesis and
similar domains. However, evaluating the performance of GANs is still an open
and challenging problem. Existing evaluation metrics primarily measure the
dissimilarity between real and generated images using automated statistical
methods. They often require large sample sizes for evaluation and do not
directly reflect human perception of image quality. In this work, we introduce
an evaluation metric called Neuroscore, for evaluating the performance of GANs,
that more directly reflects psychoperceptual image quality through the
utilization of brain signals. Our results show that Neuroscore has superior
performance to the current evaluation metrics in that: (1) It is more
consistent with human judgment; (2) The evaluation process needs much smaller
numbers of samples; and (3) It is able to rank the quality of images on a per
GAN basis. A convolutional neural network (CNN) based neuro-AI interface is
proposed to predict Neuroscore from GAN-generated images directly without the
need for neural responses. Importantly, we show that including neural responses
during the training phase of the network can significantly improve the
prediction capability of the proposed model. Codes and data can be referred at
this link: https://github.com/villawang/Neuro-AI-Interface.",1428,67
"['cs.CV', 'cs.AI', 'cs.CL']",Object Counts! Bringing Explicit Detections Back into Image Captioning,"The use of explicit object detectors as an intermediate step to image
captioning - which used to constitute an essential stage in early work - is
often bypassed in the currently dominant end-to-end approaches, where the
language model is conditioned directly on a mid-level image embedding. We argue
that explicit detections provide rich semantic information, and can thus be
used as an interpretable representation to better understand why end-to-end
image captioning systems work well. We provide an in-depth analysis of
end-to-end image captioning by exploring a variety of cues that can be derived
from such object detections. Our study reveals that end-to-end image captioning
systems rely on matching image representations to generate captions, and that
encoding the frequency, size and position of objects are complementary and all
play a role in forming a good image representation. It also reveals that
different object categories contribute in different ways towards image
captioning.",994,70
"['cs.LG', 'stat.ML']",IVE-GAN: Invariant Encoding Generative Adversarial Networks,"Generative adversarial networks (GANs) are a powerful framework for
generative tasks. However, they are difficult to train and tend to miss modes
of the true data generation process. Although GANs can learn a rich
representation of the covered modes of the data in their latent space, the
framework misses an inverse mapping from data to this latent space. We propose
Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN
framework that introduces such a mapping for individual samples from the data
by utilizing features in the data which are invariant to certain
transformations. Since the model maps individual samples to the latent space,
it naturally encourages the generator to cover all modes. We demonstrate the
effectiveness of our approach in terms of generative performance and learning
rich representations on several datasets including common benchmark image
generation tasks.",910,59
['cs.LG'],Sequoia: A Software Framework to Unify Continual Learning Research,"The field of Continual Learning (CL) seeks to develop algorithms that
accumulate knowledge and skills over time through interaction with
non-stationary environments and data distributions. Measuring progress in CL
can be difficult because a plethora of evaluation procedures (ettings) and
algorithmic solutions (methods) have emerged, each with their own potentially
disjoint set of assumptions about the CL problem. In this work, we view each
setting as a set of assumptions. We then create a tree-shaped hierarchy of the
research settings in CL, in which more general settings become the parents of
those with more restrictive assumptions. This makes it possible to use
inheritance to share and reuse research, as developing a method for a given
setting also makes it directly applicable onto any of its children. We
instantiate this idea as a publicly available software framework called
Sequoia, which features a variety of settings from both the Continual
Supervised Learning (CSL) and Continual Reinforcement Learning (CRL) domains.
Sequoia also includes a growing suite of methods which are easy to extend and
customize, in addition to more specialized methods from third-party libraries.
We hope that this new paradigm and its first implementation can serve as a
foundation for the unification and acceleration of research in CL. You can help
us grow the tree by visiting www.github.com/lebrice/Sequoia.",1411,66
"['cs.CV', 'cs.GR']",Neural Contours: Learning to Draw Lines from 3D Shapes,"This paper introduces a method for learning to generate line drawings from 3D
models. Our architecture incorporates a differentiable module operating on
geometric features of the 3D model, and an image-based module operating on
view-based shape representations. At test time, geometric and view-based
reasoning are combined with the help of a neural module to create a line
drawing. The model is trained on a large number of crowdsourced comparisons of
line drawings. Experiments demonstrate that our method achieves significant
improvements in line drawing over the state-of-the-art when evaluated on
standard benchmarks, resulting in drawings that are comparable to those
produced by experienced human artists.",712,54
"['cs.LG', 'cs.AI']",Accounting for Human Learning when Inferring Human Preferences,"Inverse reinforcement learning (IRL) is a common technique for inferring
human preferences from data. Standard IRL techniques tend to assume that the
human demonstrator is stationary, that is that their policy $\pi$ doesn't
change over time. In practice, humans interacting with a novel environment or
performing well on a novel task will change their demonstrations as they learn
more about the environment or task. We investigate the consequences of relaxing
this assumption of stationarity, in particular by modelling the human as
learning. Surprisingly, we find in some small examples that this can lead to
better inference than if the human was stationary. That is, by observing a
demonstrator who is themselves learning, a machine can infer more than by
observing a demonstrator who is noisily rational. In addition, we find evidence
that misspecification can lead to poor inference, suggesting that modelling
human learning is important, especially when the human is facing an unfamiliar
environment.",1007,62
['cs.CV'],Using Deep Networks for Drone Detection,"Drone detection is the problem of finding the smallest rectangle that
encloses the drone(s) in a video sequence. In this study, we propose a solution
using an end-to-end object detection model based on convolutional neural
networks. To solve the scarce data problem for training the network, we propose
an algorithm for creating an extensive artificial dataset by combining
background-subtracted real images. With this approach, we can achieve precision
and recall values both of which are high at the same time.",512,39
['cs.CV'],"Look globally, age locally: Face aging with an attention mechanism","Face aging is of great importance for cross-age recognition and
entertainment-related applications. Recently, conditional generative
adversarial networks (cGANs) have achieved impressive results for face aging.
Existing cGANs-based methods usually require a pixel-wise loss to keep the
identity and background consistent. However, minimizing the pixel-wise loss
between the input and synthesized images likely resulting in a ghosted or
blurry face. To address this deficiency, this paper introduces an Attention
Conditional GANs (AcGANs) approach for face aging, which utilizes attention
mechanism to only alert the regions relevant to face aging. In doing so, the
synthesized face can well preserve the background information and personal
identity without using the pixel-wise loss, and the ghost artifacts and
blurriness can be significantly reduced. Based on the benchmarked dataset
Morph, both qualitative and quantitative experiment results demonstrate
superior performance over existing algorithms in terms of image quality,
personal identity, and age accuracy.",1067,66
['cs.LG'],Unsupervised Causal Binary Concepts Discovery with VAE for Black-box Model Explanation,"We aim to explain a black-box classifier with the form: `data X is classified
as class Y because X \textit{has} A, B and \textit{does not have} C' in which
A, B, and C are high-level concepts. The challenge is that we have to discover
in an unsupervised manner a set of concepts, i.e., A, B and C, that is useful
for the explaining the classifier. We first introduce a structural generative
model that is suitable to express and discover such concepts. We then propose a
learning process that simultaneously learns the data distribution and
encourages certain concepts to have a large causal influence on the classifier
output. Our method also allows easy integration of user's prior knowledge to
induce high interpretability of concepts. Using multiple datasets, we
demonstrate that our method can discover useful binary concepts for
explanation.",847,86
"['cs.LG', 'cs.AI', 'cs.SY', 'math.OC', 'stat.ML']",Stochastic Variance Reduction Methods for Policy Evaluation,"Policy evaluation is a crucial step in many reinforcement-learning
procedures, which estimates a value function that predicts states' long-term
value under a given policy. In this paper, we focus on policy evaluation with
linear function approximation over a fixed dataset. We first transform the
empirical policy evaluation problem into a (quadratic) convex-concave saddle
point problem, and then present a primal-dual batch gradient method, as well as
two stochastic variance reduction methods for solving the problem. These
algorithms scale linearly in both sample size and feature dimension. Moreover,
they achieve linear convergence even when the saddle-point problem has only
strong concavity in the dual variables but no strong convexity in the primal
variables. Numerical experiments on benchmark problems demonstrate the
effectiveness of our methods.",859,59
"['cs.LG', 'cs.CV', 'stat.ML']",A Simple Framework for Contrastive Learning of Visual Representations,"This paper presents SimCLR: a simple framework for contrastive learning of
visual representations. We simplify recently proposed contrastive
self-supervised learning algorithms without requiring specialized architectures
or a memory bank. In order to understand what enables the contrastive
prediction tasks to learn useful representations, we systematically study the
major components of our framework. We show that (1) composition of data
augmentations plays a critical role in defining effective predictive tasks, (2)
introducing a learnable nonlinear transformation between the representation and
the contrastive loss substantially improves the quality of the learned
representations, and (3) contrastive learning benefits from larger batch sizes
and more training steps compared to supervised learning. By combining these
findings, we are able to considerably outperform previous methods for
self-supervised and semi-supervised learning on ImageNet. A linear classifier
trained on self-supervised representations learned by SimCLR achieves 76.5%
top-1 accuracy, which is a 7% relative improvement over previous
state-of-the-art, matching the performance of a supervised ResNet-50. When
fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy,
outperforming AlexNet with 100X fewer labels.",1306,69
"['cs.LG', 'cs.AI']",Sequential Diagnosis Prediction with Transformer and Ontological Representation,"Sequential diagnosis prediction on the Electronic Health Record (EHR) has
been proven crucial for predictive analytics in the medical domain. EHR data,
sequential records of a patient's interactions with healthcare systems, has
numerous inherent characteristics of temporality, irregularity and data
insufficiency. Some recent works train healthcare predictive models by making
use of sequential information in EHR data, but they are vulnerable to
irregular, temporal EHR data with the states of admission/discharge from
hospital, and insufficient data. To mitigate this, we propose an end-to-end
robust transformer-based model called SETOR, which exploits neural ordinary
differential equation to handle both irregular intervals between a patient's
visits with admitted timestamps and length of stay in each visit, to alleviate
the limitation of insufficient data by integrating medical ontology, and to
capture the dependencies between the patient's visits by employing multi-layer
transformer blocks. Experiments conducted on two real-world healthcare datasets
show that, our sequential diagnoses prediction model SETOR not only achieves
better predictive results than previous state-of-the-art approaches,
irrespective of sufficient or insufficient training data, but also derives more
interpretable embeddings of medical codes. The experimental codes are available
at the GitHub repository (https://github.com/Xueping/SETOR).",1430,79
['cs.CV'],MICIK: MIning Cross-Layer Inherent Similarity Knowledge for Deep Model Compression,"State-of-the-art deep model compression methods exploit the low-rank
approximation and sparsity pruning to remove redundant parameters from a
learned hidden layer. However, they process each hidden layer individually
while neglecting the common components across layers, and thus are not able to
fully exploit the potential redundancy space for compression. To solve the
above problem and enable further compression of a model, removing the
cross-layer redundancy and mining the layer-wise inheritance knowledge is
necessary. In this paper, we introduce a holistic model compression framework,
namely MIning Cross-layer Inherent similarity Knowledge (MICIK), to fully
excavate the potential redundancy space. The proposed MICIK framework
simultaneously, (1) learns the common and unique weight components across deep
neural network layers to increase compression rate; (2) preserves the inherent
similarity knowledge of nearby layers and distant layers to minimize the
accuracy loss and (3) can be complementary to other existing compression
techniques such as knowledge distillation. Extensive experiments on large-scale
convolutional neural networks demonstrate that MICIK is superior over
state-of-the-art model compression approaches with 16X parameter reduction on
VGG-16 and 6X on GoogLeNet, all without accuracy loss.",1324,82
['cs.CV'],VGAN-Based Image Representation Learning for Privacy-Preserving Facial Expression Recognition,"Reliable facial expression recognition plays a critical role in human-machine
interactions. However, most of the facial expression analysis methodologies
proposed to date pay little or no attention to the protection of a user's
privacy. In this paper, we propose a Privacy-Preserving Representation-Learning
Variational Generative Adversarial Network (PPRL-VGAN) to learn an image
representation that is explicitly disentangled from the identity information.
At the same time, this representation is discriminative from the standpoint of
facial expression recognition and generative as it allows expression-equivalent
face image synthesis. We evaluate the proposed model on two public datasets
under various threat scenarios. Quantitative and qualitative results
demonstrate that our approach strikes a balance between the preservation of
privacy and data utility. We further demonstrate that our model can be
effectively applied to other tasks such as expression morphing and image
completion.",994,93
['cs.CV'],PropagationNet: Propagate Points to Curve to Learn Structure Information,"Deep learning technique has dramatically boosted the performance of face
alignment algorithms. However, due to large variability and lack of samples,
the alignment problem in unconstrained situations, \emph{e.g}\onedot large head
poses, exaggerated expression, and uneven illumination, is still largely
unsolved. In this paper, we explore the instincts and reasons behind our two
proposals, \emph{i.e}\onedot Propagation Module and Focal Wing Loss, to tackle
the problem. Concretely, we present a novel structure-infused face alignment
algorithm based on heatmap regression via propagating landmark heatmaps to
boundary heatmaps, which provide structure information for further attention
map generation. Moreover, we propose a Focal Wing Loss for mining and
emphasizing the difficult samples under in-the-wild condition. In addition, we
adopt methods like CoordConv and Anti-aliased CNN from other fields that
address the shift-variance problem of CNN for face alignment. When implementing
extensive experiments on different benchmarks, \emph{i.e}\onedot WFLW, 300W,
and COFW, our method outperforms state-of-the-arts by a significant margin. Our
proposed approach achieves 4.05\% mean error on WFLW, 2.93\% mean error on 300W
full-set, and 3.71\% mean error on COFW.",1267,72
"['cs.CV', 'eess.IV']",Multi-focus Image Fusion Based on Similarity Characteristics,"A novel multi-focus image fusion algorithm performed in spatial domain based
on similarity characteristics is proposed incorporating with region
segmentation. In this paper, a new similarity measure is developed based on the
structural similarity (SSIM) index, which is more suitable for multi-focus
image segmentation. Firstly, the SSNSIM map is calculated between two input
images. Then we segment the SSNSIM map using watershed method, and merge the
small homogeneous regions with fuzzy c-means clustering algorithm (FCM). For
three source images, a joint region segmentation method based on segmentation
of two images is used to obtain the final segmentation result. Finally, the
corresponding segmented regions of the source images are fused according to
their average gradient. The performance of the image fusion method is evaluated
by several criteria including spatial frequency, average gradient, entropy,
edge retention etc. The evaluation results indicate that the proposed method is
effective and has good visual perception.",1037,60
['cs.LG'],Modifying the Symbolic Aggregate Approximation Method to Capture Segment Trend Information,"The Symbolic Aggregate approXimation (SAX) is a very popular symbolic
dimensionality reduction technique of time series data, as it has several
advantages over other dimensionality reduction techniques. One of its major
advantages is its efficiency, as it uses precomputed distances. The other main
advantage is that in SAX the distance measure defined on the reduced space
lower bounds the distance measure defined on the original space. This enables
SAX to return exact results in query-by-content tasks. Yet SAX has an inherent
drawback, which is its inability to capture segment trend information. Several
researchers have attempted to enhance SAX by proposing modifications to include
trend information. However, this comes at the expense of giving up on one or
more of the advantages of SAX. In this paper we investigate three modifications
of SAX to add trend capturing ability to it. These modifications retain the
same features of SAX in terms of simplicity, efficiency, as well as the exact
results it returns. They are simple procedures based on a different
segmentation of the time series than that used in classic-SAX. We test the
performance of these three modifications on 45 time series datasets of
different sizes, dimensions, and nature, on a classification task and we
compare it to that of classic-SAX. The results we obtained show that one of
these modifications manages to outperform classic-SAX and that another one
slightly gives better results than classic-SAX.",1486,90
"['cs.LG', 'cs.MA', 'stat.ML']",Large-Scale Traffic Signal Control Using a Novel Multi-Agent Reinforcement Learning,"Finding the optimal signal timing strategy is a difficult task for the
problem of large-scale traffic signal control (TSC). Multi-Agent Reinforcement
Learning (MARL) is a promising method to solve this problem. However, there is
still room for improvement in extending to large-scale problems and modeling
the behaviors of other agents for each individual agent. In this paper, a new
MARL, called Cooperative double Q-learning (Co-DQL), is proposed, which has
several prominent features. It uses a highly scalable independent double
Q-learning method based on double estimators and the UCB policy, which can
eliminate the over-estimation problem existing in traditional independent
Q-learning while ensuring exploration. It uses mean field approximation to
model the interaction among agents, thereby making agents learn a better
cooperative strategy. In order to improve the stability and robustness of the
learning process, we introduce a new reward allocation mechanism and a local
state sharing method. In addition, we analyze the convergence properties of the
proposed algorithm. Co-DQL is applied on TSC and tested on a multi-traffic
signal simulator. According to the results obtained on several traffic
scenarios, Co- DQL outperforms several state-of-the-art decentralized MARL
algorithms. It can effectively shorten the average waiting time of the vehicles
in the whole road system.",1391,83
"['cs.LG', 'cs.AI']",Significant Wave Height Prediction based on Wavelet Graph Neural Network,"Computational intelligence-based ocean characteristics forecasting
applications, such as Significant Wave Height (SWH) prediction, are crucial for
avoiding social and economic loss in coastal cities. Compared to the
traditional empirical-based or numerical-based forecasting models, ""soft
computing"" approaches, including machine learning and deep learning models,
have shown numerous success in recent years. In this paper, we focus on
enabling the deep learning model to learn both short-term and long-term
spatial-temporal dependencies for SWH prediction. A Wavelet Graph Neural
Network (WGNN) approach is proposed to integrate the advantages of wavelet
transform and graph neural network. Several parallel graph neural networks are
separately trained on wavelet decomposed data, and the reconstruction of each
model's prediction forms the final SWH prediction. Experimental results show
that the proposed WGNN approach outperforms other models, including the
numerical models, the machine learning models, and several deep learning
models.",1043,72
['cs.CV'],"End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression","Deformable Parts Models and Convolutional Networks each have achieved notable
performance in object detection. Yet these two approaches find their strengths
in complementary areas: DPMs are well-versed in object composition, modeling
fine-grained spatial relationships between parts; likewise, ConvNets are adept
at producing powerful image features, having been discriminatively trained
directly on the pixels. In this paper, we propose a new model that combines
these two approaches, obtaining the advantages of each. We train this model
using a new structured loss function that considers all bounding boxes within
an image, rather than isolated object instances. This enables the non-maximal
suppression (NMS) operation, previously treated as a separate post-processing
stage, to be integrated into the model. This allows for discriminative training
of our combined Convnet + DPM + NMS model in end-to-end fashion. We evaluate
our system on PASCAL VOC 2007 and 2011 datasets, achieving competitive results
on both benchmarks.",1029,101
"['stat.ML', 'math.ST', 'stat.TH']",Non-parametric Sparse Additive Auto-regressive Network Models,"Consider a multi-variate time series $(X_t)_{t=0}^{T}$ where $X_t \in
\mathbb{R}^d$ which may represent spike train responses for multiple neurons in
a brain, crime event data across multiple regions, and many others. An
important challenge associated with these time series models is to estimate an
influence network between the $d$ variables, especially when the number of
variables $d$ is large meaning we are in the high-dimensional setting. Prior
work has focused on parametric vector auto-regressive models. However,
parametric approaches are somewhat restrictive in practice. In this paper, we
use the non-parametric sparse additive model (SpAM) framework to address this
challenge. Using a combination of $\beta$ and $\phi$-mixing properties of
Markov chains and empirical process techniques for reproducing kernel Hilbert
spaces (RKHSs), we provide upper bounds on mean-squared error in terms of the
sparsity $s$, logarithm of the dimension $\log d$, number of time points $T$,
and the smoothness of the RKHSs. Our rates are sharp up to logarithm factors in
many cases. We also provide numerical experiments that support our theoretical
results and display potential advantages of using our non-parametric SpAM
framework for a Chicago crime dataset.",1258,61
"['cs.LG', 'stat.ML']",PADME: A Deep Learning-based Framework for Drug-Target Interaction Prediction,"In silico drug-target interaction (DTI) prediction is an important and
challenging problem in biomedical research with a huge potential benefit to the
pharmaceutical industry and patients. Most existing methods for DTI prediction
including deep learning models generally have binary endpoints, which could be
an oversimplification of the problem, and those methods are typically unable to
handle cold-target problems, i.e., problems involving target protein that never
appeared in the training set. Towards this, we contrived PADME (Protein And
Drug Molecule interaction prEdiction), a framework based on Deep Neural
Networks, to predict real-valued interaction strength between compounds and
proteins without requiring feature engineering. PADME takes both compound and
protein information as inputs, so it is capable of solving cold-target (and
cold-drug) problems. To our knowledge, we are the first to combine Molecular
Graph Convolution (MGC) for compound featurization with protein descriptors for
DTI prediction. We used multiple cross-validation split schemes and evaluation
metrics to measure the performance of PADME on multiple datasets, including the
ToxCast dataset, and PADME consistently dominates baseline methods. The results
of a case study, which predicts the binding affinity between various compounds
and androgen receptor (AR), suggest PADME's potential in drug development. The
scalability of PADME is another advantage in the age of Big Data.",1466,77
"['cs.CV', 'cs.LG', 'cs.MM']",Temporal Attentive Alignment for Video Domain Adaptation,"Although various image-based domain adaptation (DA) techniques have been
proposed in recent years, domain shift in videos is still not well-explored.
Most previous works only evaluate performance on small-scale datasets which are
saturated. Therefore, we first propose a larger-scale dataset with larger
domain discrepancy: UCF-HMDB_full. Second, we investigate different DA
integration methods for videos, and show that simultaneously aligning and
learning temporal dynamics achieves effective alignment even without
sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial
Adaptation Network (TA3N), which explicitly attends to the temporal dynamics
using domain discrepancy for more effective domain alignment, achieving
state-of-the-art performance on three video DA datasets. The code and data are
released at http://github.com/cmhungsteve/TA3N.",870,56
"['cs.CV', 'cs.GR', 'cs.LG']",Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images,"Existing interactive visualization tools for deep learning are mostly applied
to the training, debugging, and refinement of neural network models working on
natural images. However, visual analytics tools are lacking for the specific
application of x-ray image classification with multiple structural attributes.
In this paper, we present an interactive system for domain scientists to
visually study the multiple attributes learning models applied to x-ray
scattering images. It allows domain scientists to interactively explore this
important type of scientific images in embedded spaces that are defined on the
model prediction output, the actual labels, and the discovered feature space of
neural networks. Users are allowed to flexibly select instance images, their
clusters, and compare them regarding the specified visual representation of
attributes. The exploration is guided by the manifestation of model performance
related to mutual relationships among attributes, which often affect the
learning accuracy and effectiveness. The system thus supports domain scientists
to improve the training dataset and model, find questionable attributes labels,
and identify outlier images or spurious data clusters. Case studies and
scientists feedback demonstrate its functionalities and usefulness.",1299,89
"['cs.LG', 'cs.MA', 'cs.RO', 'stat.ML']",Robust Reinforcement Learning using Adversarial Populations,"Reinforcement Learning (RL) is an effective tool for controller design but
can struggle with issues of robustness, failing catastrophically when the
underlying system dynamics are perturbed. The Robust RL formulation tackles
this by adding worst-case adversarial noise to the dynamics and constructing
the noise distribution as the solution to a zero-sum minimax game. However,
existing work on learning solutions to the Robust RL formulation has primarily
focused on training a single RL agent against a single adversary. In this work,
we demonstrate that using a single adversary does not consistently yield
robustness to dynamics variations under standard parametrizations of the
adversary; the resulting policy is highly exploitable by new adversaries. We
propose a population-based augmentation to the Robust RL formulation in which
we randomly initialize a population of adversaries and sample from the
population uniformly during training. We empirically validate across robotics
benchmarks that the use of an adversarial population results in a more robust
policy that also improves out-of-distribution generalization. Finally, we
demonstrate that this approach provides comparable robustness and
generalization as domain randomization on these benchmarks while avoiding a
ubiquitous domain randomization failure mode.",1326,59
['cs.CV'],Scribble-Supervised Semantic Segmentation by Random Walk on Neural Representation and Self-Supervision on Neural Eigenspace,"Scribble-supervised semantic segmentation has gained much attention recently
for its promising performance without high-quality annotations. Many approaches
have been proposed. Typically, they handle this problem to either introduce a
well-labeled dataset from another related task, turn to iterative refinement
and post-processing with the graphical model, or manipulate the scribble label.
This work aims to achieve semantic segmentation supervised by scribble label
directly without auxiliary information and other intermediate manipulation.
Specifically, we impose diffusion on neural representation by random walk and
consistency on neural eigenspace by self-supervision, which forces the neural
network to produce dense and consistent predictions over the whole dataset. The
random walk embedded in the network will compute a probabilistic transition
matrix, with which the neural representation diffused to be uniform. Moreover,
given the probabilistic transition matrix, we apply the self-supervision on its
eigenspace for consistency in the image's main parts. In addition to comparing
the common scribble dataset, we also conduct experiments on the modified
datasets that randomly shrink and even drop the scribbles on image objects. The
results demonstrate the superiority of the proposed method and are even
comparable to some full-label supervised ones. The code and datasets are
available at https://github.com/panzhiyi/RW-SS.",1440,123
"['cs.LG', 'stat.ML']",On Gradient-Based Learning in Continuous Games,"We formulate a general framework for competitive gradient-based learning that
encompasses a wide breadth of multi-agent learning algorithms, and analyze the
limiting behavior of competitive gradient-based learning algorithms using
dynamical systems theory. For both general-sum and potential games, we
characterize a non-negligible subset of the local Nash equilibria that will be
avoided if each agent employs a gradient-based learning algorithm. We also shed
light on the issue of convergence to non-Nash strategies in general- and
zero-sum games, which may have no relevance to the underlying game, and arise
solely due to the choice of algorithm. The existence and frequency of such
strategies may explain some of the difficulties encountered when using gradient
descent in zero-sum games as, e.g., in the training of generative adversarial
networks. To reinforce the theoretical contributions, we provide empirical
results that highlight the frequency of linear quadratic dynamic games (a
benchmark for multi-agent reinforcement learning) that admit global Nash
equilibria that are almost surely avoided by policy gradient.",1128,46
['cs.CV'],Rank & Sort Loss for Object Detection and Instance Segmentation,"We propose Rank & Sort (RS) Loss, a ranking-based loss function to train deep
object detection and instance segmentation methods (i.e. visual detectors). RS
Loss supervises the classifier, a sub-network of these methods, to rank each
positive above all negatives as well as to sort positives among themselves with
respect to (wrt.) their localisation qualities (e.g. Intersection-over-Union -
IoU). To tackle the non-differentiable nature of ranking and sorting, we
reformulate the incorporation of error-driven update with backpropagation as
Identity Update, which enables us to model our novel sorting error among
positives. With RS Loss, we significantly simplify training: (i) Thanks to our
sorting objective, the positives are prioritized by the classifier without an
additional auxiliary head (e.g. for centerness, IoU, mask-IoU), (ii) due to its
ranking-based nature, RS Loss is robust to class imbalance, and thus, no
sampling heuristic is required, and (iii) we address the multi-task nature of
visual detectors using tuning-free task-balancing coefficients. Using RS Loss,
we train seven diverse visual detectors only by tuning the learning rate, and
show that it consistently outperforms baselines: e.g. our RS Loss improves (i)
Faster R-CNN by ~ 3 box AP and aLRP Loss (ranking-based baseline) by ~ 2 box AP
on COCO dataset, (ii) Mask R-CNN with repeat factor sampling (RFS) by 3.5 mask
AP (~ 7 AP for rare classes) on LVIS dataset; and also outperforms all
counterparts. Code is available at: https://github.com/kemaloksuz/RankSortLoss",1548,63
['cs.CV'],Syn2Real: A New Benchmark forSynthetic-to-Real Visual Domain Adaptation,"Unsupervised transfer of object recognition models from synthetic to real
data is an important problem with many potential applications. The challenge is
how to ""adapt"" a model trained on simulated images so that it performs well on
real-world data without any additional supervision. Unfortunately, current
benchmarks for this problem are limited in size and task diversity. In this
paper, we present a new large-scale benchmark called Syn2Real, which consists
of a synthetic domain rendered from 3D object models and two real-image domains
containing the same object categories. We define three related tasks on this
benchmark: closed-set object classification, open-set object classification,
and object detection. Our evaluation of multiple state-of-the-art methods
reveals a large gap in adaptation performance between the easier closed-set
classification task and the more difficult open-set and detection tasks. We
conclude that developing adaptation methods that work well across all three
tasks presents a significant future challenge for syn2real domain transfer.",1073,71
"['cs.LG', 'cs.PL']",Self-Supervised Learning to Prove Equivalence Between Programs via Semantics-Preserving Rewrite Rules,"We target the problem of synthesizing proofs of semantic equivalence between
two programs made of sequences of statements with complex symbolic expressions.
We propose a neural network architecture based on the transformer to generate
axiomatic proofs of equivalence between program pairs. We generate expressions
which include scalars and vectors and support multi-typed rewrite rules to
prove equivalence. For training the system, we develop an original training
technique, which we call self-supervised sample selection. This incremental
training improves the quality, generalizability and extensibility of the
learned model. We study the effectiveness of the system to generate proofs of
increasing length, and we demonstrate how transformer models learn to represent
complex and verifiable symbolic reasoning. Our system, S4Eq, achieves 97% proof
success on 10,000 pairs of programs while ensuring zero false positives by
design.",934,101
"['cs.CV', 'cs.AI']",Video Abnormal Event Detection by Learning to Complete Visual Cloze Tests,"Although deep neural networks (DNNs) enable great progress in video abnormal
event detection (VAD), existing solutions typically suffer from two issues: (1)
The localization of video events cannot be both precious and comprehensive. (2)
The semantics and temporal context are under-explored. To tackle those issues,
we are motivated by the prevalent cloze test in education and propose a novel
approach named Visual Cloze Completion (VCC), which conducts VAD by learning to
complete ""visual cloze tests"" (VCTs). Specifically, VCC first localizes each
video event and encloses it into a spatio-temporal cube (STC). To achieve both
precise and comprehensive localization, appearance and motion are used as
complementary cues to mark the object region associated with each event. For
each marked region, a normalized patch sequence is extracted from current and
adjacent frames and stacked into a STC. With each patch and the patch sequence
of a STC compared to a visual ""word"" and ""sentence"" respectively, we
deliberately erase a certain ""word"" (patch) to yield a VCT. Then, the VCT is
completed by training DNNs to infer the erased patch and its optical flow via
video semantics. Meanwhile, VCC fully exploits temporal context by
alternatively erasing each patch in temporal context and creating multiple
VCTs. Furthermore, we propose localization-level, event-level, model-level and
decision-level solutions to enhance VCC, which can further exploit VCC's
potential and produce significant performance improvement gain. Extensive
experiments demonstrate that VCC achieves state-of-the-art VAD performance. Our
codes and results are open at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.",1688,73
"['cs.LG', 'cs.CY']",Unsupervised learning for economic risk evaluation in the context of Covid-19 pandemic,"Justifying draconian measures during the Covid-19 pandemic was difficult not
only because of the restriction of individual rights, but also because of its
economic impact. The objective of this work is to present a machine learning
approach to identify regions that should implement similar health policies. For
that end, we successfully developed a system that gives a notion of economic
impact given the prediction of new incidental cases through unsupervised
learning and time series forecasting. This system was built taking into account
computational restrictions and low maintenance requirements in order to improve
the system's resilience. Finally this system was deployed as part of a web
application for simulation and data analysis of COVID-19, in Colombia,
available at (https://covid19.dis.eafit.edu.co).",816,86
['cs.CV'],The role of ego vision in view-invariant action recognition,"Analysis and interpretation of egocentric video data is becoming more and
more important with the increasing availability and use of wearable cameras.
Exploring and fully understanding affinities and differences between ego and
allo (or third-person) vision is paramount for the design of effective methods
to process, analyse and interpret egocentric data. In addition, a deeper
understanding of ego-vision and its peculiarities may enable new research
perspectives in which first person viewpoints can act either as a mean for
easily acquiring large amounts of data to be employed in general-purpose
recognition systems, and as a challenging test-bed to assess the usability of
techniques specifically tailored to deal with allocentric vision on more
challenging settings. Our work, with an eye to cognitive science findings,
leverages transfer learning in Convolutional Neural Networks to demonstrate
capabilities and limitations of an implicitly learnt view-invariant
representation in the specific case of action recognition.",1030,59
['cs.CV'],Cost-efficient segmentation of electron microscopy images using active learning,"Over the last decade, electron microscopy has improved up to a point that
generating high quality gigavoxel sized datasets only requires a few hours.
Automated image analysis, particularly image segmentation, however, has not
evolved at the same pace. Even though state-of-the-art methods such as U-Net
and DeepLab have improved segmentation performance substantially, the required
amount of labels remains too expensive. Active learning is the subfield in
machine learning that aims to mitigate this burden by selecting the samples
that require labeling in a smart way. Many techniques have been proposed,
particularly for image classification, to increase the steepness of learning
curves. In this work, we extend these techniques to deep CNN based image
segmentation. Our experiments on three different electron microscopy datasets
show that active learning can improve segmentation quality by 10 to 15% in
terms of Jaccard score compared to standard randomized sampling.",974,79
"['cs.LG', 'stat.ML']",Statistical Linear Estimation with Penalized Estimators: an Application to Reinforcement Learning,"Motivated by value function estimation in reinforcement learning, we study
statistical linear inverse problems, i.e., problems where the coefficients of a
linear system to be solved are observed in noise. We consider penalized
estimators, where performance is evaluated using a matrix-weighted two-norm of
the defect of the estimator measured with respect to the true, unknown
coefficients. Two objective functions are considered depending whether the
error of the defect measured with respect to the noisy coefficients is squared
or unsquared. We propose simple, yet novel and theoretically well-founded
data-dependent choices for the regularization parameters for both cases that
avoid data-splitting. A distinguishing feature of our analysis is that we
derive deterministic error bounds in terms of the error of the coefficients,
thus allowing the complete separation of the analysis of the stochastic
properties of these errors. We show that our results lead to new insights and
bounds for linear value function estimation in reinforcement learning.",1053,97
"['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE', 'stat.ML']",Semantically Decomposing the Latent Spaces of Generative Adversarial Networks,"We propose a new algorithm for training generative adversarial networks that
jointly learns latent codes for both identities (e.g. individual humans) and
observations (e.g. specific photographs). By fixing the identity portion of the
latent codes, we can generate diverse images of the same subject, and by fixing
the observation portion, we can traverse the manifold of subjects while
maintaining contingent aspects such as lighting and pose. Our algorithm
features a pairwise training scheme in which each sample from the generator
consists of two images with a common identity code. Corresponding samples from
the real dataset consist of two distinct photographs of the same subject. In
order to fool the discriminator, the generator must produce pairs that are
photorealistic, distinct, and appear to depict the same individual. We augment
both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate
pairwise training. Experiments with human judges and an off-the-shelf face
verification system demonstrate our algorithm's ability to generate convincing,
identity-matched photographs.",1106,77
['cs.CV'],Semi-Supervised Segmentation of Salt Bodies in Seismic Images using an Ensemble of Convolutional Neural Networks,"Seismic image analysis plays a crucial role in a wide range of industrial
applications and has been receiving significant attention. One of the essential
challenges of seismic imaging is detecting subsurface salt structure which is
indispensable for identification of hydrocarbon reservoirs and drill path
planning. Unfortunately, exact identification of large salt deposits is
notoriously difficult and professional seismic imaging often requires expert
human interpretation of salt bodies. Convolutional neural networks (CNNs) have
been successfully applied in many fields, and several attempts have been made
in the field of seismic imaging. But the high cost of manual annotations by
geophysics experts and scarce publicly available labeled datasets hinder the
performance of the existing CNN-based methods. In this work, we propose a
semi-supervised method for segmentation (delineation) of salt bodies in seismic
images which utilizes unlabeled data for multi-round self-training. To reduce
error amplification during self-training we propose a scheme which uses an
ensemble of CNNs. We show that our approach outperforms state-of-the-art on the
TGS Salt Identification Challenge dataset and is ranked the first among the
3234 competing methods.",1251,112
['cs.CV'],Deep Reasoning with Multi-Scale Context for Salient Object Detection,"To detect salient objects accurately, existing methods usually design complex
backbone network architectures to learn and fuse powerful features. However,
the saliency inference module that performs saliency prediction from the fused
features receives much less attention on its architecture design and typically
adopts only a few fully convolutional layers. In this paper, we find the
limited capacity of the saliency inference module indeed makes a fundamental
performance bottleneck, and enhancing its capacity is critical for obtaining
better saliency prediction. Correspondingly, we propose a deep yet light-weight
saliency inference module that adopts a multi-dilated depth-wise convolution
architecture. Such a deep inference module, though with simple architecture,
can directly perform reasoning about salient objects from the multi-scale
convolutional features fast, and give superior salient object detection
performance with less computational cost. To our best knowledge, we are the
first to reveal the importance of the inference module for salient object
detection, and present a novel architecture design with attractive efficiency
and accuracy. Extensive experimental evaluations demonstrate that our simple
framework performs favorably compared with the state-of-the-art methods with
complex backbone design.",1326,68
['cs.CV'],CAR-Net: Clairvoyant Attentive Recurrent Network,"We present an interpretable framework for path prediction that leverages
dependencies between agents' behaviors and their spatial navigation
environment. We exploit two sources of information: the past motion trajectory
of the agent of interest and a wide top-view image of the navigation scene. We
propose a Clairvoyant Attentive Recurrent Network (CAR-Net) that learns where
to look in a large image of the scene when solving the path prediction task.
Our method can attend to any area, or combination of areas, within the raw
image (e.g., road intersections) when predicting the trajectory of the agent.
This allows us to visualize fine-grained semantic elements of navigation scenes
that influence the prediction of trajectories. To study the impact of space on
agents' trajectories, we build a new dataset made of top-view images of
hundreds of scenes (Formula One racing tracks) where agents' behaviors are
heavily influenced by known areas in the images (e.g., upcoming turns). CAR-Net
successfully attends to these salient regions. Additionally, CAR-Net reaches
state-of-the-art accuracy on the standard trajectory forecasting benchmark,
Stanford Drone Dataset (SDD). Finally, we show CAR-Net's ability to generalize
to unseen scenes.",1242,48
"['cs.LG', 'cs.CV', 'stat.ML']",An empirical study on evaluation metrics of generative adversarial networks,"Evaluating generative adversarial networks (GANs) is inherently challenging.
In this paper, we revisit several representative sample-based evaluation
metrics for GANs, and address the problem of how to evaluate the evaluation
metrics. We start with a few necessary conditions for metrics to produce
meaningful scores, such as distinguishing real from generated samples,
identifying mode dropping and mode collapsing, and detecting overfitting. With
a series of carefully designed experiments, we comprehensively investigate
existing sample-based metrics and identify their strengths and limitations in
practical settings. Based on these results, we observe that kernel Maximum Mean
Discrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to
satisfy most of the desirable properties, provided that the distances between
samples are computed in a suitable feature space. Our experiments also unveil
interesting properties about the behavior of several popular GAN models, such
as whether they are memorizing training samples, and how far they are from
learning the target distribution.",1100,75
['cs.LG'],An Efficient Adversarial Attack for Tree Ensembles,"We study the problem of efficient adversarial attacks on tree based ensembles
such as gradient boosting decision trees (GBDTs) and random forests (RFs).
Since these models are non-continuous step functions and gradient does not
exist, most existing efficient adversarial attacks are not applicable. Although
decision-based black-box attacks can be applied, they cannot utilize the
special structure of trees. In our work, we transform the attack problem into a
discrete search problem specially designed for tree ensembles, where the goal
is to find a valid ""leaf tuple"" that leads to mis-classification while having
the shortest distance to the original input. With this formulation, we show
that a simple yet effective greedy algorithm can be applied to iteratively
optimize the adversarial example by moving the leaf tuple to its neighborhood
within hamming distance 1. Experimental results on several large GBDT and RF
models with up to hundreds of trees demonstrate that our method can be
thousands of times faster than the previous mixed-integer linear programming
(MILP) based approach, while also providing smaller (better) adversarial
examples than decision-based black-box attacks on general $\ell_p$ ($p=1, 2,
\infty$) norm perturbations. Our code is available at
https://github.com/chong-z/tree-ensemble-attack.",1323,50
"['cs.CV', 'cs.CR', 'cs.LG', 'eess.IV']",Error Diffusion Halftoning Against Adversarial Examples,"Adversarial examples contain carefully crafted perturbations that can fool
deep neural networks (DNNs) into making wrong predictions. Enhancing the
adversarial robustness of DNNs has gained considerable interest in recent
years. Although image transformation-based defenses were widely considered at
an earlier time, most of them have been defeated by adaptive attacks. In this
paper, we propose a new image transformation defense based on error diffusion
halftoning, and combine it with adversarial training to defend against
adversarial examples. Error diffusion halftoning projects an image into a 1-bit
space and diffuses quantization error to neighboring pixels. This process can
remove adversarial perturbations from a given image while maintaining
acceptable image quality in the meantime in favor of recognition. Experimental
results demonstrate that the proposed method is able to improve adversarial
robustness even under advanced adaptive attacks, while most of the other image
transformation-based defenses do not. We show that a proper image
transformation can still be an effective defense approach. Code:
https://github.com/shaoyuanlo/Halftoning-Defense",1168,55
['cs.CV'],Focusing on What is Relevant: Time-Series Learning and Understanding using Attention,"This paper is a contribution towards interpretability of the deep learning
models in different applications of time-series. We propose a temporal
attention layer that is capable of selecting the relevant information to
perform various tasks, including data completion, key-frame detection and
classification. The method uses the whole input sequence to calculate an
attention value for each time step. This results in more focused attention
values and more plausible visualisation than previous methods. We apply the
proposed method to three different tasks. Experimental results show that the
proposed network produces comparable results to a state of the art. In
addition, the network provides better interpretability of the decision, that
is, it generates more significant attention weight to related frames compared
to similar techniques attempted in the past.",864,84
['cs.CV'],Falling Things: A Synthetic Dataset for 3D Object Detection and Pose Estimation,"We present a new dataset, called Falling Things (FAT), for advancing the
state-of-the-art in object detection and 3D pose estimation in the context of
robotics. By synthetically combining object models and backgrounds of complex
composition and high graphical quality, we are able to generate photorealistic
images with accurate 3D pose annotations for all objects in all images. Our
dataset contains 60k annotated photos of 21 household objects taken from the
YCB dataset. For each image, we provide the 3D poses, per-pixel class
segmentation, and 2D/3D bounding box coordinates for all objects. To facilitate
testing different input modalities, we provide mono and stereo RGB images,
along with registered dense depth images. We describe in detail the generation
process and statistical analysis of the data.",810,79
"['cs.CV', 'cs.MM']",High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling,"Existing image inpainting methods often produce artifacts when dealing with
large holes in real applications. To address this challenge, we propose an
iterative inpainting method with a feedback mechanism. Specifically, we
introduce a deep generative model which not only outputs an inpainting result
but also a corresponding confidence map. Using this map as feedback, it
progressively fills the hole by trusting only high-confidence pixels inside the
hole at each iteration and focuses on the remaining pixels in the next
iteration. As it reuses partial predictions from the previous iterations as
known pixels, this process gradually improves the result. In addition, we
propose a guided upsampling network to enable generation of high-resolution
inpainting results. We achieve this by extending the Contextual Attention
module to borrow high-resolution feature patches in the input image.
Furthermore, to mimic real object removal scenarios, we collect a large object
mask dataset and synthesize more realistic training data that better simulates
user inputs. Experiments show that our method significantly outperforms
existing methods in both quantitative and qualitative evaluations. More results
and Web APP are available at https://zengxianyu.github.io/iic.",1265,89
['cs.CV'],BLADE: Filter Learning for General Purpose Computational Photography,"The Rapid and Accurate Image Super Resolution (RAISR) method of Romano,
Isidoro, and Milanfar is a computationally efficient image upscaling method
using a trained set of filters. We describe a generalization of RAISR, which we
name Best Linear Adaptive Enhancement (BLADE). This approach is a trainable
edge-adaptive filtering framework that is general, simple, computationally
efficient, and useful for a wide range of problems in computational
photography. We show applications to operations which may appear in a camera
pipeline including denoising, demosaicing, and stylization.",583,68
"['cs.CV', 'math.AP', 'math.OC', '49J20, 68U10, 65D18']",Image sequence interpolation using optimal control,"The problem of the generation of an intermediate image between two given
images in an image sequence is considered. The problem is formulated as an
optimal control problem governed by a transport equation. This approach bears
similarities with the Horn \& Schunck method for optical flow calculation but
in fact the model is quite different. The images are modelled in $BV$ and an
analysis of solutions of transport equations with values in $BV$ is included.
Moreover, the existence of optimal controls is proven and necessary conditions
are derived. Finally, two algorithms are given and numerical results are
compared with existing methods. The new method is competitive with
state-of-the-art methods and even outperforms several existing methods.",749,50
['cs.CV'],Copy and Paste GAN: Face Hallucination from Shaded Thumbnails,"Existing face hallucination methods based on convolutional neural networks
(CNN) have achieved impressive performance on low-resolution (LR) faces in a
normal illumination condition. However, their performance degrades dramatically
when LR faces are captured in low or non-uniform illumination conditions. This
paper proposes a Copy and Paste Generative Adversarial Network (CPGAN) to
recover authentic high-resolution (HR) face images while compensating for low
and non-uniform illumination. To this end, we develop two key components in our
CPGAN: internal and external Copy and Paste nets (CPnets). Specifically, our
internal CPnet exploits facial information residing in the input image to
enhance facial details; while our external CPnet leverages an external HR face
for illumination compensation. A new illumination compensation loss is thus
developed to capture illumination from the external guided face image
effectively. Furthermore, our method offsets illumination and upsamples facial
details alternately in a coarse-to-fine fashion, thus alleviating the
correspondence ambiguity between LR inputs and external HR inputs. Extensive
experiments demonstrate that our method manifests authentic HR face images in a
uniform illumination condition and outperforms state-of-the-art methods
qualitatively and quantitatively.",1330,61
['cs.CV'],Novel View Synthesis from Single Images via Point Cloud Transformation,"In this paper the argument is made that for true novel view synthesis of
objects, where the object can be synthesized from any viewpoint, an explicit 3D
shape representation isdesired. Our method estimates point clouds to capture
the geometry of the object, which can be freely rotated into the desired view
and then projected into a new image. This image, however, is sparse by nature
and hence this coarse view is used as the input of an image completion network
to obtain the dense target view. The point cloud is obtained using the
predicted pixel-wise depth map, estimated from a single RGB input
image,combined with the camera intrinsics. By using forward warping and
backward warpingbetween the input view and the target view, the network can be
trained end-to-end without supervision on depth. The benefit of using point
clouds as an explicit 3D shape for novel view synthesis is experimentally
validated on the 3D ShapeNet benchmark. Source code and data will be available
at https://lhoangan.github.io/pc4novis/.",1022,70
['cs.CV'],Image classification based on support vector machine and the fusion of complementary features,"Image Classification based on BOW (Bag-of-words) has broad application
prospect in pattern recognition field but the shortcomings are existed because
of single feature and low classification accuracy. To this end we combine three
ingredients: (i) Three features with functions of mutual complementation are
adopted to describe the images, including PHOW (Pyramid Histogram of Words),
PHOC (Pyramid Histogram of Color) and PHOG (Pyramid Histogram of Orientated
Gradients). (ii) The improvement of traditional BOW model is presented by using
dense sample and an improved K-means clustering method for constructing the
visual dictionary. (iii) An adaptive feature-weight adjusted image
categorization algorithm based on the SVM and the fusion of multiple features
is adopted. Experiments carried out on Caltech 101 database confirm the
validity of the proposed approach. From the experimental results can be seen
that the classification accuracy rate of the proposed method is improved by
7%-17% higher than that of the traditional BOW methods. This algorithm makes
full use of global, local and spatial information and has significant
improvements to the classification accuracy.",1177,93
"['cs.LG', 'math.OC']",Efficient Representation for Electric Vehicle Charging Station Operations using Reinforcement Learning,"Effectively operating electrical vehicle charging station (EVCS) is crucial
for enabling the rapid transition of electrified transportation. To solve this
problem using reinforcement learning (RL), the dimension of state/action spaces
scales with the number of EVs and is thus very large and time-varying. This
dimensionality issue affects the efficiency and convergence properties of
generic RL algorithms. We develop aggregation schemes that are based on the
emergency of EV charging, namely the laxity value. A least-laxity first (LLF)
rule is adopted to consider only the total charging power of the EVCS which
ensures the feasibility of individual EV schedules. In addition, we propose an
equivalent state aggregation that can guarantee to attain the same optimal
policy. Based on the proposed representation, policy gradient method is used to
find the best parameters for the linear Gaussian policy . Numerical results
have validated the performance improvement of the proposed representation
approaches in attaining higher rewards and more effective policies as compared
to existing approximation based approach.",1119,102
"['cs.LG', 'stat.ML']",Deep Learning of Subsurface Flow via Theory-guided Neural Network,"Active researches are currently being performed to incorporate the wealth of
scientific knowledge into data-driven approaches (e.g., neural networks) in
order to improve the latter's effectiveness. In this study, the Theory-guided
Neural Network (TgNN) is proposed for deep learning of subsurface flow. In the
TgNN, as supervised learning, the neural network is trained with available
observations or simulation data while being simultaneously guided by theory
(e.g., governing equations, other physical constraints, engineering controls,
and expert knowledge) of the underlying problem. The TgNN can achieve higher
accuracy than the ordinary Artificial Neural Network (ANN) because the former
provides physically feasible predictions and can be more readily generalized
beyond the regimes covered with the training data. Furthermore, the TgNN model
is proposed for subsurface flow with heterogeneous model parameters. Several
numerical cases of two-dimensional transient saturated flow are introduced to
test the performance of the TgNN. In the learning process, the loss function
contains data mismatch, as well as PDE constraint, engineering control, and
expert knowledge. After obtaining the parameters of the neural network by
minimizing the loss function, a TgNN model is built that not only fits the
data, but also adheres to physical/engineering constraints. Predicting the
future response can be easily realized by the TgNN model. In addition, the TgNN
model is tested in more complicated scenarios, such as prediction with changed
boundary conditions, learning from noisy data or outliers, transfer learning,
and engineering controls. Numerical results demonstrate that the TgNN model
achieves much better predictability, reliability, and generalizability than ANN
models due to the physical/engineering constraints in the former.",1840,65
['cs.CV'],MoPro: Webly Supervised Learning with Momentum Prototypes,"We propose a webly-supervised representation learning method that does not
suffer from the annotation unscalability of supervised learning, nor the
computation unscalability of self-supervised learning. Most existing works on
webly-supervised representation learning adopt a vanilla supervised learning
method without accounting for the prevalent noise in the training data, whereas
most prior methods in learning with label noise are less effective for
real-world large-scale noisy data. We propose momentum prototypes (MoPro), a
simple contrastive learning method that achieves online label noise correction,
out-of-distribution sample removal, and representation learning. MoPro achieves
state-of-the-art performance on WebVision, a weakly-labeled noisy dataset.
MoPro also shows superior performance when the pretrained model is transferred
to down-stream image classification and detection tasks. It outperforms the
ImageNet supervised pretrained model by +10.5 on 1-shot classification on VOC,
and outperforms the best self-supervised pretrained model by +17.3 when
finetuned on 1\% of ImageNet labeled samples. Furthermore, MoPro is more robust
to distribution shifts. Code and pretrained models are available at
https://github.com/salesforce/MoPro.",1256,57
['cs.CV'],Suppressing Uncertainties for Large-Scale Facial Expression Recognition,"Annotating a qualitative large-scale facial expression dataset is extremely
difficult due to the uncertainties caused by ambiguous facial expressions,
low-quality facial images, and the subjectiveness of annotators. These
uncertainties lead to a key challenge of large-scale Facial Expression
Recognition (FER) in deep learning era. To address this problem, this paper
proposes a simple yet efficient Self-Cure Network (SCN) which suppresses the
uncertainties efficiently and prevents deep networks from over-fitting
uncertain facial images. Specifically, SCN suppresses the uncertainty from two
different aspects: 1) a self-attention mechanism over mini-batch to weight each
training sample with a ranking regularization, and 2) a careful relabeling
mechanism to modify the labels of these samples in the lowest-ranked group.
Experiments on synthetic FER datasets and our collected WebEmotion dataset
validate the effectiveness of our method. Results on public benchmarks
demonstrate that our SCN outperforms current state-of-the-art methods with
\textbf{88.14}\% on RAF-DB, \textbf{60.23}\% on AffectNet, and \textbf{89.35}\%
on FERPlus. The code will be available at
\href{https://github.com/kaiwang960112/Self-Cure-Network}{https://github.com/kaiwang960112/Self-Cure-Network}.",1280,71
"['cs.CV', 'cs.AI']",BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation,"Scene graphs are nodes and edges consisting of objects and object-object
relationships, respectively. Scene graph generation (SGG) aims to identify the
objects and their relationships. We propose a bidirectional GRU (BiGRU)
transformer network (BGT-Net) for the scene graph generation for images. This
model implements novel object-object communication to enhance the object
information using a BiGRU layer. Thus, the information of all objects in the
image is available for the other objects, which can be leveraged later in the
object prediction step. This object information is used in a transformer
encoder to predict the object class as well as to create object-specific edge
information via the use of another transformer encoder. To handle the dataset
bias induced by the long-tailed relationship distribution, softening with a
log-softmax function and adding a bias adaptation term to regulate the bias for
every relation prediction individually showed to be an effective approach. We
conducted an elaborate study on experiments and ablations using open-source
datasets, i.e., Visual Genome, Open-Images, and Visual Relationship Detection
datasets, demonstrating the effectiveness of the proposed model over state of
the art.",1233,73
"['cs.LG', 'stat.ML']",Inductive Graph Embeddings through Locality Encodings,"Learning embeddings from large-scale networks is an open challenge. Despite
the overwhelming number of existing methods, is is unclear how to exploit
network structure in a way that generalizes easily to unseen nodes, edges or
graphs. In this work, we look at the problem of finding inductive network
embeddings in large networks without domain-dependent node/edge attributes. We
propose to use a set of basic predefined local encodings as the basis of a
learning algorithm. In particular, we consider the degree frequencies at
different distances from a node, which can be computed efficiently for
relatively short distances and a large number of nodes. Interestingly, the
resulting embeddings generalize well across unseen or distant regions in the
network, both in unsupervised settings, when combined with language model
learning, as well as in supervised tasks, when used as additional features in a
neural network. Despite its simplicity, this method achieves state-of-the-art
performance in tasks such as role detection, link prediction and node
classification, and represents an inductive network embedding method directly
applicable to large unattributed networks.",1173,53
"['cs.CV', 'cs.LG', 'cs.NE', 'eess.IV']",MUXConv: Information Multiplexing in Convolutional Neural Networks,"Convolutional neural networks have witnessed remarkable improvements in
computational efficiency in recent years. A key driving force has been the idea
of trading-off model expressivity and efficiency through a combination of
$1\times 1$ and depth-wise separable convolutions in lieu of a standard
convolutional layer. The price of the efficiency, however, is the sub-optimal
flow of information across space and channels in the network. To overcome this
limitation, we present MUXConv, a layer that is designed to increase the flow
of information by progressively multiplexing channel and spatial information in
the network, while mitigating computational complexity. Furthermore, to
demonstrate the effectiveness of MUXConv, we integrate it within an efficient
multi-objective evolutionary algorithm to search for the optimal model
hyper-parameters while simultaneously optimizing accuracy, compactness, and
computational efficiency. On ImageNet, the resulting models, dubbed MUXNets,
match the performance (75.3% top-1 accuracy) and multiply-add operations (218M)
of MobileNetV3 while being 1.6$\times$ more compact, and outperform other
mobile models in all the three criteria. MUXNet also performs well under
transfer learning and when adapted to object detection. On the ChestX-Ray 14
benchmark, its accuracy is comparable to the state-of-the-art while being
$3.3\times$ more compact and $14\times$ more efficient. Similarly, detection on
PASCAL VOC 2007 is 1.2% more accurate, 28% faster and 6% more compact compared
to MobileNetV2. Code is available from
https://github.com/human-analysis/MUXConv",1604,66
"['cs.LG', 'cs.DS', 'cs.NA', 'math.NA']",Graph coarsening: From scientific computing to machine learning,"The general method of graph coarsening or graph reduction has been a
remarkably useful and ubiquitous tool in scientific computing and it is now
just starting to have a similar impact in machine learning. The goal of this
paper is to take a broad look into coarsening techniques that have been
successfully deployed in scientific computing and see how similar principles
are finding their way in more recent applications related to machine learning.
In scientific computing, coarsening plays a central role in algebraic multigrid
methods as well as the related class of multilevel incomplete LU
factorizations. In machine learning, graph coarsening goes under various names,
e.g., graph downsampling or graph reduction. Its goal in most cases is to
replace some original graph by one which has fewer nodes, but whose structure
and characteristics are similar to those of the original graph. As will be
seen, a common strategy in these methods is to rely on spectral properties to
define the coarse graph.",1004,63
"['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']",LatticeNet: Fast Point Cloud Segmentation Using Permutohedral Lattices,"Deep convolutional neural networks (CNNs) have shown outstanding performance
in the task of semantically segmenting images. However, applying the same
methods on 3D data still poses challenges due to the heavy memory requirements
and the lack of structured data. Here, we propose LatticeNet, a novel approach
for 3D semantic segmentation, which takes as input raw point clouds. A PointNet
describes the local geometry which we embed into a sparse permutohedral
lattice. The lattice allows for fast convolutions while keeping a low memory
footprint. Further, we introduce DeformSlice, a novel learned data-dependent
interpolation for projecting lattice features back onto the point cloud. We
present results of 3D segmentation on various datasets where our method
achieves state-of-the-art performance.",801,70
['cs.CV'],Dual Adversarial Inference for Text-to-Image Synthesis,"Synthesizing images from a given text description involves engaging two types
of information: the content, which includes information explicitly described in
the text (e.g., color, composition, etc.), and the style, which is usually not
well described in the text (e.g., location, quantity, size, etc.). However, in
previous works, it is typically treated as a process of generating images only
from the content, i.e., without considering learning meaningful style
representations. In this paper, we aim to learn two variables that are
disentangled in the latent space, representing content and style respectively.
We achieve this by augmenting current text-to-image synthesis frameworks with a
dual adversarial inference mechanism. Through extensive experiments, we show
that our model learns, in an unsupervised manner, style representations
corresponding to certain meaningful information present in the image that are
not well described in the text. The new framework also improves the quality of
synthesized images when evaluated on Oxford-102, CUB and COCO datasets.",1072,54
"['cs.CV', 'cs.MM']","Computational Attention System for Children, Adults and Elderly","The existing computational visual attention systems have focused on the
objective to basically simulate and understand the concept of visual attention
system in adults. Consequently, the impact of observer's age in scene viewing
behavior has rarely been considered. This study quantitatively analyzed the
age-related differences in gaze landings during scene viewing for three
different class of images: naturals, man-made, and fractals. Observer's of
different age-group have shown different scene viewing tendencies independent
to the class of the image viewed. Several interesting observations are drawn
from the results. First, gaze landings for man-made dataset showed that whereas
child observers focus more on the scene foreground, i.e., locations that are
near, elderly observers tend to explore the scene background, i.e., locations
farther in the scene. Considering this result a framework is proposed in this
paper to quantitatively measure the depth bias tendency across age groups.
Second, the quantitative analysis results showed that children exhibit the
lowest exploratory behavior level but the highest central bias tendency among
the age groups and across the different scene categories. Third,
inter-individual similarity metrics reveal that an adult had significantly
lower gaze consistency with children and elderly compared to other adults for
all the scene categories. Finally, these analysis results were consequently
leveraged to develop a more accurate age-adapted saliency model independent to
the image type. The prediction accuracy suggests that our model fits better to
the collected eye-gaze data of the observers belonging to different age groups
than the existing models do.",1707,63
"['stat.ML', 'cs.LG']",Top-down particle filtering for Bayesian decision trees,"Decision tree learning is a popular approach for classification and
regression in machine learning and statistics, and Bayesian
formulations---which introduce a prior distribution over decision trees, and
formulate learning as posterior inference given data---have been shown to
produce competitive performance. Unlike classic decision tree learning
algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing
Bayesian algorithms produce an approximation to the posterior distribution by
evolving a complete tree (or collection thereof) iteratively via local Monte
Carlo modifications to the structure of the tree, e.g., using Markov chain
Monte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that
instead works in a top-down manner, mimicking the behavior and speed of classic
algorithms. We demonstrate empirically that our approach delivers accuracy
comparable to the most popular MCMC method, but operates more than an order of
magnitude faster, and thus represents a better computation-accuracy tradeoff.",1046,55
"['cs.LG', 'cs.AI', 'cs.CY', 'stat.ML']",Fairness in Deep Learning: A Computational Perspective,"Deep learning is increasingly being used in high-stake decision making
applications that affect individual lives. However, deep learning models might
exhibit algorithmic discrimination behaviors with respect to protected groups,
potentially posing negative impacts on individuals and society. Therefore,
fairness in deep learning has attracted tremendous attention recently. We
provide a review covering recent progresses to tackle algorithmic fairness
problems of deep learning from the computational perspective. Specifically, we
show that interpretability can serve as a useful ingredient to diagnose the
reasons that lead to algorithmic discrimination. We also discuss fairness
mitigation approaches categorized according to three stages of deep learning
life-cycle, aiming to push forward the area of fairness in deep learning and
build genuinely fair and reliable deep learning systems.",892,54
"['cs.LG', 'cs.FL', 'stat.ML']",Distillation of Weighted Automata from Recurrent Neural Networks using a Spectral Approach,"This paper is an attempt to bridge the gap between deep learning and
grammatical inference. Indeed, it provides an algorithm to extract a
(stochastic) formal language from any recurrent neural network trained for
language modelling. In detail, the algorithm uses the already trained network
as an oracle -- and thus does not require the access to the inner
representation of the black-box -- and applies a spectral approach to infer a
weighted automaton.
  As weighted automata compute linear functions, they are computationally more
efficient than neural networks and thus the nature of the approach is the one
of knowledge distillation. We detail experiments on 62 data sets (both
synthetic and from real-world applications) that allow an in-depth study of the
abilities of the proposed algorithm. The results show the WA we extract are
good approximations of the RNN, validating the approach. Moreover, we show how
the process provides interesting insights toward the behavior of RNN learned on
data, enlarging the scope of this work to the one of explainability of deep
learning models.",1090,90
"['cs.LG', 'stat.ML']",On the Interpretability and Evaluation of Graph Representation Learning,"With the rising interest in graph representation learning, a variety of
approaches have been proposed to effectively capture a graph's properties.
While these approaches have improved performance in graph machine learning
tasks compared to traditional graph techniques, they are still perceived as
techniques with limited insight into the information encoded in these
representations. In this work, we explore methods to interpret node embeddings
and propose the creation of a robust evaluation framework for comparing graph
representation learning algorithms and hyperparameters. We test our methods on
graphs with different properties and investigate the relationship between
embedding training parameters and the ability of the produced embedding to
recover the structure of the original graph in a downstream task.",818,71
"['cs.CV', 'cs.RO']","Good Graph to Optimize: Cost-Effective, Budget-Aware Bundle Adjustment in Visual SLAM","The cost-efficiency of visual(-inertial) SLAM (VSLAM) is a critical
characteristic of resource-limited applications. While hardware and algorithm
advances have been significantly improved the cost-efficiency of VSLAM
front-ends, the cost-efficiency of VSLAM back-ends remains a bottleneck. This
paper describes a novel, rigorous method to improve the cost-efficiency of
local BA in a BA-based VSLAM back-end. An efficient algorithm, called Good
Graph, is developed to select size-reduced graphs optimized in local BA with
condition preservation. To better suit BA-based VSLAM back-ends, the Good Graph
predicts future estimation needs, dynamically assigns an appropriate size
budget, and selects a condition-maximized subgraph for BA estimation.
Evaluations are conducted on two scenarios: 1) VSLAM as standalone process, and
2) VSLAM as part of closed-loop navigation system. Results from the first
scenario show Good Graph improves accuracy and robustness of VSLAM estimation,
when computational limits exist. Results from the second scenario, indicate
that Good Graph benefits the trajectory tracking performance of VSLAM-based
closed-loop navigation systems, which is a primary application of VSLAM.",1203,85
['cs.CV'],Looking Twice for Partial Clues: Weakly-supervised Part-Mentored Attention Network for Vehicle Re-Identification,"Vehicle re-identification (Re-ID) is to retrieve images of the same vehicle
across different cameras. Two key challenges lie in the subtle inter-instance
discrepancy caused by near-duplicate identities and the large intra-instance
variance caused by different views. Since the holistic appearance suffers from
viewpoint variation and distortion, part-level feature learning has been
introduced to enhance vehicle description. However, existing approaches to
localize and amplify significant parts often fail to handle spatial
misalignment as well as occlusion and require expensive annotations. In this
paper, we propose a weakly supervised Part-Mentored Attention Network (PMANet)
composed of a Part Attention Network (PANet) for vehicle part localization with
self-attention and a Part-Mentored Network (PMNet) for mentoring the global and
local feature aggregation. Firstly, PANet is introduced to predict a foreground
mask and pinpoint $K$ prominent vehicle parts only with weak identity
supervision. Secondly, we propose a PMNet to learn global and part-level
features with multi-scale attention and aggregate them in $K$ main-partial
tasks via part transfer. Like humans who first differentiate objects with
general information and then observe salient parts for more detailed clues,
PANet and PMNet construct a two-stage attention structure to perform a
coarse-to-fine search among identities. Finally, we address this Re-ID issue as
a multi-task problem, including global feature learning, identity
classification, and part transfer. We adopt Homoscedastic Uncertainty to learn
the optimal weighing of different losses. Comprehensive experiments are
conducted on two benchmark datasets. Our approach outperforms recent
state-of-the-art methods by averagely 2.63% in CMC@1 on VehicleID and 2.2% in
mAP on VeRi776. Results on occluded test sets also demonstrate the
generalization ability of PMANet.",1905,112
['cs.CV'],Cross-Identity Motion Transfer for Arbitrary Objects through Pose-Attentive Video Reassembling,"We propose an attention-based networks for transferring motions between
arbitrary objects. Given a source image(s) and a driving video, our networks
animate the subject in the source images according to the motion in the driving
video. In our attention mechanism, dense similarities between the learned
keypoints in the source and the driving images are computed in order to
retrieve the appearance information from the source images. Taking a different
approach from the well-studied warping based models, our attention-based model
has several advantages. By reassembling non-locally searched pieces from the
source contents, our approach can produce more realistic outputs. Furthermore,
our system can make use of multiple observations of the source appearance (e.g.
front and sides of faces) to make the results more accurate. To reduce the
training-testing discrepancy of the self-supervised learning, a novel
cross-identity training scheme is additionally introduced. With the training
scheme, our networks is trained to transfer motions between different subjects,
as in the real testing scenario. Experimental results validate that our method
produces visually pleasing results in various object domains, showing better
performances compared to previous works.",1267,94
['cs.CV'],Efficient MRF Energy Propagation for Video Segmentation via Bilateral Filters,"Segmentation of an object from a video is a challenging task in multimedia
applications. Depending on the application, automatic or interactive methods
are desired; however, regardless of the application type, efficient computation
of video object segmentation is crucial for time-critical applications;
specifically, mobile and interactive applications require near real-time
efficiencies. In this paper, we address the problem of video segmentation from
the perspective of efficiency. We initially redefine the problem of video
object segmentation as the propagation of MRF energies along the temporal
domain. For this purpose, a novel and efficient method is proposed to propagate
MRF energies throughout the frames via bilateral filters without using any
global texture, color or shape model. Recently presented bi-exponential filter
is utilized for efficiency, whereas a novel technique is also developed to
dynamically solve graph-cuts for varying, non-lattice graphs in general linear
filtering scenario. These improvements are experimented for both automatic and
interactive video segmentation scenarios. Moreover, in addition to the
efficiency, segmentation quality is also tested both quantitatively and
qualitatively. Indeed, for some challenging examples, significant time
efficiency is observed without loss of segmentation quality.",1345,77
"['cs.LG', 'cs.AI']",Can $Q$-Learning with Graph Networks Learn a Generalizable Branching Heuristic for a SAT Solver?,"We present Graph-$Q$-SAT, a branching heuristic for a Boolean SAT solver
trained with value-based reinforcement learning (RL) using Graph Neural
Networks for function approximation. Solvers using Graph-$Q$-SAT are complete
SAT solvers that either provide a satisfying assignment or proof of
unsatisfiability, which is required for many SAT applications. The branching
heuristics commonly used in SAT solvers make poor decisions during their
warm-up period, whereas Graph-$Q$-SAT is trained to examine the structure of
the particular problem instance to make better decisions early in the search.
Training Graph-$Q$-SAT is data efficient and does not require elaborate dataset
preparation or feature engineering. We train Graph-$Q$-SAT using RL interfacing
with MiniSat solver and show that Graph-$Q$-SAT can reduce the number of
iterations required to solve SAT problems by 2-3X. Furthermore, it generalizes
to unsatisfiable SAT instances, as well as to problems with 5X more variables
than it was trained on. We show that for larger problems, reductions in the
number of iterations lead to wall clock time reductions, the ultimate goal when
designing heuristics. We also show positive zero-shot transfer behavior when
testing Graph-$Q$-SAT on a task family different from that used for training.
While more work is needed to apply Graph-$Q$-SAT to reduce wall clock time in
modern SAT solving settings, it is a compelling proof-of-concept showing that
RL equipped with Graph Neural Networks can learn a generalizable branching
heuristic for SAT search.",1553,96
"['cs.LG', 'adap-org', 'cond-mat.stat-mech', 'cs.DC', 'cs.NI', 'nlin.AO', 'I.2.6; I.2.11']",Using Collective Intelligence to Route Internet Traffic,"A COllective INtelligence (COIN) is a set of interacting reinforcement
learning (RL) algorithms designed in an automated fashion so that their
collective behavior optimizes a global utility function. We summarize the
theory of COINs, then present experiments using that theory to design COINs to
control internet traffic routing. These experiments indicate that COINs
outperform all previously investigated RL-based, shortest path routing
algorithms.",450,55
['cs.CV'],Detecting Mammals in UAV Images: Best Practices to address a substantially Imbalanced Dataset with Deep Learning,"Knowledge over the number of animals in large wildlife reserves is a vital
necessity for park rangers in their efforts to protect endangered species.
Manual animal censuses are dangerous and expensive, hence Unmanned Aerial
Vehicles (UAVs) with consumer level digital cameras are becoming a popular
alternative tool to estimate livestock. Several works have been proposed that
semi-automatically process UAV images to detect animals, of which some employ
Convolutional Neural Networks (CNNs), a recent family of deep learning
algorithms that proved very effective in object detection in large datasets
from computer vision. However, the majority of works related to wildlife
focuses only on small datasets (typically subsets of UAV campaigns), which
might be detrimental when presented with the sheer scale of real study areas
for large mammal census. Methods may yield thousands of false alarms in such
cases. In this paper, we study how to scale CNNs to large wildlife census tasks
and present a number of recommendations to train a CNN on a large UAV dataset.
We further introduce novel evaluation protocols that are tailored to censuses
and model suitability for subsequent human verification of detections. Using
our recommendations, we are able to train a CNN reducing the number of false
positives by an order of magnitude compared to previous state-of-the-art.
Setting the requirements at 90% recall, our CNN allows to reduce the amount of
data required for manual verification by three times, thus making it possible
for rangers to screen all the data acquired efficiently and to detect almost
all animals in the reserve automatically.",1644,112
"['cs.LG', 'physics.chem-ph']",Flexible dual-branched message passing neural network for quantum mechanical property prediction with molecular conformation,"A molecule is a complex of heterogeneous components, and the spatial
arrangements of these components determine the whole molecular properties and
characteristics. With the advent of deep learning in computational chemistry,
several studies have focused on how to predict molecular properties based on
molecular configurations. Message passing neural network provides an effective
framework for capturing molecular geometric features with the perspective of a
molecule as a graph. However, most of these studies assumed that all
heterogeneous molecular features, such as atomic charge, bond length, or other
geometric features always contribute equivalently to the target prediction,
regardless of the task type. In this study, we propose a dual-branched neural
network for molecular property prediction based on message-passing framework.
Our model learns heterogeneous molecular features with different scales, which
are trained flexibly according to each prediction target. In addition, we
introduce a discrete branch to learn single atom features without local
aggregation, apart from message-passing steps. We verify that this novel
structure can improve the model performance with faster convergence in most
targets. The proposed model outperforms other recent models with sparser
representations. Our experimental results indicate that in the chemical
property prediction tasks, the diverse chemical nature of targets should be
carefully considered for both model performance and generalizability.",1504,124
"['stat.ML', 'cs.LG', 'stat.ME']",Automatic Model Building in GEFCom 2017 Qualifying Match,"The Tangent Works team participated in GEFCom 2017 to test its automatic
model building strategy for time series known as Tangent Information Modeller
(TIM). Model building using TIM combined with historical temperature shuffling
resulted in winning the competition. This strategy involved one remaining
degree of freedom, a decision on using a trend variable. This paper describes
our modelling efforts in the competition, and furthermore outlines a fully
automated scenario where the decision on using the trend variable is handled by
TIM. The results show that such a setup would also win the competition.",608,56
"['cs.LG', 'cs.AI']",Shared Learning : Enhancing Reinforcement in $Q$-Ensembles,"Deep Reinforcement Learning has been able to achieve amazing successes in a
variety of domains from video games to continuous control by trying to maximize
the cumulative reward. However, most of these successes rely on algorithms that
require a large amount of data to train in order to obtain results on par with
human-level performance. This is not feasible if we are to deploy these systems
on real world tasks and hence there has been an increased thrust in exploring
data efficient algorithms. To this end, we propose the Shared Learning
framework aimed at making $Q$-ensemble algorithms data-efficient. For achieving
this, we look into some principles of transfer learning which aim to study the
benefits of information exchange across tasks in reinforcement learning and
adapt transfer to learning our value function estimates in a novel manner. In
this paper, we consider the special case of transfer between the value function
estimates in the $Q$-ensemble architecture of BootstrappedDQN. We further
empirically demonstrate how our proposed framework can help in speeding up the
learning process in $Q$-ensembles with minimum computational overhead on a
suite of Atari 2600 Games.",1191,58
"['cs.LG', 'stat.ML']",First Order Generative Adversarial Networks,"GANs excel at learning high dimensional distributions, but they can update
generator parameters in directions that do not correspond to the steepest
descent direction of the objective. Prominent examples of problematic update
directions include those used in both Goodfellow's original GAN and the
WGAN-GP. To formally describe an optimal update direction, we introduce a
theoretical framework which allows the derivation of requirements on both the
divergence and corresponding method for determining an update direction, with
these requirements guaranteeing unbiased mini-batch updates in the direction of
steepest descent. We propose a novel divergence which approximates the
Wasserstein distance while regularizing the critic's first order information.
Together with an accompanying update direction, this divergence fulfills the
requirements for unbiased steepest descent updates. We verify our method, the
First Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a
new state of the art on the One Billion Word language generation task. Code to
reproduce experiments is available.",1104,43
['cs.CV'],Learning Category-level Shape Saliency via Deep Implicit Surface Networks,"This paper is motivated from a fundamental curiosity on what defines a
category of object shapes. For example, we may have the common knowledge that a
plane has wings, and a chair has legs. Given the large shape variations among
different instances of a same category, we are formally interested in
developing a quantity defined for individual points on a continuous object
surface; the quantity specifies how individual surface points contribute to the
formation of the shape as the category. We term such a quantity as
category-level shape saliency or shape saliency for short. Technically, we
propose to learn saliency maps for shape instances of a same category from a
deep implicit surface network; sensible saliency scores for sampled points in
the implicit surface field are predicted by constraining the capacity of input
latent code. We also enhance the saliency prediction with an additional loss of
contrastive training. We expect such learned surface maps of shape saliency to
have the properties of smoothness, symmetry, and semantic representativeness.
We verify these properties by comparing our method with alternative ways of
saliency computation. Notably, we show that by leveraging the learned shape
saliency, we are able to reconstruct either category-salient or
instance-specific parts of object surfaces; semantic representativeness of the
learned saliency is also reflected in its efficacy to guide the selection of
surface points for better point cloud classification.",1492,73
['cs.CV'],In-Domain GAN Inversion for Real Image Editing,"Recent work has shown that a variety of semantics emerge in the latent space
of Generative Adversarial Networks (GANs) when being trained to synthesize
images. However, it is difficult to use these learned semantics for real image
editing. A common practice of feeding a real image to a trained GAN generator
is to invert it back to a latent code. However, existing inversion methods
typically focus on reconstructing the target image by pixel values yet fail to
land the inverted code in the semantic domain of the original latent space. As
a result, the reconstructed image cannot well support semantic editing through
varying the inverted code. To solve this problem, we propose an in-domain GAN
inversion approach, which not only faithfully reconstructs the input image but
also ensures the inverted code to be semantically meaningful for editing. We
first learn a novel domain-guided encoder to project a given image to the
native latent space of GANs. We then propose domain-regularized optimization by
involving the encoder as a regularizer to fine-tune the code produced by the
encoder and better recover the target image. Extensive experiments suggest that
our inversion method achieves satisfying real image reconstruction and more
importantly facilitates various image editing tasks, significantly
outperforming start-of-the-arts.",1341,46
"['cs.CV', 'cs.LG', 'eess.IV']",Correction Filter for Single Image Super-Resolution: Robustifying Off-the-Shelf Deep Super-Resolvers,"The single image super-resolution task is one of the most examined inverse
problems in the past decade. In the recent years, Deep Neural Networks (DNNs)
have shown superior performance over alternative methods when the acquisition
process uses a fixed known downsampling kernel-typically a bicubic kernel.
However, several recent works have shown that in practical scenarios, where the
test data mismatch the training data (e.g. when the downsampling kernel is not
the bicubic kernel or is not available at training), the leading DNN methods
suffer from a huge performance drop. Inspired by the literature on generalized
sampling, in this work we propose a method for improving the performance of
DNNs that have been trained with a fixed kernel on observations acquired by
other kernels. For a known kernel, we design a closed-form correction filter
that modifies the low-resolution image to match one which is obtained by
another kernel (e.g. bicubic), and thus improves the results of existing
pre-trained DNNs. For an unknown kernel, we extend this idea and propose an
algorithm for blind estimation of the required correction filter. We show that
our approach outperforms other super-resolution methods, which are designed for
general downsampling kernels.",1260,100
"['cs.CV', 'cs.LG', 'stat.ML']",Geometry-Aware Neural Rendering,"Understanding the 3-dimensional structure of the world is a core challenge in
computer vision and robotics. Neural rendering approaches learn an implicit 3D
model by predicting what a camera would see from an arbitrary viewpoint. We
extend existing neural rendering to more complex, higher dimensional scenes
than previously possible. We propose Epipolar Cross Attention (ECA), an
attention mechanism that leverages the geometry of the scene to perform
efficient non-local operations, requiring only $O(n)$ comparisons per spatial
dimension instead of $O(n^2)$. We introduce three new simulated datasets
inspired by real-world robotics and demonstrate that ECA significantly improves
the quantitative and qualitative performance of Generative Query Networks
(GQN).",764,31
"['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'stat.ML']",Variational Algorithms for Marginal MAP,"Marginal MAP problems are notoriously difficult tasks for graphical models.
We derive a general variational framework for solving marginal MAP problems, in
which we apply analogues of the Bethe, tree-reweighted, and mean field
approximations. We then derive a ""mixed"" message passing algorithm and a
convergent alternative using CCCP to solve the BP-type approximations.
Theoretically, we give conditions under which the decoded solution is a global
or local optimum, and obtain novel upper bounds on solutions. Experimentally we
demonstrate that our algorithms outperform related approaches. We also show
that EM and variational EM comprise a special case of our framework.",674,39
"['cs.CV', 'cs.MM']",Cluster-wise Unsupervised Hashing for Cross-Modal Similarity Search,"Large-scale cross-modal hashing similarity retrieval has attracted more and
more attention in modern search applications such as search engines and
autopilot, showing great superiority in computation and storage. However,
current unsupervised cross-modal hashing methods still have some limitations:
(1)many methods relax the discrete constraints to solve the optimization
objective which may significantly degrade the retrieval performance;(2)most
existing hashing model project heterogenous data into a common latent space,
which may always lose sight of diversity in heterogenous data;(3)transforming
real-valued data point to binary codes always results in abundant loss of
information, producing the suboptimal continuous latent space. To overcome
above problems, in this paper, a novel Cluster-wise Unsupervised Hashing (CUH)
method is proposed. Specifically, CUH jointly performs the multi-view
clustering that projects the original data points from different modalities
into its own low-dimensional latent semantic space and finds the cluster
centroid points and the common clustering indicators in its own low-dimensional
space, and learns the compact hash codes and the corresponding linear hash
functions. An discrete optimization framework is developed to learn the unified
binary codes across modalities under the guidance cluster-wise code-prototypes.
The reasonableness and effectiveness of CUH is well demonstrated by
comprehensive experiments on diverse benchmark datasets.",1490,67
"['cs.LG', 'eess.SP']",A Scalable Algorithm for Anomaly Detection via Learning-Based Controlled Sensing,"We address the problem of sequentially selecting and observing processes from
a given set to find the anomalies among them. The decision-maker observes one
process at a time and obtains a noisy binary indicator of whether or not the
corresponding process is anomalous. In this setting, we develop an anomaly
detection algorithm that chooses the process to be observed at a given time
instant, decides when to stop taking observations, and makes a decision
regarding the anomalous processes. The objective of the detection algorithm is
to arrive at a decision with an accuracy exceeding a desired value while
minimizing the delay in decision making. Our algorithm relies on a Markov
decision process defined using the marginal probability of each process being
normal or anomalous, conditioned on the observations. We implement the
detection algorithm using the deep actor-critic reinforcement learning
framework. Unlike prior work on this topic that has exponential complexity in
the number of processes, our algorithm has computational and memory
requirements that are both polynomial in the number of processes. We
demonstrate the efficacy of our algorithm using numerical experiments by
comparing it with the state-of-the-art methods.",1237,80
['cs.CV'],Endoscopic Depth Measurement and Super-Spectral-Resolution Imaging,"Intra-operative measurements of tissue shape and multi/ hyperspectral
information have the potential to provide surgical guidance and decision making
support. We report an optical probe based system to combine sparse
hyperspectral measurements and spectrally-encoded structured lighting (SL) for
surface measurements. The system provides informative signals for navigation
with a surgical interface. By rapidly switching between SL and white light (WL)
modes, SL information is combined with structure-from-motion (SfM) from white
light images, based on SURF feature detection and Lucas-Kanade (LK) optical
flow to provide quasi-dense surface shape reconstruction with known scale in
real-time. Furthermore, ""super-spectral-resolution"" was realized, whereby the
RGB images and sparse hyperspectral data were integrated to recover dense
pixel-level hyperspectral stacks, by using convolutional neural networks to
upscale the wavelength dimension. Validation and demonstration of this system
is reported on ex vivo/in vivo animal/ human experiments.",1047,66
"['cs.LG', 'cs.HC', 'eess.SP', 'stat.ML']",Deep Convolutional Neural Network for Automated Detection of Mind Wandering using EEG Signals,"Mind wandering (MW) is a ubiquitous phenomenon which reflects a shift in
attention from task-related to task-unrelated thoughts. There is a need for
intelligent interfaces that can reorient attention when MW is detected due to
its detrimental effects on performance and productivity. In this paper, we
propose a deep learning model for MW detection using Electroencephalogram (EEG)
signals. Specifically, we develop a channel-wise deep convolutional neural
network (CNN) model to classify the features of focusing state and MW extracted
from EEG signals. This is the first study that employs CNN to automatically
detect MW using only EEG data. The experimental results on the collected
dataset demonstrate promising performance with 91.78% accuracy, 92.84%
sensitivity, and 90.73% specificity.",793,93
"['cs.CV', 'physics.optics']",An optical biomimetic eyes with interested object imaging,"We presented an optical system to perform imaging interested objects in
complex scenes, like the creature easy see the interested prey in the hunt for
complex environments. It utilized Deep-learning network to learn the interested
objects's vision features and designed the corresponding ""imaging matrices"",
furthermore the learned matrixes act as the measurement matrix to complete
compressive imaging with a single-pixel camera, finally we can using the
compressed image data to only image the interested objects without the rest
objects and backgrounds of the scenes with the previous Deep-learning network.
Our results demonstrate that no matter interested object is single feature or
rich details, the interference can be successfully filtered out and this idea
can be applied in some common applications that effectively improve the
performance. This bio-inspired optical system can act as the creature eye to
achieve success on interested-based object imaging, object detection, object
recognition and object tracking, etc.",1030,57
"['cs.CV', 'cs.LG', 'eess.IV']",Recurrent Neural Networks for video object detection,"There is lots of scientific work about object detection in images. For many
applications like for example autonomous driving the actual data on which
classification has to be done are videos. This work compares different methods,
especially those which use Recurrent Neural Networks to detect objects in
videos. We differ between feature-based methods, which feed feature maps of
different frames into the recurrent units, box-level methods, which feed
bounding boxes with class probabilities into the recurrent units and methods
which use flow networks. This study indicates common outcomes of the compared
methods like the benefit of including the temporal context into object
detection and states conclusions and guidelines for video object detection
networks.",763,52
['cs.CV'],Recognition of Images of Korean Characters Using Embedded Networks,"Despite the significant success in the field of text recognition, complex and
unsolved problems still exist in this field. In recent years, the recognition
accuracy of the English language has greatly increased, while the problem of
recognition of hieroglyphs has received much less attention. Hieroglyph
recognition or image recognition with Korean, Japanese or Chinese characters
have differences from the traditional text recognition task. This article
discusses the main differences between hieroglyph languages and the Latin
alphabet in the context of image recognition. A light-weight method for
recognizing images of the hieroglyphs is proposed and tested on a public
dataset of Korean hieroglyph images. Despite the existing solutions, the
proposed method is suitable for mobile devices. Its recognition accuracy is
better than the accuracy of the open-source OCR framework. The presented method
of training embedded net bases on the similarities in the recognition data.",979,66
"['cs.LG', 'cs.AI', 'stat.ML']",Neural-encoding Human Experts' Domain Knowledge to Warm Start Reinforcement Learning,"Deep reinforcement learning has been successful in a variety of tasks, such
as game playing and robotic manipulation. However, attempting to learn
\textit{tabula rasa} disregards the logical structure of many domains as well
as the wealth of readily available knowledge from domain experts that could
help ""warm start"" the learning process. We present a novel reinforcement
learning technique that allows for intelligent initialization of a neural
network weights and architecture. Our approach permits the encoding domain
knowledge directly into a neural decision tree, and improves upon that
knowledge with policy gradient updates. We empirically validate our approach on
two OpenAI Gym tasks and two modified StarCraft 2 tasks, showing that our novel
architecture outperforms multilayer-perceptron and recurrent architectures. Our
knowledge-based framework finds superior policies compared to imitation
learning-based and prior knowledge-based approaches. Importantly, we
demonstrate that our approach can be used by untrained humans to initially
provide >80% increase in expected reward relative to baselines prior to
training (p < 0.001), which results in a >60% increase in expected reward after
policy optimization (p = 0.011).",1234,84
"['cs.LG', 'cs.AI', 'stat.ML']",Theoretically Expressive and Edge-aware Graph Learning,"We propose a new Graph Neural Network that combines recent advancements in
the field. We give theoretical contributions by proving that the model is
strictly more general than the Graph Isomorphism Network and the Gated Graph
Neural Network, as it can approximate the same functions and deal with
arbitrary edge values. Then, we show how a single node information can flow
through the graph unchanged.",401,54
['cs.CV'],"Motion-Based Generator Model: Unsupervised Disentanglement of Appearance, Trackable and Intrackable Motions in Dynamic Patterns","Dynamic patterns are characterized by complex spatial and motion patterns.
Understanding dynamic patterns requires a disentangled representational model
that separates the factorial components. A commonly used model for dynamic
patterns is the state space model, where the state evolves over time according
to a transition model and the state generates the observed image frames
according to an emission model. To model the motions explicitly, it is natural
for the model to be based on the motions or the displacement fields of the
pixels. Thus in the emission model, we let the hidden state generate the
displacement field, which warps the trackable component in the previous image
frame to generate the next frame while adding a simultaneously emitted residual
image to account for the change that cannot be explained by the deformation.
The warping of the previous image is about the trackable part of the change of
image frame, while the residual image is about the intrackable part of the
image. We use a maximum likelihood algorithm to learn the model that iterates
between inferring latent noise vectors that drive the transition model and
updating the parameters given the inferred latent vectors. Meanwhile we adopt a
regularization term to penalize the norms of the residual images to encourage
the model to explain the change of image frames by trackable motion. Unlike
existing methods on dynamic patterns, we learn our model in unsupervised
setting without ground truth displacement fields. In addition, our model
defines a notion of intrackability by the separation of warped component and
residual component in each image frame. We show that our method can synthesize
realistic dynamic pattern, and disentangling appearance, trackable and
intrackable motions. The learned models are useful for motion transfer, and it
is natural to adopt it to define and measure intrackability of a dynamic
pattern.",1915,127
"['cs.LG', 'cs.AI', 'stat.ML']",Lyapunov-based uncertainty-aware safe reinforcement learning,"Reinforcement learning (RL) has shown a promising performance in learning
optimal policies for a variety of sequential decision-making tasks. However, in
many real-world RL problems, besides optimizing the main objectives, the agent
is expected to satisfy a certain level of safety (e.g., avoiding collisions in
autonomous driving). While RL problems are commonly formalized as Markov
decision processes (MDPs), safety constraints are incorporated via constrained
Markov decision processes (CMDPs). Although recent advances in safe RL have
enabled learning safe policies in CMDPs, these safety requirements should be
satisfied during both training and in the deployment process. Furthermore, it
is shown that in memory-based and partially observable environments, these
methods fail to maintain safety over unseen out-of-distribution observations.
To address these limitations, we propose a Lyapunov-based uncertainty-aware
safe RL model. The introduced model adopts a Lyapunov function that converts
trajectory-based constraints to a set of local linear constraints. Furthermore,
to ensure the safety of the agent in highly uncertain environments, an
uncertainty quantification method is developed that enables identifying
risk-averse actions through estimating the probability of constraint
violations. Moreover, a Transformers model is integrated to provide the agent
with memory to process long time horizons of information via the self-attention
mechanism. The proposed model is evaluated in grid-world navigation tasks where
safety is defined as avoiding static and dynamic obstacles in fully and
partially observable environments. The results of these experiments show a
significant improvement in the performance of the agent both in achieving
optimality and satisfying safety constraints.",1797,60
['cs.CV'],Effect of Annotation Errors on Drone Detection with YOLOv3,"Following the recent advances in deep networks, object detection and tracking
algorithms with deep learning backbones have been improved significantly;
however, this rapid development resulted in the necessity of large amounts of
annotated labels. Even if the details of such semi-automatic annotation
processes for most of these datasets are not known precisely, especially for
the video annotations, some automated labeling processes are usually employed.
Unfortunately, such approaches might result with erroneous annotations. In this
work, different types of annotation errors for object detection problem are
simulated and the performance of a popular state-of-the-art object detector,
YOLOv3, with erroneous annotations during training and testing stages is
examined. Moreover, some inevitable annotation errors in CVPR-2020 Anti-UAV
Challenge dataset is also examined in this manner, while proposing a solution
to correct such annotation errors of this valuable data set.",978,58
"['cs.LG', 'stat.ML']",The Mirage of Action-Dependent Baselines in Reinforcement Learning,"Policy gradient methods are a widely used class of model-free reinforcement
learning algorithms where a state-dependent baseline is used to reduce gradient
estimator variance. Several recent papers extend the baseline to depend on both
the state and action and suggest that this significantly reduces variance and
improves sample efficiency without introducing bias into the gradient
estimates. To better understand this development, we decompose the variance of
the policy gradient estimator and numerically show that learned
state-action-dependent baselines do not in fact reduce variance over a
state-dependent baseline in commonly tested benchmark domains. We confirm this
unexpected result by reviewing the open-source code accompanying these prior
papers, and show that subtle implementation decisions cause deviations from the
methods presented in the papers and explain the source of the previously
observed empirical gains. Furthermore, the variance decomposition highlights
areas for improvement, which we demonstrate by illustrating a simple change to
the typical value function parameterization that can significantly improve
performance.",1150,66
['cs.CV'],Dynamic Facial Expression Generation on Hilbert Hypersphere with Conditional Wasserstein Generative Adversarial Nets,"In this work, we propose a novel approach for generating videos of the six
basic facial expressions given a neutral face image. We propose to exploit the
face geometry by modeling the facial landmarks motion as curves encoded as
points on a hypersphere. By proposing a conditional version of manifold-valued
Wasserstein generative adversarial network (GAN) for motion generation on the
hypersphere, we learn the distribution of facial expression dynamics of
different classes, from which we synthesize new facial expression motions. The
resulting motions can be transformed to sequences of landmarks and then to
images sequences by editing the texture information using another conditional
Generative Adversarial Network. To the best of our knowledge, this is the first
work that explores manifold-valued representations with GAN to address the
problem of dynamic facial expression generation. We evaluate our proposed
approach both quantitatively and qualitatively on two public datasets;
Oulu-CASIA and MUG Facial Expression. Our experimental results demonstrate the
effectiveness of our approach in generating realistic videos with continuous
motion, realistic appearance and identity preservation. We also show the
efficiency of our framework for dynamic facial expressions generation, dynamic
facial expression transfer and data augmentation for training improved emotion
recognition models.",1396,116
"['cs.LG', 'cs.CV']",Parzen Window Approximation on Riemannian Manifold,"In graph motivated learning, label propagation largely depends on data
affinity represented as edges between connected data points. The affinity
assignment implicitly assumes even distribution of data on the manifold. This
assumption may not hold and may lead to inaccurate metric assignment due to
drift towards high-density regions. The drift affected heat kernel based
affinity with a globally fixed Parzen window either discards genuine neighbors
or forces distant data points to become a member of the neighborhood. This
yields a biased affinity matrix. In this paper, the bias due to uneven data
sampling on the Riemannian manifold is catered to by a variable Parzen window
determined as a function of neighborhood size, ambient dimension, flatness
range, etc. Additionally, affinity adjustment is used which offsets the effect
of uneven sampling responsible for the bias. An affinity metric which takes
into consideration the irregular sampling effect to yield accurate label
propagation is proposed. Extensive experiments on synthetic and real-world data
sets confirm that the proposed method increases the classification accuracy
significantly and outperforms existing Parzen window estimators in graph
Laplacian manifold regularization methods.",1254,50
"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",How Much Can I Trust You? -- Quantifying Uncertainties in Explaining Neural Networks,"Explainable AI (XAI) aims to provide interpretations for predictions made by
learning machines, such as deep neural networks, in order to make the machines
more transparent for the user and furthermore trustworthy also for applications
in e.g. safety-critical areas. So far, however, no methods for quantifying
uncertainties of explanations have been conceived, which is problematic in
domains where a high confidence in explanations is a prerequisite. We therefore
contribute by proposing a new framework that allows to convert any arbitrary
explanation method for neural networks into an explanation method for Bayesian
neural networks, with an in-built modeling of uncertainties. Within the
Bayesian framework a network's weights follow a distribution that extends
standard single explanation scores and heatmaps to distributions thereof, in
this manner translating the intrinsic network model uncertainties into a
quantification of explanation uncertainties. This allows us for the first time
to carve out uncertainties associated with a model explanation and subsequently
gauge the appropriate level of explanation confidence for a user (using
percentiles). We demonstrate the effectiveness and usefulness of our approach
extensively in various experiments, both qualitatively and quantitatively.",1301,84
"['cs.LG', 'math.OC', 'stat.ML']",Loss landscapes and optimization in over-parameterized non-linear systems and neural networks,"The success of deep learning is due, to a large extent, to the remarkable
effectiveness of gradient-based optimization methods applied to large neural
networks. The purpose of this work is to propose a modern view and a general
mathematical framework for loss landscapes and efficient optimization in
over-parameterized machine learning models and systems of non-linear equations,
a setting that includes over-parameterized deep neural networks. Our starting
observation is that optimization problems corresponding to such systems are
generally not convex, even locally. We argue that instead they satisfy PL$^*$,
a variant of the Polyak-Lojasiewicz condition on most (but not all) of the
parameter space, which guarantees both the existence of solutions and efficient
optimization by (stochastic) gradient descent (SGD/GD). The PL$^*$ condition of
these systems is closely related to the condition number of the tangent kernel
associated to a non-linear system showing how a PL$^*$-based non-linear theory
parallels classical analyses of over-parameterized linear equations. We show
that wide neural networks satisfy the PL$^*$ condition, which explains the
(S)GD convergence to a global minimum. Finally we propose a relaxation of the
PL$^*$ condition applicable to ""almost"" over-parameterized systems.",1304,93
"['cs.CV', 'cs.CL', 'cs.LG']",VisualBERT: A Simple and Performant Baseline for Vision and Language,"We propose VisualBERT, a simple and flexible framework for modeling a broad
range of vision-and-language tasks. VisualBERT consists of a stack of
Transformer layers that implicitly align elements of an input text and regions
in an associated input image with self-attention. We further propose two
visually-grounded language model objectives for pre-training VisualBERT on
image caption data. Experiments on four vision-and-language tasks including
VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with
state-of-the-art models while being significantly simpler. Further analysis
demonstrates that VisualBERT can ground elements of language to image regions
without any explicit supervision and is even sensitive to syntactic
relationships, tracking, for example, associations between verbs and image
regions corresponding to their arguments.",866,68
['cs.CV'],Semantic Flow-guided Motion Removal Method for Robust Mapping,"Moving objects in scenes are still a severe challenge for the SLAM system.
Many efforts have tried to remove the motion regions in the images by detecting
moving objects. In this way, the keypoints belonging to motion regions will be
ignored in the later calculations. In this paper, we proposed a novel motion
removal method, leveraging semantic information and optical flow to extract
motion regions. Different from previous works, we don't predict moving objects
or motion regions directly from image sequences. We computed rigid optical
flow, synthesized by the depth and pose, and compared it against the estimated
optical flow to obtain initial motion regions. Then, we utilized K-means to
finetune the motion region masks with instance segmentation masks. The
ORB-SLAM2 integrated with the proposed motion removal method achieved the best
performance in both indoor and outdoor dynamic environments.",906,61
"['cs.CV', 'eess.IV']",Improved Image Generation via Sparse Modeling,"The interest of the deep learning community in image synthesis has grown
massively in recent years. Nowadays, deep generative methods, and especially
Generative Adversarial Networks (GANs), are leading to state-of-the-art
performance, capable of synthesizing images that appear realistic. While the
efforts for improving the quality of the generated images are extensive, most
attempts still consider the generator part as an uncorroborated ""black-box"". In
this paper, we aim to provide a better understanding and design of the image
generation process. We interpret existing generators as implicitly relying on
sparsity-inspired models. More specifically, we show that generators can be
viewed as manifestations of the Convolutional Sparse Coding (CSC) and its
Multi-Layered version (ML-CSC) synthesis processes. We leverage this
observation by explicitly enforcing a sparsifying regularization on
appropriately chosen activation layers in the generator, and demonstrate that
this leads to improved image synthesis. Furthermore, we show that the same
rationale and benefits apply to generators serving inverse problems,
demonstrated on the Deep Image Prior (DIP) method.",1171,45
['cs.CV'],Domain Adaptive Object Detection via Asymmetric Tri-way Faster-RCNN,"Conventional object detection models inevitably encounter a performance drop
as the domain disparity exists. Unsupervised domain adaptive object detection
is proposed recently to reduce the disparity between domains, where the source
domain is label-rich while the target domain is label-agnostic. The existing
models follow a parameter shared siamese structure for adversarial domain
alignment, which, however, easily leads to the collapse and out-of-control risk
of the source domain and brings negative impact to feature adaption. The main
reason is that the labeling unfairness (asymmetry) between source and target
makes the parameter sharing mechanism unable to adapt. Therefore, in order to
avoid the source domain collapse risk caused by parameter sharing, we propose
an asymmetric tri-way Faster-RCNN (ATF) for domain adaptive object detection.
Our ATF model has two distinct merits: 1) A ancillary net supervised by source
label is deployed to learn ancillary target features and simultaneously
preserve the discrimination of source domain, which enhances the structural
discrimination (object classification vs. bounding box regression) of domain
alignment. 2) The asymmetric structure consisting of a chief net and an
independent ancillary net essentially overcomes the parameter sharing aroused
source risk collapse. The adaption safety of the proposed ATF detector is
guaranteed. Extensive experiments on a number of datasets, including
Cityscapes, Foggy-cityscapes, KITTI, Sim10k, Pascal VOC, Clipart and
Watercolor, demonstrate the SOTA performance of our method.",1579,67
['cs.CV'],An Investigation of Feature Selection and Transfer Learning for Writer-Independent Offline Handwritten Signature Verification,"SigNet is a state of the art model for feature representation used for
handwritten signature verification (HSV). This representation is based on a
Deep Convolutional Neural Network (DCNN) and contains 2048 dimensions. When
transposed to a dissimilarity space generated by the dichotomy transformation
(DT), related to the writer-independent (WI) approach, these features may
include redundant information. This paper investigates the presence of
overfitting when using Binary Particle Swarm Optimization (BPSO) to perform the
feature selection in a wrapper mode. We proposed a method based on a global
validation strategy with an external archive to control overfitting during the
search for the most discriminant representation. Moreover, an investigation is
also carried out to evaluate the use of the selected features in a transfer
learning context. The analysis is carried out on a writer-independent approach
on the CEDAR, MCYT and GPDS datasets. The experimental results showed the
presence of overfitting when no validation is used during the optimization
process and the improvement when the global validation strategy with an
external archive is used. Also, the space generated after feature selection can
be used in a transfer learning context.",1255,125
"['cs.CV', 'cs.AI']",What Would You Expect? Anticipating Egocentric Actions with Rolling-Unrolling LSTMs and Modality Attention,"Egocentric action anticipation consists in understanding which objects the
camera wearer will interact with in the near future and which actions they will
perform. We tackle the problem proposing an architecture able to anticipate
actions at multiple temporal scales using two LSTMs to 1) summarize the past,
and 2) formulate predictions about the future. The input video is processed
considering three complimentary modalities: appearance (RGB), motion (optical
flow) and objects (object-based features). Modality-specific predictions are
fused using a novel Modality ATTention (MATT) mechanism which learns to weigh
modalities in an adaptive fashion. Extensive evaluations on two large-scale
benchmark datasets show that our method outperforms prior art by up to +7% on
the challenging EPIC-Kitchens dataset including more than 2500 actions, and
generalizes to EGTEA Gaze+. Our approach is also shown to generalize to the
tasks of early action recognition and action recognition. Our method is ranked
first in the public leaderboard of the EPIC-Kitchens egocentric action
anticipation challenge 2019. Please see our web pages for code and examples:
http://iplab.dmi.unict.it/rulstm - https://github.com/fpv-iplab/rulstm.",1222,106
"['cs.LG', 'stat.ML']",Gradient Normalization & Depth Based Decay For Deep Learning,"In this paper we introduce a novel method of gradient normalization and decay
with respect to depth. Our method leverages the simple concept of normalizing
all gradients in a deep neural network, and then decaying said gradients with
respect to their depth in the network. Our proposed normalization and decay
techniques can be used in conjunction with most current state of the art
optimizers and are a very simple addition to any network. This method, although
simple, showed improvements in convergence time on state of the art networks
such as DenseNet and ResNet on image classification tasks, as well as on an
LSTM for natural language processing tasks.",659,60
"['cs.LG', 'cs.AI', 'stat.ML']",Homophily Outlier Detection in Non-IID Categorical Data,"Most of existing outlier detection methods assume that the outlier factors
(i.e., outlierness scoring measures) of data entities (e.g., feature values and
data objects) are Independent and Identically Distributed (IID). This
assumption does not hold in real-world applications where the outlierness of
different entities is dependent on each other and/or taken from different
probability distributions (non-IID). This may lead to the failure of detecting
important outliers that are too subtle to be identified without considering the
non-IID nature. The issue is even intensified in more challenging contexts,
e.g., high-dimensional data with many noisy features. This work introduces a
novel outlier detection framework and its two instances to identify outliers in
categorical data by capturing non-IID outlier factors. Our approach first
defines and incorporates distribution-sensitive outlier factors and their
interdependence into a value-value graph-based representation. It then models
an outlierness propagation process in the value graph to learn the outlierness
of feature values. The learned value outlierness allows for either direct
outlier detection or outlying feature selection. The graph representation and
mining approach is employed here to well capture the rich non-IID
characteristics. Our empirical results on 15 real-world data sets with
different levels of data complexities show that (i) the proposed outlier
detection methods significantly outperform five state-of-the-art methods at the
95%/99% confidence level, achieving 10%-28% AUC improvement on the 10 most
complex data sets; and (ii) the proposed feature selection methods
significantly outperform three competing methods in enabling subsequent outlier
detection of two different existing detectors.",1783,55
"['cs.LG', 'stat.ML']",Domain-adversarial Network Alignment,"Network alignment is a critical task to a wide variety of fields. Many
existing works leverage on representation learning to accomplish this task
without eliminating domain representation bias induced by domain-dependent
features, which yield inferior alignment performance. This paper proposes a
unified deep architecture (DANA) to obtain a domain-invariant representation
for network alignment via an adversarial domain classifier. Specifically, we
employ the graph convolutional networks to perform network embedding under the
domain adversarial principle, given a small set of observed anchors. Then, the
semi-supervised learning framework is optimized by maximizing a posterior
probability distribution of observed anchors and the loss of a domain
classifier simultaneously. We also develop a few variants of our model, such
as, direction-aware network alignment, weight-sharing for directed networks and
simplification of parameter space. Experiments on three real-world social
network datasets demonstrate that our proposed approaches achieve
state-of-the-art alignment results.",1085,36
['cs.LG'],"Nonnegative Spectral Analysis with Adaptive Graph and $L_{2,0}$-Norm Regularization for Unsupervised Feature Selection","Feature selection is used to reduce feature dimension while maintain model's
performance, which has been an important data preprocessing in many fields.
Since obtaining annotated data is laborious or even infeasible in many cases,
unsupervised feature selection is more practical in reality. Although a lots of
methods have been proposed, these methods select features independently, thus
it is no guarantee that the group of selected features is optimal. What's more,
the number of selected features must be tuned carefully to get a satisfactory
result. In this paper, we propose a novel unsupervised feature selection method
which incorporate spectral analysis with a $l_{2,0}$-norm regularized term.
After optimization, a group of optimal features will be selected, and the
number of selected features will be determined automatically. What's more, a
nonnegative constraint with respect to the class indicators is imposed to learn
more accurate cluster labels, and a graph regularized term is added to learn
the similarity matrix adaptively. An efficient and simple iterative algorithm
is designed to optimize the proposed problem. Experiments on six different
benchmark data sets validate the effectiveness of the proposed approach.",1236,118
"['cs.LG', 'cs.AI', 'stat.ML']",A Composable Specification Language for Reinforcement Learning Tasks,"Reinforcement learning is a promising approach for learning control policies
for robot tasks. However, specifying complex tasks (e.g., with multiple
objectives and safety constraints) can be challenging, since the user must
design a reward function that encodes the entire task. Furthermore, the user
often needs to manually shape the reward to ensure convergence of the learning
algorithm. We propose a language for specifying complex control tasks, along
with an algorithm that compiles specifications in our language into a reward
function and automatically performs reward shaping. We implement our approach
in a tool called SPECTRL, and show that it outperforms several state-of-the-art
baselines.",702,68
"['cs.LG', 'cs.NE', 'stat.ML']",Sum-Product-Quotient Networks,"We present a novel tractable generative model that extends Sum-Product
Networks (SPNs) and significantly boosts their power. We call it
Sum-Product-Quotient Networks (SPQNs), whose core concept is to incorporate
conditional distributions into the model by direct computation using quotient
nodes, e.g. $P(A|B) = \frac{P(A,B)}{P(B)}$. We provide sufficient conditions
for the tractability of SPQNs that generalize and relax the decomposable and
complete tractability conditions of SPNs. These relaxed conditions give rise to
an exponential boost to the expressive efficiency of our model, i.e. we prove
that there are distributions which SPQNs can compute efficiently but require
SPNs to be of exponential size. Thus, we narrow the gap in expressivity between
tractable graphical models and other Neural Network-based generative models.",835,29
['cs.CV'],Spatial Pyramid Based Graph Reasoning for Semantic Segmentation,"The convolution operation suffers from a limited receptive filed, while
global modeling is fundamental to dense prediction tasks, such as semantic
segmentation. In this paper, we apply graph convolution into the semantic
segmentation task and propose an improved Laplacian. The graph reasoning is
directly performed in the original feature space organized as a spatial
pyramid. Different from existing methods, our Laplacian is data-dependent and
we introduce an attention diagonal matrix to learn a better distance metric. It
gets rid of projecting and re-projecting processes, which makes our proposed
method a light-weight module that can be easily plugged into current computer
vision architectures. More importantly, performing graph reasoning directly in
the feature space retains spatial relationships and makes spatial pyramid
possible to explore multiple long-range contextual patterns from different
scales. Experiments on Cityscapes, COCO Stuff, PASCAL Context and PASCAL VOC
demonstrate the effectiveness of our proposed methods on semantic segmentation.
We achieve comparable performance with advantages in computational and memory
overhead.",1154,63
"['cs.CV', 'cs.DS']",The Minimum Edit Arborescence Problem and Its Use in Compressing Graph Collections [Extended Version],"The inference of minimum spanning arborescences within a set of objects is a
general problem which translates into numerous application-specific
unsupervised learning tasks. We introduce a unified and generic structure
called edit arborescence that relies on edit paths between data in a
collection, as well as the Min Edit Arborescence Problem, which asks for an
edit arborescence that minimizes the sum of costs of its inner edit paths.
Through the use of suitable cost functions, this generic framework allows to
model a variety of problems. In particular, we show that by introducing
encoding size preserving edit costs, it can be used as an efficient method for
compressing collections of labeled graphs. Experiments on various graph
datasets, with comparisons to standard compression tools, show the potential of
our method.",830,101
"['cs.CV', 'cs.AI', 'cs.LG']",Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection,"Despite progress in visual perception tasks such as image classification and
detection, computers still struggle to understand the interdependency of
objects in the scene as a whole, e.g., relations between objects or their
attributes. Existing methods often ignore global context cues capturing the
interactions among different object instances, and can only recognize a handful
of types by exhaustively training individual detectors for all possible
relationships. To capture such global interdependency, we propose a deep
Variation-structured Reinforcement Learning (VRL) framework to sequentially
discover object relationships and attributes in the whole image. First, a
directed semantic action graph is built using language priors to provide a rich
and compact representation of semantic correlations between object categories,
predicates, and attributes. Next, we use a variation-structured traversal over
the action graph to construct a small, adaptive action set for each step based
on the current state and historical actions. In particular, an ambiguity-aware
object mining scheme is used to resolve semantic ambiguity among object
categories that the object detector fails to distinguish. We then make
sequential predictions using a deep RL framework, incorporating global context
cues and semantic embeddings of previously extracted phrases in the state
vector. Our experiments on the Visual Relationship Detection (VRD) dataset and
the large-scale Visual Genome dataset validate the superiority of VRL, which
can achieve significantly better detection results on datasets involving
thousands of relationship and attribute types. We also demonstrate that VRL is
able to predict unseen types embedded in our action graph by learning
correlations on shared graph nodes.",1780,96
['cs.CV'],SCSP: Spectral Clustering Filter Pruning with Soft Self-adaption Manners,"Deep Convolutional Neural Networks (CNN) has achieved significant success in
computer vision field. However, the high computational cost of the deep complex
models prevents the deployment on edge devices with limited memory and
computational resource. In this paper, we proposed a novel filter pruning for
convolutional neural networks compression, namely spectral clustering filter
pruning with soft self-adaption manners (SCSP). We first apply spectral
clustering on filters layer by layer to explore their intrinsic connections and
only count on efficient groups. By self-adaption manners, the pruning
operations can be done in few epochs to let the network gradually choose
meaningful groups. According to this strategy, we not only achieve model
compression while keeping considerable performance, but also find a novel angle
to interpret the model compression process.",874,72
['cs.LG'],Gradient Normalization for Generative Adversarial Networks,"In this paper, we propose a novel normalization method called gradient
normalization (GN) to tackle the training instability of Generative Adversarial
Networks (GANs) caused by the sharp gradient space. Unlike existing work such
as gradient penalty and spectral normalization, the proposed GN only imposes a
hard 1-Lipschitz constraint on the discriminator function, which increases the
capacity of the discriminator. Moreover, the proposed gradient normalization
can be applied to different GAN architectures with little modification.
Extensive experiments on four datasets show that GANs trained with gradient
normalization outperform existing methods in terms of both Frechet Inception
Distance and Inception Score.",718,58
['cs.CV'],T-Net: Deep Stacked Scale-Iteration Network for Image Dehazing,"Hazy images reduce the visibility of the image content, and haze will lead to
failure in handling subsequent computer vision tasks. In this paper, we address
the problem of image dehazing by proposing a dehazing network named T-Net,
which consists of a backbone network based on the U-Net architecture and a dual
attention module. And it can achieve multi-scale feature fusion by using skip
connections with a new fusion strategy. Furthermore, by repeatedly unfolding
the plain T-Net, Stack T-Net is proposed to take advantage of the dependence of
deep features across stages via a recursive strategy. In order to reduce
network parameters, the intra-stage recursive computation of ResNet is adopted
in our Stack T-Net. And we take both the stage-wise result and the original
hazy image as input to each T-Net and finally output the prediction of clean
image. Experimental results on both synthetic and real-world images demonstrate
that our plain T-Net and the advanced Stack T-Net perform favorably against the
state-of-the-art dehazing algorithms, and show that our Stack T-Net could
further improve the dehazing effect, demonstrating the effectiveness of the
recursive strategy.",1182,62
"['cs.LG', 'stat.ML']",Regret Bounds for Discounted MDPs,"Reinforcement learning (RL) has traditionally been understood from an
episodic perspective; the concept of non-episodic RL, where there is no restart
and therefore no reliable recovery, remains elusive. A fundamental question in
non-episodic RL is how to measure the performance of a learner and derive
algorithms to maximize such performance. Conventional wisdom is to maximize the
difference between the average reward received by the learner and the maximal
long-term average reward. In this paper, we argue that if the total time budget
is relatively limited compared to the complexity of the environment, such
comparison may fail to reflect the finite-time optimality of the learner. We
propose a family of measures, called $\gamma$-regret, which we believe to
better capture the finite-time optimality. We give motivations and derive lower
and upper bounds for such measures. Note: A follow-up work (arXiv:2010.00587)
has improved both our lower and upper bound, the gap is now closed at
$\tilde{\Theta}\left(\frac{\sqrt{SAT}}{(1 - \gamma)^{\frac{1}{2}}}\right)$.",1069,33
"['cs.LG', 'cs.CL', 'cs.IR', 'stat.ML']",Transfer Learning Between Related Tasks Using Expected Label Proportions,"Deep learning systems thrive on abundance of labeled training data but such
data is not always available, calling for alternative methods of supervision.
One such method is expectation regularization (XR) (Mann and McCallum, 2007),
where models are trained based on expected label proportions. We propose a
novel application of the XR framework for transfer learning between related
tasks, where knowing the labels of task A provides an estimation of the label
proportion of task B. We then use a model trained for A to label a large
corpus, and use this corpus with an XR loss to train a model for task B. To
make the XR framework applicable to large-scale deep-learning setups, we
propose a stochastic batched approximation procedure. We demonstrate the
approach on the task of Aspect-based Sentiment classification, where we
effectively use a sentence-level sentiment predictor to train accurate
aspect-based predictor. The method improves upon fully supervised neural system
trained on aspect-level data, and is also cumulative with LM-based pretraining,
as we demonstrate by improving a BERT-based Aspect-based Sentiment model.",1132,72
"['cs.LG', 'cs.AI', 'stat.ML']","Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher","Knowledge distillation is a strategy of training a student network with guide
of the soft output from a teacher network. It has been a successful method of
model compression and knowledge transfer. However, currently knowledge
distillation lacks a convincing theoretical understanding. On the other hand,
recent finding on neural tangent kernel enables us to approximate a wide neural
network with a linear model of the network's random features. In this paper, we
theoretically analyze the knowledge distillation of a wide neural network.
First we provide a transfer risk bound for the linearized model of the network.
Then we propose a metric of the task's training difficulty, called data
inefficiency. Based on this metric, we show that for a perfect teacher, a high
ratio of teacher's soft labels can be beneficial. Finally, for the case of
imperfect teacher, we find that hard labels can correct teacher's wrong
prediction, which explains the practice of mixing hard and soft labels.",989,97
"['cs.LG', 'math.NA', 'stat.ML']",Krylov Subspace Recycling for Fast Iterative Least-Squares in Machine Learning,"Solving symmetric positive definite linear problems is a fundamental
computational task in machine learning. The exact solution, famously, is
cubicly expensive in the size of the matrix. To alleviate this problem, several
linear-time approximations, such as spectral and inducing-point methods, have
been suggested and are now in wide use. These are low-rank approximations that
choose the low-rank space a priori and do not refine it over time. While this
allows linear cost in the data-set size, it also causes a finite, uncorrected
approximation error. Authors from numerical linear algebra have explored ways
to iteratively refine such low-rank approximations, at a cost of a small number
of matrix-vector multiplications. This idea is particularly interesting in the
many situations in machine learning where one has to solve a sequence of
related symmetric positive definite linear problems. From the machine learning
perspective, such deflation methods can be interpreted as transfer learning of
a low-rank approximation across a time-series of numerical tasks. We study the
use of such methods for our field. Our empirical results show that, on
regression and classification problems of intermediate size, this approach can
interpolate between low computational cost and numerical precision.",1299,78
['cs.CV'],Red blood cell image generation for data augmentation using Conditional Generative Adversarial Networks,"In this paper, we describe how to apply image-to-image translation techniques
to medical blood smear data to generate new data samples and meaningfully
increase small datasets. Specifically, given the segmentation mask of the
microscopy image, we are able to generate photorealistic images of blood cells
which are further used alongside real data during the network training for
segmentation and object detection tasks. This image data generation approach is
based on conditional generative adversarial networks which have proven
capabilities to high-quality image synthesis. In addition to synthesizing blood
images, we synthesize segmentation mask as well which leads to a diverse
variety of generated samples. The effectiveness of the technique is thoroughly
analyzed and quantified through a number of experiments on a manually collected
and annotated dataset of blood smear taken under a microscope.",905,103
"['cs.CV', 'cs.GR', 'cs.LG']",DeepCAD: A Deep Generative Network for Computer-Aided Design Models,"Deep generative models of 3D shapes have received a great deal of research
interest. Yet, almost all of them generate discrete shape representations, such
as voxels, point clouds, and polygon meshes. We present the first 3D generative
model for a drastically different shape representation --- describing a shape
as a sequence of computer-aided design (CAD) operations. Unlike meshes and
point clouds, CAD models encode the user creation process of 3D shapes, widely
used in numerous industrial and engineering design tasks. However, the
sequential and irregular structure of CAD operations poses significant
challenges for existing 3D generative models. Drawing an analogy between CAD
operations and natural language, we propose a CAD generative network based on
the Transformer. We demonstrate the performance of our model for both shape
autoencoding and random shape generation. To train our network, we create a new
CAD dataset consisting of 178,238 models and their CAD construction sequences.
We have made this dataset publicly available to promote future research on this
topic.",1085,67
"['cs.LG', 'cs.AI', 'cs.RO', 'I.2.6; I.2.9']",Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search,"In principle, reinforcement learning and policy search methods can enable
robots to learn highly complex and general skills that may allow them to
function amid the complexity and diversity of the real world. However, training
a policy that generalizes well across a wide range of real-world conditions
requires far greater quantity and diversity of experience than is practical to
collect with a single robot. Fortunately, it is possible for multiple robots to
share their experience with one another, and thereby, learn a policy
collectively. In this work, we explore distributed and asynchronous policy
learning as a means to achieve generalization and improved training times on
challenging, real-world manipulation tasks. We propose a distributed and
asynchronous version of Guided Policy Search and use it to demonstrate
collective policy learning on a vision-based door opening task using four
robots. We show that it achieves better generalization, utilization, and
training times than the single robot alternative.",1023,90
"['cs.LG', 'cs.AI']",Temporal Knowledge Graph Forecasting with Neural ODE,"Learning node representation on dynamically-evolving, multi-relational graph
data has gained great research interest. However, most of the existing models
for temporal knowledge graph forecasting use Recurrent Neural Network (RNN)
with discrete depth to capture temporal information, while time is a continuous
variable. Inspired by Neural Ordinary Differential Equation (NODE), we extend
the idea of continuum-depth models to time-evolving multi-relational graph
data, and propose a novel Temporal Knowledge Graph Forecasting model with NODE.
Our model captures temporal information through NODE and structural information
through a Graph Neural Network (GNN). Thus, our graph ODE model achieves a
continuous model in time and efficiently learns node representation for future
prediction. We evaluate our model on six temporal knowledge graph datasets by
performing link forecasting. Experiment results show the superiority of our
model.",938,52
['stat.ML'],Learning a Common Substructure of Multiple Graphical Gaussian Models,"Properties of data are frequently seen to vary depending on the sampled
situations, which usually changes along a time evolution or owing to
environmental effects. One way to analyze such data is to find invariances, or
representative features kept constant over changes. The aim of this paper is to
identify one such feature, namely interactions or dependencies among variables
that are common across multiple datasets collected under different conditions.
To that end, we propose a common substructure learning (CSSL) framework based
on a graphical Gaussian model. We further present a simple learning algorithm
based on the Dual Augmented Lagrangian and the Alternating Direction Method of
Multipliers. We confirm the performance of CSSL over other existing techniques
in finding unchanging dependency structures in multiple datasets through
numerical simulations on synthetic data and through a real world application to
anomaly detection in automobile sensors.",965,68
['cs.CV'],Pointwise Rotation-Invariant Network with Adaptive Sampling and 3D Spherical Voxel Convolution,"Point cloud analysis without pose priors is very challenging in real
applications, as the orientations of point clouds are often unknown. In this
paper, we propose a brand new point-set learning framework PRIN, namely,
Pointwise Rotation-Invariant Network, focusing on rotation-invariant feature
extraction in point clouds analysis. We construct spherical signals by Density
Aware Adaptive Sampling to deal with distorted point distributions in spherical
space. In addition, we propose Spherical Voxel Convolution and Point
Re-sampling to extract rotation-invariant features for each point. Our network
can be applied to tasks ranging from object classification, part segmentation,
to 3D feature matching and label alignment. We show that, on the dataset with
randomly rotated point clouds, PRIN demonstrates better performance than
state-of-the-art methods without any data augmentation. We also provide
theoretical analysis for the rotation-invariance achieved by our methods.",978,94
"['cs.LG', 'stat.ML']",FedMD: Heterogenous Federated Learning via Model Distillation,"Federated learning enables the creation of a powerful centralized model
without compromising data privacy of multiple participants. While successful,
it does not incorporate the case where each participant independently designs
its own model. Due to intellectual property concerns and heterogeneous nature
of tasks and data, this is a widespread requirement in applications of
federated learning to areas such as health care and AI as a service. In this
work, we use transfer learning and knowledge distillation to develop a
universal framework that enables federated learning when each agent owns not
only their private data, but also uniquely designed models. We test our
framework on the MNIST/FEMNIST dataset and the CIFAR10/CIFAR100 dataset and
observe fast improvement across all participating models. With 10 distinct
participants, the final test accuracy of each model on average receives a 20%
gain on top of what's possible without collaboration and is only a few percent
lower than the performance each model would have obtained if all private
datasets were pooled and made directly available for all participants.",1125,61
"['cs.LG', 'stat.ML']",Neural Rule Ensembles: Encoding Sparse Feature Interactions into Neural Networks,"Artificial Neural Networks form the basis of very powerful learning methods.
It has been observed that a naive application of fully connected neural
networks to data with many irrelevant variables often leads to overfitting. In
an attempt to circumvent this issue, a prior knowledge pertaining to what
features are relevant and their possible feature interactions can be encoded
into these networks. In this work, we use decision trees to capture such
relevant features and their interactions and define a mapping to encode
extracted relationships into a neural network. This addresses the
initialization related concern of fully connected neural networks. At the same
time through feature selection it enables learning of compact representations
compared to state of the art tree-based approaches. Empirical evaluations and
simulation studies show the superiority of such an approach over fully
connected neural networks and tree-based approaches",947,80
['cs.CV'],PDWN: Pyramid Deformable Warping Network for Video Interpolation,"Video interpolation aims to generate a non-existent intermediate frame given
the past and future frames. Many state-of-the-art methods achieve promising
results by estimating the optical flow between the known frames and then
generating the backward flows between the middle frame and the known frames.
However, these methods usually suffer from the inaccuracy of estimated optical
flows and require additional models or information to compensate for flow
estimation errors. Following the recent development in using deformable
convolution (DConv) for video interpolation, we propose a light but effective
model, called Pyramid Deformable Warping Network (PDWN). PDWN uses a pyramid
structure to generate DConv offsets of the unknown middle frame with respect to
the known frames through coarse-to-fine successive refinements. Cost volumes
between warped features are calculated at every pyramid level to help the
offset inference. At the finest scale, the two warped frames are adaptively
blended to generate the middle frame. Lastly, a context enhancement network
further enhances the contextual detail of the final output. Ablation studies
demonstrate the effectiveness of the coarse-to-fine offset refinement, cost
volumes, and DConv. Our method achieves better or on-par accuracy compared to
state-of-the-art models on multiple datasets while the number of model
parameters and the inference time are substantially less than previous models.
Moreover, we present an extension of the proposed framework to use four input
frames, which can achieve significant improvement over using only two input
frames, with only a slight increase in the model size and inference time.",1674,64
['cs.CV'],Cycle-consistent Generative Adversarial Networks for Neural Style Transfer using data from Chang'E-4,"Generative Adversarial Networks (GANs) have had tremendous applications in
Computer Vision. Yet, in the context of space science and planetary exploration
the door is open for major advances. We introduce tools to handle planetary
data from the mission Chang'E-4 and present a framework for Neural Style
Transfer using Cycle-consistency from rendered images. The experiments are
conducted in the context of the Iris Lunar Rover, a nano-rover that will be
deployed in lunar terrain in 2021 as the flagship of Carnegie Mellon, being the
first unmanned rover of America to be on the Moon.",585,100
"['cs.CV', 'cs.LG', 'eess.IV']",GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking with Multi-Feature Learning,"3D Multi-object tracking (MOT) is crucial to autonomous systems. Recent work
uses a standard tracking-by-detection pipeline, where feature extraction is
first performed independently for each object in order to compute an affinity
matrix. Then the affinity matrix is passed to the Hungarian algorithm for data
association. A key process of this standard pipeline is to learn discriminative
features for different objects in order to reduce confusion during data
association. In this work, we propose two techniques to improve the
discriminative feature learning for MOT: (1) instead of obtaining features for
each object independently, we propose a novel feature interaction mechanism by
introducing the Graph Neural Network. As a result, the feature of one object is
informed of the features of other objects so that the object feature can lean
towards the object with similar feature (i.e., object probably with a same ID)
and deviate from objects with dissimilar features (i.e., object probably with
different IDs), leading to a more discriminative feature for each object; (2)
instead of obtaining the feature from either 2D or 3D space in prior work, we
propose a novel joint feature extractor to learn appearance and motion features
from 2D and 3D space simultaneously. As features from different modalities
often have complementary information, the joint feature can be more
discriminate than feature from each individual modality. To ensure that the
joint feature extractor does not heavily rely on one modality, we also propose
an ensemble training paradigm. Through extensive evaluation, our proposed
method achieves state-of-the-art performance on KITTI and nuScenes 3D MOT
benchmarks. Our code will be made available at
https://github.com/xinshuoweng/GNN3DMOT",1771,87
['cs.CV'],Discriminative Local Sparse Representations for Robust Face Recognition,"A key recent advance in face recognition models a test face image as a sparse
linear combination of a set of training face images. The resulting sparse
representations have been shown to possess robustness against a variety of
distortions like random pixel corruption, occlusion and disguise. This approach
however makes the restrictive (in many scenarios) assumption that test faces
must be perfectly aligned (or registered) to the training data prior to
classification. In this paper, we propose a simple yet robust local block-based
sparsity model, using adaptively-constructed dictionaries from local features
in the training data, to overcome this misalignment problem. Our approach is
inspired by human perception: we analyze a series of local discriminative
features and combine them to arrive at the final classification decision. We
propose a probabilistic graphical model framework to explicitly mine the
conditional dependencies between these distinct sparse local features. In
particular, we learn discriminative graphs on sparse representations obtained
from distinct local slices of a face. Conditional correlations between these
sparse features are first discovered (in the training phase), and subsequently
exploited to bring about significant improvements in recognition rates.
Experimental results obtained on benchmark face databases demonstrate the
effectiveness of the proposed algorithms in the presence of multiple
registration errors (such as translation, rotation, and scaling) as well as
under variations of pose and illumination.",1556,71
"['cs.LG', 'cs.CY', 'stat.ML']",Crime Prediction Using Spatio-Temporal Data,"A crime is a punishable offence that is harmful for an individual and his
society. It is obvious to comprehend the patterns of criminal activity to
prevent them. Research can help society to prevent and solve crime activates.
Study shows that only 10 percent offenders commits 50 percent of the total
offences. The enforcement team can respond faster if they have early
information and pre-knowledge about crime activities of the different points of
a city. In this paper, supervised learning technique is used to predict crimes
with better accuracy. The proposed system predicts crimes by analyzing data-set
that contains records of previously committed crimes and their patterns. The
system stands on two main algorithms - i) decision tree, and ii) k-nearest
neighbor. Random Forest algorithm and Adaboost are used to increase the
accuracy of the prediction. Finally, oversampling is used for better accuracy.
The proposed system is feed with a criminal-activity data set of twelve years
of San Francisco city.",1012,43
"['cs.CV', 'cs.LG']",Automatic Pass Annotation from Soccer VideoStreams Based on Object Detection and LSTM,"Soccer analytics is attracting increasing interest in academia and industry,
thanks to the availability of data that describe all the spatio-temporal events
that occur in each match. These events (e.g., passes, shots, fouls) are
collected by human operators manually, constituting a considerable cost for
data providers in terms of time and economic resources. In this paper, we
describe PassNet, a method to recognize the most frequent events in soccer,
i.e., passes, from video streams. Our model combines a set of artificial neural
networks that perform feature extraction from video streams, object detection
to identify the positions of the ball and the players, and classification of
frame sequences as passes or not passes. We test PassNet on different
scenarios, depending on the similarity of conditions to the match used for
training. Our results show good classification results and significant
improvement in the accuracy of pass detection with respect to baseline
classifiers, even when the match's video conditions of the test and training
sets are considerably different. PassNet is the first step towards an automated
event annotation system that may break the time and the costs for event
annotation, enabling data collections for minor and non-professional divisions,
youth leagues and, in general, competitions whose matches are not currently
annotated by data providers.",1390,85
"['cs.CV', 'cs.GR']",Illumination-Invariant Image from 4-Channel Images: The Effect of Near-Infrared Data in Shadow Removal,"Removing the effect of illumination variation in images has been proved to be
beneficial in many computer vision applications such as object recognition and
semantic segmentation. Although generating illumination-invariant images has
been studied in the literature before, it has not been investigated on real
4-channel (4D) data. In this study, we examine the quality of
illumination-invariant images generated from red, green, blue, and
near-infrared (RGBN) data. Our experiments show that the near-infrared channel
substantively contributes toward removing illumination. As shown in our
numerical and visual results, the illumination-invariant image obtained by RGBN
data is superior compared to that obtained by RGB alone.",726,102
['cs.CV'],Human Action Recognition with Multi-Laplacian Graph Convolutional Networks,"Convolutional neural networks are nowadays witnessing a major success in
different pattern recognition problems. These learning models were basically
designed to handle vectorial data such as images but their extension to
non-vectorial and semi-structured data (namely graphs with variable sizes,
topology, etc.) remains a major challenge, though a few interesting solutions
are currently emerging. In this paper, we introduce MLGCN; a novel spectral
Multi-Laplacian Graph Convolutional Network. The main contribution of this
method resides in a new design principle that learns graph-laplacians as convex
combinations of other elementary laplacians each one dedicated to a particular
topology of the input graphs. We also introduce a novel pooling operator, on
graphs, that proceeds in two steps: context-dependent node expansion is
achieved, followed by a global average pooling; the strength of this two-step
process resides in its ability to preserve the discrimination power of nodes
while achieving permutation invariance. Experiments conducted on SBU and
UCF-101 datasets, show the validity of our method for the challenging task of
action recognition.",1159,74
['cs.LG'],What to Prioritize? Natural Language Processing for the Development of a Modern Bug Tracking Solution in Hardware Development,"Managing large numbers of incoming bug reports and finding the most critical
issues in hardware development is time consuming, but crucial in order to
reduce development costs. In this paper, we present an approach to predict the
time to fix, the risk and the complexity of debugging and resolution of a bug
report using different supervised machine learning algorithms, namely Random
Forest, Naive Bayes, SVM, MLP and XGBoost. Further, we investigate the effect
of the application of active learning and we evaluate the impact of different
text representation techniques, namely TF-IDF, Word2Vec, Universal Sentence
Encoder and XLNet on the model's performance. The evaluation shows that a
combination of text embeddings generated through the Universal Sentence Encoder
and MLP as classifier outperforms all other methods, and is well suited to
predict the risk and complexity of bug tickets.",893,125
"['cs.CV', 'cs.LG', 'cs.RO']",Learning a Domain-Agnostic Visual Representation for Autonomous Driving via Contrastive Loss,"Deep neural networks have been widely studied in autonomous driving
applications such as semantic segmentation or depth estimation. However,
training a neural network in a supervised manner requires a large amount of
annotated labels which are expensive and time-consuming to collect. Recent
studies leverage synthetic data collected from a virtual environment which are
much easier to acquire and more accurate compared to data from the real world,
but they usually suffer from poor generalization due to the inherent domain
shift problem. In this paper, we propose a Domain-Agnostic Contrastive Learning
(DACL) which is a two-stage unsupervised domain adaptation framework with
cyclic adversarial training and contrastive loss. DACL leads the neural network
to learn domain-agnostic representation to overcome performance degradation
when there exists a difference between training and test data distribution. Our
proposed approach achieves better performance in the monocular depth estimation
task compared to previous state-of-the-art methods and also shows effectiveness
in the semantic segmentation task.",1110,92
"['cs.LG', 'cs.CY']",Decoupled classifiers for fair and efficient machine learning,"When it is ethical and legal to use a sensitive attribute (such as gender or
race) in machine learning systems, the question remains how to do so. We show
that the naive application of machine learning algorithms using sensitive
features leads to an inherent tradeoff in accuracy between groups. We provide a
simple and efficient decoupling technique, that can be added on top of any
black-box machine learning algorithm, to learn different classifiers for
different groups. Transfer learning is used to mitigate the problem of having
too little data on any one group.
  The method can apply to a range of fairness criteria. In particular, we
require the application designer to specify as joint loss function that makes
explicit the trade-off between fairness and accuracy. Our reduction is shown to
efficiently find the minimum loss as long as the objective has a certain
natural monotonicity property which may be of independent interest in the study
of fairness in algorithms.",980,61
"['cs.LG', 'cs.AI']",Interactive Search Based on Deep Reinforcement Learning,"With the continuous development of machine learning technology, major
e-commerce platforms have launched recommendation systems based on it to serve
a large number of customers with different needs more efficiently. Compared
with traditional supervised learning, reinforcement learning can better capture
the user's state transition in the decision-making process, and consider a
series of user actions, not just the static characteristics of the user at a
certain moment. In theory, it will have a long-term perspective, producing a
more effective recommendation. The special requirements of reinforcement
learning for data make it need to rely on an offline virtual system for
training. Our project mainly establishes a virtual user environment for offline
training. At the same time, we tried to improve a reinforcement learning
algorithm based on bi-clustering to expand the action space and recommended
path space of the recommendation agent.",947,55
"['cs.LG', 'stat.ML']",Exploration in Action Space,"Parameter space exploration methods with black-box optimization have recently
been shown to outperform state-of-the-art approaches in continuous control
reinforcement learning domains. In this paper, we examine reasons why these
methods work better and the situations in which they are worse than traditional
action space exploration methods. Through a simple theoretical analysis, we
show that when the parametric complexity required to solve the reinforcement
learning problem is greater than the product of action space dimensionality and
horizon length, exploration in action space is preferred. This is also shown
empirically by comparing simple exploration methods on several toy problems.",695,27
"['cs.CV', 'cs.HC']",Vision based body gesture meta features for Affective Computing,"Early detection of psychological distress is key to effective treatment.
Automatic detection of distress, such as depression, is an active area of
research. Current approaches utilise vocal, facial, and bodily modalities. Of
these, the bodily modality is the least investigated, partially due to the
difficulty in extracting bodily representations from videos, and partially due
to the lack of viable datasets. Existing body modality approaches use automatic
categorization of expressions to represent body language as a series of
specific expressions, much like words within natural language. In this
dissertation I present a new type of feature, within the body modality, that
represents meta information of gestures, such as speed, and use it to predict a
non-clinical depression label. This differs to existing work by representing
overall behaviour as a small set of aggregated meta features derived from a
person's movement. In my method I extract pose estimation from videos, detect
gestures within body parts, extract meta information from individual gestures,
and finally aggregate these features to generate a small feature vector for use
in prediction tasks. I introduce a new dataset of 65 video recordings of
interviews with self-evaluated distress, personality, and demographic labels.
This dataset enables the development of features utilising the whole body in
distress detection tasks. I evaluate my newly introduced meta-features for
predicting depression, anxiety, perceived stress, somatic stress, five standard
personality measures, and gender. A linear regression based classifier using
these features achieves a 82.70% F1 score for predicting depression within my
novel dataset.",1701,63
['cs.CV'],A Locally Adapting Technique for Boundary Detection using Image Segmentation,"Rapid growth in the field of quantitative digital image analysis is paving
the way for researchers to make precise measurements about objects in an image.
To compute quantities from the image such as the density of compressed
materials or the velocity of a shockwave, we must determine object boundaries.
Images containing regions that each have a spatial trend in intensity are of
particular interest. We present a supervised image segmentation method that
incorporates spatial information to locate boundaries between regions with
overlapping intensity histograms. The segmentation of a pixel is determined by
comparing its intensity to distributions from local, nearby pixel intensities.
Because of the statistical nature of the algorithm, we use maximum likelihood
estimation theory to quantify uncertainty about each boundary. We demonstrate
the success of this algorithm on a radiograph of a multicomponent cylinder and
on an optical image of a laser-induced shockwave, and we provide final boundary
locations with associated bands of uncertainty.",1053,76
['cs.LG'],A Capsule-unified Framework of Deep Neural Networks for Graphical Programming,"Recently, the growth of deep learning has produced a large number of deep
neural networks. How to describe these networks unifiedly is becoming an
important issue. We first formalize neural networks in a mathematical
definition, give their directed graph representations, and prove a generation
theorem about the induced networks of connected directed acyclic graphs. Then,
using the concept of capsule to extend neural networks, we set up a
capsule-unified framework for deep learning, including a mathematical
definition of capsules, an induced model for capsule networks and a universal
backpropagation algorithm for training them. Finally, we discuss potential
applications of the framework to graphical programming with standard graphical
symbols of capsules, neurons, and connections.",790,77
['cs.CV'],Multi-modal gated recurrent units for image description,"Using a natural language sentence to describe the content of an image is a
challenging but very important task. It is challenging because a description
must not only capture objects contained in the image and the relationships
among them, but also be relevant and grammatically correct. In this paper a
multi-modal embedding model based on gated recurrent units (GRU) which can
generate variable-length description for a given image. In the training step,
we apply the convolutional neural network (CNN) to extract the image feature.
Then the feature is imported into the multi-modal GRU as well as the
corresponding sentence representations. The multi-modal GRU learns the
inter-modal relations between image and sentence. And in the testing step, when
an image is imported to our multi-modal GRU model, a sentence which describes
the image content is generated. The experimental results demonstrate that our
multi-modal GRU model obtains the state-of-the-art performance on Flickr8K,
Flickr30K and MS COCO datasets.",1017,55
"['cs.LG', 'cs.AI', 'math.ST', 'stat.TH']",Non-parametric Models for Non-negative Functions,"Linear models have shown great effectiveness and flexibility in many fields
such as machine learning, signal processing and statistics. They can represent
rich spaces of functions while preserving the convexity of the optimization
problems where they are used, and are simple to evaluate, differentiate and
integrate. However, for modeling non-negative functions, which are crucial for
unsupervised learning, density estimation, or non-parametric Bayesian methods,
linear models are not applicable directly. Moreover, current state-of-the-art
models like generalized linear models either lead to non-convex optimization
problems, or cannot be easily integrated. In this paper we provide the first
model for non-negative functions which benefits from the same good properties
of linear models. In particular, we prove that it admits a representer theorem
and provide an efficient dual formulation for convex problems. We study its
representation power, showing that the resulting space of functions is strictly
richer than that of generalized linear models. Finally we extend the model and
the theoretical results to functions with outputs in convex cones. The paper is
complemented by an experimental evaluation of the model showing its
effectiveness in terms of formulation, algorithmic derivation and practical
results on the problems of density estimation, regression with heteroscedastic
errors, and multiple quantile regression.",1433,48
"['cs.LG', 'cs.DC', 'math.OC']",Local AdaGrad-Type Algorithm for Stochastic Convex-Concave Minimax Problems,"Large scale convex-concave minimax problems arise in numerous applications,
including game theory, robust training, and training of generative adversarial
networks. Despite their wide applicability, solving such problems efficiently
and effectively is challenging in the presence of large amounts of data using
existing stochastic minimax methods. We study a class of stochastic minimax
methods and develop a communication-efficient distributed stochastic
extragradient algorithm, LocalAdaSEG, with an adaptive learning rate suitable
for solving convex-concave minimax problem in the Parameter-Server model.
LocalAdaSEG has three main features: (i) periodic communication strategy
reduces the communication cost between workers and the server; (ii) an adaptive
learning rate that is computed locally and allows for tuning-free
implementation; and (iii) theoretically, a nearly linear speed-up with respect
to the dominant variance term, arising from estimation of the stochastic
gradient, is proven in both the smooth and nonsmooth convex-concave settings.
LocalAdaSEG is used to solve a stochastic bilinear game, and train generative
adversarial network. We compare LocalAdaSEG against several existing optimizers
for minimax problems and demonstrate its efficacy through several experiments
in both the homogeneous and heterogeneous settings.",1344,75
['cs.CV'],Temporal Feature Networks for CNN based Object Detection,"For reliable environment perception, the use of temporal information is
essential in some situations. Especially for object detection, sometimes a
situation can only be understood in the right perspective through temporal
information. Since image-based object detectors are currently based almost
exclusively on CNN architectures, an extension of their feature extraction with
temporal features seems promising.
  Within this work we investigate different architectural components for a
CNN-based temporal information extraction. We present a Temporal Feature
Network which is based on the insights gained from our architectural
investigations. This network is trained from scratch without any ImageNet
information based pre-training as these images are not available with temporal
information. The object detector based on this network is evaluated against the
non-temporal counterpart as baseline and achieves competitive results in an
evaluation on the KITTI object detection dataset.",987,56
['stat.ML'],Computational Optimal Transport,"Optimal transport (OT) theory can be informally described using the words of
the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in
hand has to move a large pile of sand lying on a construction site. The goal of
the worker is to erect with all that sand a target pile with a prescribed shape
(for example, that of a giant sand castle). Naturally, the worker wishes to
minimize her total effort, quantified for instance as the total distance or
time spent carrying shovelfuls of sand. Mathematicians interested in OT cast
that problem as that of comparing two probability distributions, two different
piles of sand of the same volume. They consider all of the many possible ways
to morph, transport or reshape the first pile into the second, and associate a
""global"" cost to every such transport, using the ""local"" consideration of how
much it costs to move a grain of sand from one place to another. Recent years
have witnessed the spread of OT in several fields, thanks to the emergence of
approximate solvers that can scale to sizes and dimensions that are relevant to
data sciences. Thanks to this newfound scalability, OT is being increasingly
used to unlock various problems in imaging sciences (such as color or texture
processing), computer vision and graphics (for shape manipulation) or machine
learning (for regression, classification and density fitting). This short book
reviews OT with a bias toward numerical methods and their applications in data
sciences, and sheds lights on the theoretical properties of OT that make it
particularly useful for some of these applications.",1612,31
['cs.CV'],Richer and Deeper Supervision Network for Salient Object Detection,"Recent Salient Object Detection (SOD) systems are mostly based on
Convolutional Neural Networks (CNNs). Specifically, Deeply Supervised Saliency
(DSS) system has shown it is very useful to add short connections to the
network and supervising on the side output. In this work, we propose a new SOD
system which aims at designing a more efficient and effective way to pass back
global information. Richer and Deeper Supervision (RDS) is applied to better
combine features from each side output without demanding much extra
computational space. Meanwhile, the backbone network used for SOD is normally
pre-trained on the object classification dataset, ImageNet. But the pre-trained
model has been trained on cropped images in order to only focus on
distinguishing features within the region of the object. But the ignored
background information is also significant in the task of SOD. We try to solve
this problem by introducing the training data designed for object detection. A
coarse global information is learned based on an entire image with its bounding
box before training on the SOD dataset. The large-scale of object images can
slightly improve the performance of SOD. Our experiment shows the proposed RDS
network achieves the state-of-the-art results on five public SOD datasets.",1287,66
"['cs.LG', 'cs.SY', 'eess.SY']",Estimate Three-Phase Distribution Line Parameters With Physics-Informed Graphical Learning Method,"Accurate estimates of network parameters are essential for modeling,
monitoring, and control in power distribution systems. In this paper, we
develop a physics-informed graphical learning algorithm to estimate network
parameters of three-phase power distribution systems. Our proposed algorithm
uses only readily available smart meter data to estimate the three-phase series
resistance and reactance of the primary distribution line segments. We first
develop a parametric physics-based model to replace the black-box deep neural
networks in the conventional graphical neural network (GNN). Then we derive the
gradient of the loss function with respect to the network parameters and use
stochastic gradient descent (SGD) to estimate the physical parameters. Prior
knowledge of network parameters is also considered to further improve the
accuracy of estimation. Comprehensive numerical study results show that our
proposed algorithm yields high accuracy and outperforms existing methods.",987,97
"['cs.LG', 'stat.ML']",Expected path length on random manifolds,"Manifold learning seeks a low dimensional representation that faithfully
captures the essence of data. Current methods can successfully learn such
representations, but do not provide a meaningful set of operations that are
associated with the representation. Working towards operational representation
learning, we endow the latent space of a large class of generative models with
a random Riemannian metric, which provides us with elementary operators. As
computational tools are unavailable for random Riemannian manifolds, we study
deterministic approximations and derive tight error bounds on expected
distances.",616,40
['cs.CV'],Distance-based Hyperspherical Classification for Multi-source Open-Set Domain Adaptation,"Vision systems trained in closed-world scenarios will inevitably fail when
presented with new environmental conditions, new data distributions and novel
classes at deployment time. How to move towards open-world learning is a long
standing research question, but the existing solutions mainly focus on specific
aspects of the problem (single domain Open-Set, multi-domain Closed-Set), or
propose complex strategies which combine multiple losses and manually tuned
hyperparameters. In this work we tackle multi-source Open-Set domain adaptation
by introducing HyMOS: a straightforward supervised model that exploits the
power of contrastive learning and the properties of its hyperspherical feature
space to correctly predict known labels on the target, while rejecting samples
belonging to any unknown class. HyMOS includes a tailored data balancing to
enforce cross-source alignment and introduces style transfer among the instance
transformations of contrastive learning for source-target adaptation, avoiding
the risk of negative transfer. Finally a self-training strategy refines the
model without the need for handcrafted thresholds. We validate our method over
three challenging datasets and provide an extensive quantitative and
qualitative experimental analysis. The obtained results show that HyMOS
outperforms several Open-Set and universal domain adaptation approaches,
defining the new state-of-the-art.",1415,88
"['cs.LG', 'stat.ML']",Can Agents Learn by Analogy? An Inferable Model for PAC Reinforcement Learning,"Model-based reinforcement learning algorithms make decisions by building and
utilizing a model of the environment. However, none of the existing algorithms
attempts to infer the dynamics of any state-action pair from known state-action
pairs before meeting it for sufficient times. We propose a new model-based
method called Greedy Inference Model (GIM) that infers the unknown dynamics
from known dynamics based on the internal spectral properties of the
environment. In other words, GIM can ""learn by analogy"". We further introduce a
new exploration strategy which ensures that the agent rapidly and evenly visits
unknown state-action pairs. GIM is much more computationally efficient than
state-of-the-art model-based algorithms, as the number of dynamic programming
operations is independent of the environment size. Lower sample complexity
could also be achieved under mild conditions compared against methods without
inferring. Experimental results demonstrate the effectiveness and efficiency of
GIM in a variety of real-world tasks.",1040,78
"['cs.LG', 'cs.AI']","Seeing Differently, Acting Similarly: Imitation Learning with Heterogeneous Observations","In many real-world imitation learning tasks, the demonstrator and the learner
have to act in different but full observation spaces. This situation generates
significant obstacles for existing imitation learning approaches to work, even
when they are combined with traditional space adaptation techniques. The main
challenge lies in bridging expert's occupancy measures to learner's dynamically
changing occupancy measures under the different observation spaces. In this
work, we model the above learning problem as Heterogeneous Observations
Imitation Learning (HOIL). We propose the Importance Weighting with REjection
(IWRE) algorithm based on the techniques of importance-weighting, learning with
rejection, and active querying to solve the key challenge of occupancy measure
matching. Experimental results show that IWRE can successfully solve HOIL
tasks, including the challenging task of transforming the vision-based
demonstrations to random access memory (RAM)-based policies under the Atari
domain.",1007,88
"['stat.ML', 'cs.LG']",Thompson Sampling for Learning Parameterized Markov Decision Processes,"We consider reinforcement learning in parameterized Markov Decision Processes
(MDPs), where the parameterization may induce correlation across transition
probabilities or rewards. Consequently, observing a particular state transition
might yield useful information about other, unobserved, parts of the MDP. We
present a version of Thompson sampling for parameterized reinforcement learning
problems, and derive a frequentist regret bound for priors over general
parameter spaces. The result shows that the number of instants where suboptimal
actions are chosen scales logarithmically with time, with high probability. It
holds for prior distributions that put significant probability near the true
model, without any additional, specific closed-form structure such as conjugate
or product-form priors. The constant factor in the logarithmic scaling encodes
the information complexity of learning the MDP in terms of the Kullback-Leibler
geometry of the parameter space.",970,70
"['cs.LG', 'cs.AI', 'eess.SP', '68T05 (Primary) 62H12, 68T07 (Secondary)', 'J.2']",Autoencoder-based Representation Learning from Heterogeneous Multivariate Time Series Data of Mechatronic Systems,"Sensor and control data of modern mechatronic systems are often available as
heterogeneous time series with different sampling rates and value ranges.
Suitable classification and regression methods from the field of supervised
machine learning already exist for predictive tasks, for example in the context
of condition monitoring, but their performance scales strongly with the number
of labeled training data. Their provision is often associated with high effort
in the form of person-hours or additional sensors. In this paper, we present a
method for unsupervised feature extraction using autoencoder networks that
specifically addresses the heterogeneous nature of the database and reduces the
amount of labeled training data required compared to existing methods. Three
public datasets of mechatronic systems from different application domains are
used to validate the results.",883,113
['cs.CV'],Self-supervised Video Representation Learning by Uncovering Spatio-temporal Statistics,"This paper proposes a novel pretext task to address the self-supervised video
representation learning problem. Specifically, given an unlabeled video clip,
we compute a series of spatio-temporal statistical summaries, such as the
spatial location and dominant direction of the largest motion, the spatial
location and dominant color of the largest color diversity along the temporal
axis, etc. Then a neural network is built and trained to yield the statistical
summaries given the video frames as inputs. In order to alleviate the learning
difficulty, we employ several spatial partitioning patterns to encode rough
spatial locations instead of exact spatial Cartesian coordinates. Our approach
is inspired by the observation that human visual system is sensitive to rapidly
changing contents in the visual field, and only needs impressions about rough
spatial locations to understand the visual contents. To validate the
effectiveness of the proposed approach, we conduct extensive experiments with
four 3D backbone networks, i.e., C3D, 3D-ResNet, R(2+1)D and S3D-G. The results
show that our approach outperforms the existing approaches across these
backbone networks on four downstream video analysis tasks including action
recognition, video retrieval, dynamic scene recognition, and action similarity
labeling. The source code is publicly available at:
https://github.com/laura-wang/video_repres_sts.",1406,86
"['cs.LG', 'math.OC']",Structured Sparsity Inducing Adaptive Optimizers for Deep Learning,"The parameters of a neural network are naturally organized in groups, some of
which might not contribute to its overall performance. To prune out unimportant
groups of parameters, we can include some non-differentiable penalty to the
objective function, and minimize it using proximal gradient methods. In this
paper, we derive the weighted proximal operator, which is a necessary component
of these proximal methods, of two structured sparsity inducing penalties.
Moreover, they can be approximated efficiently with a numerical solver, and
despite this approximation, we prove that existing convergence guarantees are
preserved when these operators are integrated as part of a generic adaptive
proximal method. Finally, we show that this adaptive method, together with the
weighted proximal operators derived here, is indeed capable of finding
solutions with structure in their sparsity patterns, on representative examples
from computer vision and natural language processing.",978,66
['cs.CV'],OpenMatch: Open-set Consistency Regularization for Semi-supervised Learning with Outliers,"Semi-supervised learning (SSL) is an effective means to leverage unlabeled
data to improve a model's performance. Typical SSL methods like FixMatch assume
that labeled and unlabeled data share the same label space. However, in
practice, unlabeled data can contain categories unseen in the labeled set,
i.e., outliers, which can significantly harm the performance of SSL algorithms.
To address this problem, we propose a novel Open-set Semi-Supervised Learning
(OSSL) approach called OpenMatch. Learning representations of inliers while
rejecting outliers is essential for the success of OSSL. To this end, OpenMatch
unifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers.
The OVA-classifier outputs the confidence score of a sample being an inlier,
providing a threshold to detect outliers. Another key contribution is an
open-set soft-consistency regularization loss, which enhances the smoothness of
the OVA-classifier with respect to input transformations and greatly improves
outlier detection. OpenMatch achieves state-of-the-art performance on three
datasets, and even outperforms a fully supervised model in detecting outliers
unseen in unlabeled data on CIFAR10.",1193,89
['cs.LG'],OMPQ: Orthogonal Mixed Precision Quantization,"To bridge the ever increasing gap between deep neural networks' complexity
and hardware capability, network quantization has attracted more and more
research attention. The latest trend of mixed precision quantization takes
advantage of hardware's multiple bit-width arithmetic operations to unleash the
full potential of network quantization. However, this also results in a
difficult integer programming formulation, and forces most existing approaches
to use an extremely time-consuming search process even with various
relaxations. Instead of solving a problem of the original integer programming,
we propose to optimize a proxy metric, the concept of network orthogonality,
which is highly correlated with the loss of the integer programming but also
easy to optimize with linear programming. This approach reduces the search time
and required data amount by orders of magnitude, with little compromise on
quantization accuracy. Specifically, on post-training quantization, we achieve
71.27% Top-1 accuracy on MobileNetV2, which only takes 9 seconds for searching
and 1.4 GPU hours for finetuning on ImageNet. Our codes are avaliable at
https://github.com/MAC-AutoML/OMPQ.",1177,45
"['cs.CV', 'cs.AI']",Border-SegGCN: Improving Semantic Segmentation by Refining the Border Outline using Graph Convolutional Network,"We present Border-SegGCN, a novel architecture to improve semantic
segmentation by refining the border outline using graph convolutional networks
(GCN). The semantic segmentation network such as Unet or DeepLabV3+ is used as
a base network to have pre-segmented output. This output is converted into a
graphical structure and fed into the GCN to improve the border pixel prediction
of the pre-segmented output. We explored and studied the factors such as border
thickness, number of edges for a node, and the number of features to be fed
into the GCN by performing experiments. We demonstrate the effectiveness of the
Border-SegGCN on the CamVid and Carla dataset, achieving a test set performance
of 81.96% without any post-processing on CamVid dataset. It is higher than the
reported state of the art mIoU achieved on CamVid dataset by 0.404%",844,111
['cs.CV'],Deep CNN ensembles and suggestive annotations for infant brain MRI segmentation,"Precise 3D segmentation of infant brain tissues is an essential step towards
comprehensive volumetric studies and quantitative analysis of early brain
developement. However, computing such segmentations is very challenging,
especially for 6-month infant brain, due to the poor image quality, among other
difficulties inherent to infant brain MRI, e.g., the isointense contrast
between white and gray matter and the severe partial volume effect due to small
brain sizes. This study investigates the problem with an ensemble of semi-dense
fully convolutional neural networks (CNNs), which employs T1-weighted and
T2-weighted MR images as input. We demonstrate that the ensemble agreement is
highly correlated with the segmentation errors. Therefore, our method provides
measures that can guide local user corrections. To the best of our knowledge,
this work is the first ensemble of 3D CNNs for suggesting annotations within
images. Furthermore, inspired by the very recent success of dense networks, we
propose a novel architecture, SemiDenseNet, which connects all convolutional
layers directly to the end of the network. Our architecture allows the
efficient propagation of gradients during training, while limiting the number
of parameters, requiring one order of magnitude less parameters than popular
medical image segmentation networks such as 3D U-Net. Another contribution of
our work is the study of the impact that early or late fusions of multiple
image modalities might have on the performances of deep architectures. We
report evaluations of our method on the public data of the MICCAI iSEG-2017
Challenge on 6-month infant brain MRI segmentation, and show very competitive
results among 21 teams, ranking first or second in most metrics.",1750,79
"['cs.LG', 'stat.ML']",Adaptive Exact Learning of Decision Trees from Membership Queries,"In this paper we study the adaptive learnability of decision trees of depth
at most $d$ from membership queries. This has many applications in automated
scientific discovery such as drugs development and software update problem.
Feldman solves the problem in a randomized polynomial time algorithm that asks
$\tilde O(2^{2d})\log n$ queries and Kushilevitz-Mansour in a deterministic
polynomial time algorithm that asks $ 2^{18d+o(d)}\log n$ queries. We improve
the query complexity of both algorithms. We give a randomized polynomial time
algorithm that asks $\tilde O(2^{2d}) + 2^{d}\log n$ queries and a
deterministic polynomial time algorithm that asks $2^{5.83d}+2^{2d+o(d)}\log n$
queries.",695,65
"['stat.ML', 'cs.CR', 'cs.LG']",Adversarial Reinforcement Learning under Partial Observability in Autonomous Computer Network Defence,"Recent studies have demonstrated that reinforcement learning (RL) agents are
susceptible to adversarial manipulation, similar to vulnerabilities previously
demonstrated in the supervised learning setting. While most existing work
studies the problem in the context of computer vision or console games, this
paper focuses on reinforcement learning in autonomous cyber defence under
partial observability. We demonstrate that under the black-box setting, where
the attacker has no direct access to the target RL model, causative
attacks---attacks that target the training process---can poison RL agents even
if the attacker only has partial observability of the environment. In addition,
we propose an inversion defence method that aims to apply the opposite
perturbation to that which an attacker might use to generate their adversarial
samples. Our experimental results illustrate that the countermeasure can
effectively reduce the impact of the causative attack, while not significantly
affecting the training process in non-attack scenarios.",1043,101
"['cs.LG', 'cs.RO']",Data-efficient Deep Reinforcement Learning for Dexterous Manipulation,"Deep learning and reinforcement learning methods have recently been used to
solve a variety of problems in continuous control domains. An obvious
application of these techniques is dexterous manipulation tasks in robotics
which are difficult to solve using traditional control theory or
hand-engineered approaches. One example of such a task is to grasp an object
and precisely stack it on another. Solving this difficult and practically
relevant problem in the real world is an important long-term goal for the field
of robotics. Here we take a step towards this goal by examining the problem in
simulation and providing models and techniques aimed at solving it. We
introduce two extensions to the Deep Deterministic Policy Gradient algorithm
(DDPG), a model-free Q-learning based method, which make it significantly more
data-efficient and scalable. Our results show that by making extensive use of
off-policy data and replay, it is possible to find control policies that
robustly grasp objects and stack them. Further, our results hint that it may
soon be feasible to train successful stacking policies by collecting
interactions on real robots.",1149,69
"['cs.LG', 'cs.SI']",Higher-Order Attribute-Enhancing Heterogeneous Graph Neural Networks,"Graph neural networks (GNNs) have been widely used in deep learning on
graphs. They can learn effective node representations that achieve superior
performances in graph analysis tasks such as node classification and node
clustering. However, most methods ignore the heterogeneity in real-world
graphs. Methods designed for heterogeneous graphs, on the other hand, fail to
learn complex semantic representations because they only use meta-paths instead
of meta-graphs. Furthermore, they cannot fully capture the content-based
correlations between nodes, as they either do not use the self-attention
mechanism or only use it to consider the immediate neighbors of each node,
ignoring the higher-order neighbors. We propose a novel Higher-order
Attribute-Enhancing (HAE) framework that enhances node embedding in a
layer-by-layer manner. Under the HAE framework, we propose a Higher-order
Attribute-Enhancing Graph Neural Network (HAEGNN) for heterogeneous network
representation learning. HAEGNN simultaneously incorporates meta-paths and
meta-graphs for rich, heterogeneous semantics, and leverages the self-attention
mechanism to explore content-based nodes interactions. The unique higher-order
architecture of HAEGNN allows examining the first-order as well as higher-order
neighborhoods. Moreover, HAEGNN shows good explainability as it learns the
importances of different meta-paths and meta-graphs. HAEGNN is also
memory-efficient, for it avoids per meta-path based matrix calculation.
Experimental results not only show HAEGNN superior performance against the
state-of-the-art methods in node classification, node clustering, and
visualization, but also demonstrate its superiorities in terms of memory
efficiency and explainability.",1739,68
"['cs.CV', 'eess.IV']",Unrolling of Deep Graph Total Variation for Image Denoising,"While deep learning (DL) architectures like convolutional neural networks
(CNNs) have enabled effective solutions in image denoising, in general their
implementations overly rely on training data, lack interpretability, and
require tuning of a large parameter set. In this paper, we combine classical
graph signal filtering with deep feature learning into a competitive hybrid
design -- one that utilizes interpretable analytical low-pass graph filters and
employs 80% fewer network parameters than state-of-the-art DL denoising scheme
DnCNN. Specifically, to construct a suitable similarity graph for graph
spectral filtering, we first adopt a CNN to learn feature representations per
pixel, and then compute feature distances to establish edge weights. Given a
constructed graph, we next formulate a convex optimization problem for
denoising using a graph total variation (GTV) prior. Via a $l_1$ graph
Laplacian reformulation, we interpret its solution in an iterative procedure as
a graph low-pass filter and derive its frequency response. For fast filter
implementation, we realize this response using a Lanczos approximation.
Experimental results show that in the case of statistical mistmatch, our
algorithm outperformed DnCNN by up to 3dB in PSNR.",1255,59
['cs.CV'],MANet: Multimodal Attention Network based Point- View fusion for 3D Shape Recognition,"3D shape recognition has attracted more and more attention as a task of 3D
vision research. The proliferation of 3D data encourages various deep learning
methods based on 3D data. Now there have been many deep learning models based
on point-cloud data or multi-view data alone. However, in the era of big data,
integrating data of two different modals to obtain a unified 3D shape
descriptor is bound to improve the recognition accuracy. Therefore, this paper
proposes a fusion network based on multimodal attention mechanism for 3D shape
recognition. Considering the limitations of multi-view data, we introduce a
soft attention scheme, which can use the global point-cloud features to filter
the multi-view features, and then realize the effective fusion of the two
features. More specifically, we obtain the enhanced multi-view features by
mining the contribution of each multi-view image to the overall shape
recognition, and then fuse the point-cloud features and the enhanced multi-view
features to obtain a more discriminative 3D shape descriptor. We have performed
relevant experiments on the ModelNet40 dataset, and experimental results verify
the effectiveness of our method.",1185,85
"['cs.CV', 'cs.LG', 'stat.ML']",Amortised MAP Inference for Image Super-resolution,"Image super-resolution (SR) is an underdetermined inverse problem, where a
large number of plausible high-resolution images can explain the same
downsampled image. Most current single image SR methods use empirical risk
minimisation, often with a pixel-wise mean squared error (MSE) loss. However,
the outputs from such methods tend to be blurry, over-smoothed and generally
appear implausible. A more desirable approach would employ Maximum a Posteriori
(MAP) inference, preferring solutions that always have a high probability under
the image prior, and thus appear more plausible. Direct MAP estimation for SR
is non-trivial, as it requires us to build a model for the image prior from
samples. Furthermore, MAP inference is often performed via optimisation-based
iterative algorithms which don't compare well with the efficiency of
neural-network-based alternatives. Here we introduce new methods for amortised
MAP inference whereby we calculate the MAP estimate directly using a
convolutional neural network. We first introduce a novel neural network
architecture that performs a projection to the affine subspace of valid SR
solutions ensuring that the high resolution output of the network is always
consistent with the low resolution input. We show that, using this
architecture, the amortised MAP inference problem reduces to minimising the
cross-entropy between two distributions, similar to training generative models.
We propose three methods to solve this optimisation problem: (1) Generative
Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates
gradient-estimates from denoising to train the network, and (3) a baseline
method using a maximum-likelihood-trained image prior. Our experiments show
that the GAN based approach performs best on real image data. Lastly, we
establish a connection between GANs and amortised variational inference as in
e.g. variational autoencoders.",1909,50
"['stat.ML', 'cond-mat.dis-nn', 'cs.LG']",The Geometry of Over-parameterized Regression and Adversarial Perturbations,"Classical regression has a simple geometric description in terms of a
projection of the training labels onto the column space of the design matrix.
However, for over-parameterized models -- where the number of fit parameters is
large enough to perfectly fit the training data -- this picture becomes
uninformative. Here, we present an alternative geometric interpretation of
regression that applies to both under- and over-parameterized models. Unlike
the classical picture which takes place in the space of training labels, our
new picture resides in the space of input features. This new feature-based
perspective provides a natural geometric interpretation of the double-descent
phenomenon in the context of bias and variance, explaining why it can occur
even in the absence of label noise. Furthermore, we show that adversarial
perturbations -- small perturbations to the input features that result in large
changes in label values -- are a generic feature of biased models, arising from
the underlying geometry. We demonstrate these ideas by analyzing three minimal
models for over-parameterized linear least squares regression: without basis
functions (input features equal model features) and with linear or nonlinear
basis functions (two-layer neural networks with linear or nonlinear activation
functions, respectively).",1329,75
['cs.CV'],Multiple Hypothesis Colorization,"In this work we focus on the problem of colorization for image compression.
Since color information occupies a large proportion of the total storage size
of an image, a method that can predict accurate color from its grayscale
version can produce dramatic reduction in image file size. But colorization for
compression poses several challenges. First, while colorization for artistic
purposes simply involves predicting plausible chroma, colorization for
compression requires generating output colors that are as close as possible to
the ground truth. Second, many objects in the real world exhibit multiple
possible colors. Thus, to disambiguate the colorization problem some additional
information must be stored to reproduce the true colors with good accuracy. To
account for the multimodal color distribution of objects we propose a deep
tree-structured network that generates multiple color hypotheses for every
pixel from a grayscale picture (as opposed to a single color produced by most
prior colorization approaches). We show how to leverage the multimodal output
of our model to reproduce with high fidelity the true colors of an image by
storing very little additional information. In the experiments we show that our
proposed method outperforms traditional JPEG color coding by a large margin,
producing colors that are nearly indistinguishable from the ground truth at the
storage cost of just a few hundred bytes for high-resolution pictures!",1456,32
"['cs.CV', 'cs.RO']",FIDNet: LiDAR Point Cloud Semantic Segmentation with Fully Interpolation Decoding,"Projecting the point cloud on the 2D spherical range image transforms the
LiDAR semantic segmentation to a 2D segmentation task on the range image.
However, the LiDAR range image is still naturally different from the regular 2D
RGB image; for example, each position on the range image encodes the unique
geometry information. In this paper, we propose a new projection-based LiDAR
semantic segmentation pipeline that consists of a novel network structure and
an efficient post-processing step. In our network structure, we design a FID
(fully interpolation decoding) module that directly upsamples the
multi-resolution feature maps using bilinear interpolation. Inspired by the 3D
distance interpolation used in PointNet++, we argue this FID module is a 2D
version distance interpolation on $(\theta, \phi)$ space. As a parameter-free
decoding module, the FID largely reduces the model complexity by maintaining
good performance. Besides the network structure, we empirically find that our
model predictions have clear boundaries between different semantic classes.
This makes us rethink whether the widely used K-nearest-neighbor
post-processing is still necessary for our pipeline. Then, we realize the
many-to-one mapping causes the blurring effect that some points are mapped into
the same pixel and share the same label. Therefore, we propose to process those
occluded points by assigning the nearest predicted label to them. This NLA
(nearest label assignment) post-processing step shows a better performance than
KNN with faster inference speed in the ablation study. On the SemanticKITTI
dataset, our pipeline achieves the best performance among all projection-based
methods with $64 \times 2048$ resolution and all point-wise solutions. With a
ResNet-34 as the backbone, both the training and testing of our model can be
finished on a single RTX 2080 Ti with 11G memory. The code is released.",1901,81
"['cs.CV', 'cs.AI']",Stacked Deep Multi-Scale Hierarchical Network for Fast Bokeh Effect Rendering from a Single Image,"The Bokeh Effect is one of the most desirable effects in photography for
rendering artistic and aesthetic photos. Usually, it requires a DSLR camera
with different aperture and shutter settings and certain photography skills to
generate this effect. In smartphones, computational methods and additional
sensors are used to overcome the physical lens and sensor limitations to
achieve such effect. Most of the existing methods utilized additional sensor's
data or pretrained network for fine depth estimation of the scene and sometimes
use portrait segmentation pretrained network module to segment salient objects
in the image. Because of these reasons, networks have many parameters, become
runtime intensive and unable to run in mid-range devices. In this paper, we
used an end-to-end Deep Multi-Scale Hierarchical Network (DMSHN) model for
direct Bokeh effect rendering of images captured from the monocular camera. To
further improve the perceptual quality of such effect, a stacked model
consisting of two DMSHN modules is also proposed. Our model does not rely on
any pretrained network module for Monocular Depth Estimation or Saliency
Detection, thus significantly reducing the size of model and run time. Stacked
DMSHN achieves state-of-the-art results on a large scale EBB! dataset with
around 6x less runtime compared to the current state-of-the-art model in
processing HD quality images.",1399,97
"['stat.ML', 'cs.IT', 'cs.LG', 'eess.SP', 'math.IT']",Fundamental Limits of Ridge-Regularized Empirical Risk Minimization in High Dimensions,"Empirical Risk Minimization (ERM) algorithms are widely used in a variety of
estimation and prediction tasks in signal-processing and machine learning
applications. Despite their popularity, a theory that explains their
statistical properties in modern regimes where both the number of measurements
and the number of unknown parameters is large is only recently emerging. In
this paper, we characterize for the first time the fundamental limits on the
statistical accuracy of convex ERM for inference in high-dimensional
generalized linear models. For a stylized setting with Gaussian features and
problem dimensions that grow large at a proportional rate, we start with sharp
performance characterizations and then derive tight lower bounds on the
estimation and prediction error that hold over a wide class of loss functions
and for any value of the regularization parameter. Our precise analysis has
several attributes. First, it leads to a recipe for optimally tuning the loss
function and the regularization parameter. Second, it allows to precisely
quantify the sub-optimality of popular heuristic choices: for instance, we show
that optimally-tuned least-squares is (perhaps surprisingly) approximately
optimal for standard logistic data, but the sub-optimality gap grows
drastically as the signal strength increases. Third, we use the bounds to
precisely assess the merits of ridge-regularization as a function of the
over-parameterization ratio. Notably, our bounds are expressed in terms of the
Fisher Information of random variables that are simple functions of the data
distribution, thus making ties to corresponding bounds in classical statistics.",1661,86
['cs.CV'],Describe and Attend to Track: Learning Natural Language guided Structural Representation and Visual Attention for Object Tracking,"The tracking-by-detection framework requires a set of positive and negative
training samples to learn robust tracking models for precise localization of
target objects. However, existing tracking models mostly treat different
samples independently while ignores the relationship information among them. In
this paper, we propose a novel structure-aware deep neural network to overcome
such limitations. In particular, we construct a graph to represent the pairwise
relationships among training samples, and additionally take the natural
language as the supervised information to learn both feature representations
and classifiers robustly. To refine the states of the target and re-track the
target when it is back to view from heavy occlusion and out of view, we
elaborately design a novel subnetwork to learn the target-driven visual
attentions from the guidance of both visual and natural language cues.
Extensive experiments on five tracking benchmark datasets validated the
effectiveness of our proposed method.",1016,129
"['cs.LG', 'stat.ML']",Approximate Inference and Stochastic Optimal Control,"We propose a novel reformulation of the stochastic optimal control problem as
an approximate inference problem, demonstrating, that such a interpretation
leads to new practical methods for the original problem. In particular we
characterise a novel class of iterative solutions to the stochastic optimal
control problem based on a natural relaxation of the exact dual formulation.
These theoretical insights are applied to the Reinforcement Learning problem
where they lead to new model free, off policy methods for discrete and
continuous problems.",549,52
['cs.CV'],Perturb-and-MPM: Quantifying Segmentation Uncertainty in Dense Multi-Label CRFs,"This paper proposes a novel approach for uncertainty quantification in dense
Conditional Random Fields (CRFs). The presented approach, called
Perturb-and-MPM, enables efficient, approximate sampling from dense multi-label
CRFs via random perturbations. An analytic error analysis was performed which
identified the main cause of approximation error as well as showed that the
error is bounded. Spatial uncertainty maps can be derived from the
Perturb-and-MPM model, which can be used to visualize uncertainty in image
segmentation results. The method is validated on synthetic and clinical
Magnetic Resonance Imaging data. The effectiveness of the approach is
demonstrated on the challenging problem of segmenting the tumor core in
glioblastoma. We found that areas of high uncertainty correspond well to
wrongly segmented image regions. Furthermore, we demonstrate the potential use
of uncertainty maps to refine imaging biomarkers in the case of extent of
resection and residual tumor volume in brain tumor patients.",1018,79
"['cs.CV', 'cs.LG', 'cs.RO']",Is attention to bounding boxes all you need for pedestrian action prediction?,"The human driver is no longer the only one concerned with the complexity of
the driving scenarios. Autonomous vehicles (AV) are similarly becoming involved
in the process. Nowadays, the development of AV in urban places underpins
essential safety concerns for vulnerable road users (VRUs) such as pedestrians.
Therefore, to make the roads safer, it is critical to classify and predict
their future behavior. In this paper, we present a framework based on multiple
variations of the Transformer models to reason attentively about the dynamic
evolution of the pedestrians' past trajectory and predict its future actions of
crossing or not crossing the street. We proved that using only bounding boxes
as input to our model can outperform the previous state-of-the-art models and
reach a prediction accuracy of 91 % and an F1-score of 0.83 on the PIE dataset
up to two seconds ahead in the future. In addition, we introduced a large-size
simulated dataset (CP2A) using CARLA for action prediction. Our model has
similarly reached high accuracy (91 %) and F1-score (0.91) on this dataset.
Interestingly, we showed that pre-training our Transformer model on the
simulated dataset and then fine-tuning it on the real dataset can be very
effective for the action prediction task.",1272,77
['cs.CV'],Network-Agnostic Knowledge Transfer for Medical Image Segmentation,"Conventional transfer learning leverages weights of pre-trained networks, but
mandates the need for similar neural architectures. Alternatively, knowledge
distillation can transfer knowledge between heterogeneous networks but often
requires access to the original training data or additional generative
networks. Knowledge transfer between networks can be improved by being agnostic
to the choice of network architecture and reducing the dependence on original
training data. We propose a knowledge transfer approach from a teacher to a
student network wherein we train the student on an independent transferal
dataset, whose annotations are generated by the teacher. Experiments were
conducted on five state-of-the-art networks for semantic segmentation and seven
datasets across three imaging modalities. We studied knowledge transfer from a
single teacher, combination of knowledge transfer and fine-tuning, and
knowledge transfer from multiple teachers. The student model with a single
teacher achieved similar performance as the teacher; and the student model with
multiple teachers achieved better performance than the teachers. The salient
features of our algorithm include: 1)no need for original training data or
generative networks, 2) knowledge transfer between different architectures, 3)
ease of implementation for downstream tasks by using the downstream task
dataset as the transferal dataset, 4) knowledge transfer of an ensemble of
models, trained independently, into one student model. Extensive experiments
demonstrate that the proposed algorithm is effective for knowledge transfer and
easily tunable.",1621,66
['cs.CV'],ICE-GAN: Identity-aware and Capsule-Enhanced GAN with Graph-based Reasoning for Micro-Expression Recognition and Synthesis,"Micro-expressions are reflections of people's true feelings and motives,
which attract an increasing number of researchers into the study of automatic
facial micro-expression recognition. The short detection window, the subtle
facial muscle movements, and the limited training samples make micro-expression
recognition challenging. To this end, we propose a novel Identity-aware and
Capsule-Enhanced Generative Adversarial Network with graph-based reasoning
(ICE-GAN), introducing micro-expression synthesis as an auxiliary task to
assist recognition. The generator produces synthetic faces with controllable
micro-expressions and identity-aware features, whose long-ranged dependencies
are captured through the graph reasoning module (GRM), and the discriminator
detects the image authenticity and expression classes. Our ICE-GAN was
evaluated on Micro-Expression Grand Challenge 2019 (MEGC2019) with a
significant improvement (12.9%) over the winner and surpassed other
state-of-the-art methods.",997,122
"['cs.LG', 'stat.ML']",Short-term Demand Forecasting for Online Car-hailing Services using Recurrent Neural Networks,"Short-term traffic flow prediction is one of the crucial issues in
intelligent transportation system, which is an important part of smart cities.
Accurate predictions can enable both the drivers and the passengers to make
better decisions about their travel route, departure time and travel origin
selection, which can be helpful in traffic management. Multiple models and
algorithms based on time series prediction and machine learning were applied to
this issue and achieved acceptable results. Recently, the availability of
sufficient data and computational power, motivates us to improve the prediction
accuracy via deep-learning approaches. Recurrent neural networks have become
one of the most popular methods for time series forecasting, however, due to
the variety of these networks, the question that which type is the most
appropriate one for this task remains unsolved. In this paper, we use three
kinds of recurrent neural networks including simple RNN units, GRU and LSTM
neural network to predict short-term traffic flow. The dataset from TAP30
Corporation is used for building the models and comparing RNNs with several
well-known models, such as DEMA, LASSO and XGBoost. The results show that all
three types of RNNs outperform the others, however, more simple RNNs such as
simple recurrent units and GRU perform work better than LSTM in terms of
accuracy and training time.",1390,93
"['cs.LG', 'stat.ML']",Meta-Reinforcement Learning Robust to Distributional Shift via Model Identification and Experience Relabeling,"Reinforcement learning algorithms can acquire policies for complex tasks
autonomously. However, the number of samples required to learn a diverse set of
skills can be prohibitively large. While meta-reinforcement learning methods
have enabled agents to leverage prior experience to adapt quickly to new tasks,
their performance depends crucially on how close the new task is to the
previously experienced tasks. Current approaches are either not able to
extrapolate well, or can do so at the expense of requiring extremely large
amounts of data for on-policy meta-training. In this work, we present model
identification and experience relabeling (MIER), a meta-reinforcement learning
algorithm that is both efficient and extrapolates well when faced with
out-of-distribution tasks at test time. Our method is based on a simple
insight: we recognize that dynamics models can be adapted efficiently and
consistently with off-policy data, more easily than policies and value
functions. These dynamics models can then be used to continue training policies
and value functions for out-of-distribution tasks without using
meta-reinforcement learning at all, by generating synthetic experience for the
new task.",1204,109
"['cs.LG', 'stat.ML']",Understanding and Resolving Performance Degradation in Graph Convolutional Networks,"A Graph Convolutional Network (GCN) stacks several layers and in each layer
performs a PROPagation operation (PROP) and a TRANsformation operation (TRAN)
for learning node representations over graph-structured data. Though powerful,
GCNs tend to suffer performance drop when the model gets deep. Previous works
focus on PROPs to study and mitigate this issue, but the role of TRANs is
barely investigated. In this work, we study performance degradation of GCNs by
experimentally examining how stacking only TRANs or PROPs works. We find that
TRANs contribute significantly, or even more than PROPs, to declining
performance, and moreover that they tend to amplify node-wise feature variance
in GCNs, causing variance inflammation that we identify as a key factor for
causing performance drop. Motivated by such observations, we propose a
variance-controlling technique termed Node Normalization (NodeNorm), which
scales each node's features using its own standard deviation. Experimental
results validate the effectiveness of NodeNorm on addressing performance
degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow
ones in cases where deep models are needed, and to achieve comparable results
with shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and
can well generalize to other GNN architectures. Code is publicly available at
https://github.com/miafei/NodeNorm.",1407,83
['cs.CV'],Joint Image Filtering with Deep Convolutional Networks,"Joint image filters leverage the guidance image as a prior and transfer the
structural details from the guidance image to the target image for suppressing
noise or enhancing spatial resolution. Existing methods either rely on various
explicit filter constructions or hand-designed objective functions, thereby
making it difficult to understand, improve, and accelerate these filters in a
coherent framework. In this paper, we propose a learning-based approach for
constructing joint filters based on Convolutional Neural Networks. In contrast
to existing methods that consider only the guidance image, the proposed
algorithm can selectively transfer salient structures that are consistent with
both guidance and target images. We show that the model trained on a certain
type of data, e.g., RGB and depth images, generalizes well to other modalities,
e.g., flash/non-Flash and RGB/NIR images. We validate the effectiveness of the
proposed joint filter through extensive experimental evaluations with
state-of-the-art methods.",1025,54
['cs.CV'],Learning Rank Reduced Interpolation with Principal Component Analysis,"In computer vision most iterative optimization algorithms, both sparse and
dense, rely on a coarse and reliable dense initialization to bootstrap their
optimization procedure. For example, dense optical flow algorithms profit
massively in speed and robustness if they are initialized well in the basin of
convergence of the used loss function. The same holds true for methods as
sparse feature tracking when initial flow or depth information for new features
at arbitrary positions is needed. This makes it extremely important to have
techniques at hand that allow to obtain from only very few available
measurements a dense but still approximative sketch of a desired 2D structure
(e.g. depth maps, optical flow, disparity maps, etc.). The 2D map is regarded
as sample from a 2D random process. The method presented here exploits the
complete information given by the principal component analysis (PCA) of that
process, the principal basis and its prior distribution. The method is able to
determine a dense reconstruction from sparse measurement. When facing
situations with only very sparse measurements, typically the number of
principal components is further reduced which results in a loss of
expressiveness of the basis. We overcome this problem and inject prior
knowledge in a maximum a posterior (MAP) approach. We test our approach on the
KITTI and the virtual KITTI datasets and focus on the interpolation of depth
maps for driving scenes. The evaluation of the results show good agreement to
the ground truth and are clearly better than results of interpolation by the
nearest neighbor method which disregards statistical information.",1646,69
"['cs.LG', 'stat.ML']",Provably adaptive reinforcement learning in metric spaces,"We study reinforcement learning in continuous state and action spaces endowed
with a metric. We provide a refined analysis of the algorithm of Sinclair,
Banerjee, and Yu (2019) and show that its regret scales with the \emph{zooming
dimension} of the instance. This parameter, which originates in the bandit
literature, captures the size of the subsets of near optimal actions and is
always smaller than the covering dimension used in previous analyses. As such,
our results are the first provably adaptive guarantees for reinforcement
learning in metric spaces.",561,57
['cs.CV'],M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis,"Conditional image synthesis aims to create an image according to some
multi-modal guidance in the forms of textual descriptions, reference images,
and image blocks to preserve, as well as their combinations. In this paper,
instead of investigating these control signals separately, we propose a new
two-stage architecture, UFC-BERT, to unify any number of multi-modal controls.
In UFC-BERT, both the diverse control signals and the synthesized image are
uniformly represented as a sequence of discrete tokens to be processed by
Transformer. Different from existing two-stage autoregressive approaches such
as DALL-E and VQGAN, UFC-BERT adopts non-autoregressive generation (NAR) at the
second stage to enhance the holistic consistency of the synthesized image, to
support preserving specified image blocks, and to improve the synthesis speed.
Further, we design a progressive algorithm that iteratively improves the
non-autoregressively generated image, with the help of two estimators developed
for evaluating the compliance with the controls and evaluating the fidelity of
the synthesized image, respectively. Extensive experiments on a newly collected
large-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal
CelebA-HQ verify that UFC-BERT can synthesize high-fidelity images that comply
with flexible multi-modal controls.",1343,69
"['cs.CV', 'cs.AI', 'cs.LG']",SPDA: Superpixel-based Data Augmentation for Biomedical Image Segmentation,"Supervised training a deep neural network aims to ""teach"" the network to
mimic human visual perception that is represented by image-and-label pairs in
the training data. Superpixelized (SP) images are visually perceivable to
humans, but a conventionally trained deep learning model often performs poorly
when working on SP images. To better mimic human visual perception, we think it
is desirable for the deep learning model to be able to perceive not only raw
images but also SP images. In this paper, we propose a new superpixel-based
data augmentation (SPDA) method for training deep learning models for
biomedical image segmentation. Our method applies a superpixel generation
scheme to all the original training images to generate superpixelized images.
The SP images thus obtained are then jointly used with the original training
images to train a deep learning model. Our experiments of SPDA on four
biomedical image datasets show that SPDA is effective and can consistently
improve the performance of state-of-the-art fully convolutional networks for
biomedical image segmentation in 2D and 3D images. Additional studies also
demonstrate that SPDA can practically reduce the generalization gap.",1202,74
['cs.CV'],Fast Point Voxel Convolution Neural Network with Selective Feature Fusion for Point Cloud Semantic Segmentation,"We present a novel lightweight convolutional neural network for point cloud
analysis. In contrast to many current CNNs which increase receptive field by
downsampling point cloud, our method directly operates on the entire point sets
without sampling and achieves good performances efficiently. Our network
consists of point voxel convolution (PVC) layer as building block. Each layer
has two parallel branches, namely the voxel branch and the point branch. For
the voxel branch specifically, we aggregate local features on non-empty voxel
centers to reduce geometric information loss caused by voxelization, then apply
volumetric convolutions to enhance local neighborhood geometry encoding. For
the point branch, we use Multi-Layer Perceptron (MLP) to extract fine-detailed
point-wise features. Outputs from these two branches are adaptively fused via a
feature selection module. Moreover, we supervise the output from every PVC
layer to learn different levels of semantic information. The final prediction
is made by averaging all intermediate predictions. We demonstrate empirically
that our method is able to achieve comparable results while being fast and
memory efficient. We evaluate our method on popular point cloud datasets for
object classification and semantic segmentation tasks.",1292,111
['cs.CV'],ScarfNet: Multi-scale Features with Deeply Fused and Redistributed Semantics for Enhanced Object Detection,"Convolutional neural network (CNN) has led to significant progress in object
detection. In order to detect the objects in various sizes, the object
detectors often exploit the hierarchy of the multi-scale feature maps called
feature pyramid, which is readily obtained by the CNN architecture. However,
the performance of these object detectors is limited since the bottom-level
feature maps, which experience fewer convolutional layers, lack the semantic
information needed to capture the characteristics of the small objects. In
order to address such problem, various methods have been proposed to increase
the depth for the bottom-level features used for object detection. While most
approaches are based on the generation of additional features through the
top-down pathway with lateral connections, our approach directly fuses
multi-scale feature maps using bidirectional long short term memory (biLSTM) in
effort to generate deeply fused semantics. Then, the resulting semantic
information is redistributed to the individual pyramidal feature at each scale
through the channel-wise attention model. We integrate our semantic combining
and attentive redistribution feature network (ScarfNet) with baseline object
detectors, i.e., Faster R-CNN, single-shot multibox detector (SSD) and
RetinaNet. Our experiments show that our method outperforms the existing
feature pyramid methods as well as the baseline detectors and achieve the state
of the art performances in the PASCAL VOC and COCO detection benchmarks.",1513,106
"['cs.CV', 'cs.LG']","""Double-DIP"": Unsupervised Image Decomposition via Coupled Deep-Image-Priors","Many seemingly unrelated computer vision tasks can be viewed as a special
case of image decomposition into separate layers. For example, image
segmentation (separation into foreground and background layers); transparent
layer separation (into reflection and transmission layers); Image dehazing
(separation into a clear image and a haze map), and more. In this paper we
propose a unified framework for unsupervised layer decomposition of a single
image, based on coupled ""Deep-image-Prior"" (DIP) networks. It was shown
[Ulyanov et al] that the structure of a single DIP generator network is
sufficient to capture the low-level statistics of a single image. We show that
coupling multiple such DIPs provides a powerful tool for decomposing images
into their basic components, for a wide variety of applications. This
capability stems from the fact that the internal statistics of a mixture of
layers is more complex than the statistics of each of its individual
components. We show the power of this approach for Image-Dehazing, Fg/Bg
Segmentation, Watermark-Removal, Transparency Separation in images and video,
and more. These capabilities are achieved in a totally unsupervised way, with
no training examples other than the input image/video itself.",1251,76
['cs.CV'],"Large-scale, Fast and Accurate Shot Boundary Detection through Spatio-temporal Convolutional Neural Networks","Shot boundary detection (SBD) is an important pre-processing step for video
manipulation. Here, each segment of frames is classified as either sharp,
gradual or no transition. Current SBD techniques analyze hand-crafted features
and attempt to optimize both detection accuracy and processing speed. However,
the heavy computations of optical flow prevents this. To achieve this aim, we
present an SBD technique based on spatio-temporal Convolutional Neural Networks
(CNN). Since current datasets are not large enough to train an accurate SBD
CNN, we present a new dataset containing more than 3.5 million frames of sharp
and gradual transitions. The transitions are generated synthetically using
image compositing models. Our dataset contain additional 70,000 frames of
important hard-negative no transitions. We perform the largest evaluation to
date for one SBD algorithm, on real and synthetic data, containing more than
4.85 million frames. In comparison to the state of the art, we outperform
dissolve gradual detection, generate competitive performance for sharp
detections and produce significant improvement in wipes. In addition, we are up
to 11 times faster than the state of the art.",1194,108
['cs.CV'],Revisiting Gray Pixel for Statistical Illumination Estimation,"We present a statistical color constancy method that relies on novel gray
pixel detection and mean shift clustering. The method, called Mean Shifted Grey
Pixel -- MSGP, is based on the observation: true-gray pixels are aligned
towards one single direction. Our solution is compact, easy to compute and
requires no training. Experiments on two real-world benchmarks show that the
proposed approach outperforms state-of-the-art methods in the camera-agnostic
scenario. In the setting where the camera is known, MSGP outperforms all
statistical methods.",550,61
"['cs.LG', 'cs.AI', 'cs.LO']",LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning,"While designing inductive bias in neural architectures has been widely
studied, we hypothesize that transformer networks are flexible enough to learn
inductive bias from suitable generic tasks. Here, we replace architecture
engineering by encoding inductive bias in the form of datasets. Inspired by
Peirce's view that deduction, induction, and abduction form an irreducible set
of reasoning primitives, we design three synthetic tasks that are intended to
require the model to have these three abilities. We specifically design these
synthetic tasks in a way that they are devoid of mathematical knowledge to
ensure that only the fundamental reasoning biases can be learned from these
tasks. This defines a new pre-training methodology called ""LIME"" (Learning
Inductive bias for Mathematical rEasoning). Models trained with LIME
significantly outperform vanilla transformers on three very different large
mathematical reasoning benchmarks. Unlike dominating the computation cost as
traditional pre-training approaches, LIME requires only a small fraction of the
computation cost of the typical downstream task.",1111,70
"['cs.CV', 'cs.LG']",Monocular Depth Estimation Using Multi Scale Neural Network And Feature Fusion,"Depth estimation from monocular images is a challenging problem in computer
vision. In this paper, we tackle this problem using a novel network
architecture using multi scale feature fusion. Our network uses two different
blocks, first which uses different filter sizes for convolution and merges all
the individual feature maps. The second block uses dilated convolutions in
place of fully connected layers thus reducing computations and increasing the
receptive field. We present a new loss function for training the network which
uses a depth regression term, SSIM loss term and a multinomial logistic loss
term combined. We train and test our network on Make 3D dataset, NYU Depth V2
dataset and Kitti dataset using standard evaluation metrics for depth
estimation comprised of RMSE loss and SILog loss. Our network outperforms
previous state of the art methods with lesser parameters.",889,78
"['cs.LG', 'cs.CV']",EC-GAN: Low-Sample Classification using Semi-Supervised Algorithms and GANs,"Semi-supervised learning has been gaining attention as it allows for
performing image analysis tasks such as classification with limited labeled
data. Some popular algorithms using Generative Adversarial Networks (GANs) for
semi-supervised classification share a single architecture for classification
and discrimination. However, this may require a model to converge to a separate
data distribution for each task, which may reduce overall performance. While
progress in semi-supervised learning has been made, less addressed are
small-scale, fully-supervised tasks where even unlabeled data is unavailable
and unattainable. We therefore, propose a novel GAN model namely External
Classifier GAN (EC-GAN), that utilizes GANs and semi-supervised algorithms to
improve classification in fully-supervised regimes. Our method leverages a GAN
to generate artificial data used to supplement supervised classification. More
specifically, we attach an external classifier, hence the name EC-GAN, to the
GAN's generator, as opposed to sharing an architecture with the discriminator.
Our experiments demonstrate that EC-GAN's performance is comparable to the
shared architecture method, far superior to the standard data augmentation and
regularization-based approach, and effective on a small, realistic dataset.",1303,75
"['cs.LG', 'cs.MA', 'stat.ML']",Efficient Ridesharing Dispatch Using Multi-Agent Reinforcement Learning,"With the advent of ride-sharing services, there is a huge increase in the
number of people who rely on them for various needs. Most of the earlier
approaches tackling this issue required handcrafted functions for estimating
travel times and passenger waiting times. Traditional Reinforcement Learning
(RL) based methods attempting to solve the ridesharing problem are unable to
accurately model the complex environment in which taxis operate. Prior
Multi-Agent Deep RL based methods based on Independent DQN (IDQN) learn
decentralized value functions prone to instability due to the concurrent
learning and exploring of multiple agents. Our proposed method based on QMIX is
able to achieve centralized training with decentralized execution. We show that
our model performs better than the IDQN baseline on a fixed grid size and is
able to generalize well to smaller or larger grid sizes. Also, our algorithm is
able to outperform IDQN baseline in the scenario where we have a variable
number of passengers and cars in each episode. Code for our paper is publicly
available at: https://github.com/UMich-ML-Group/RL-Ridesharing.",1126,71
['cs.CV'],Relationship-Aware Spatial Perception Fusion for Realistic Scene Layout Generation,"The significant progress on Generative Adversarial Networks (GANs) have made
it possible to generate surprisingly realistic images for single object based
on natural language descriptions. However, controlled generation of images for
multiple entities with explicit interactions is still difficult to achieve due
to the scene layout generation heavily suffer from the diversity object scaling
and spatial locations. In this paper, we proposed a novel framework for
generating realistic image layout from textual scene graphs. In our framework,
a spatial constraint module is designed to fit reasonable scaling and spatial
layout of object pairs with considering relationship between them. Moreover, a
contextual fusion module is introduced for fusing pair-wise spatial information
in terms of object dependency in scene graph. By using these two modules, our
proposed framework tends to generate more commonsense layout which is helpful
for realistic image generation. Experimental results including quantitative
results, qualitative results and user studies on two different scene graph
datasets demonstrate our proposed framework's ability to generate complex and
logical layout with multiple objects from scene graph.",1220,82
"['cs.CV', 'cs.RO']",NudgeSeg: Zero-Shot Object Segmentation by Repeated Physical Interaction,"Recent advances in object segmentation have demonstrated that deep neural
networks excel at object segmentation for specific classes in color and depth
images. However, their performance is dictated by the number of classes and
objects used for training, thereby hindering generalization to never seen
objects or zero-shot samples. To exacerbate the problem further, object
segmentation using image frames rely on recognition and pattern matching cues.
Instead, we utilize the 'active' nature of a robot and their ability to
'interact' with the environment to induce additional geometric constraints for
segmenting zero-shot samples.
  In this paper, we present the first framework to segment unknown objects in a
cluttered scene by repeatedly 'nudging' at the objects and moving them to
obtain additional motion cues at every step using only a monochrome monocular
camera. We call our framework NudgeSeg. These motion cues are used to refine
the segmentation masks. We successfully test our approach to segment novel
objects in various cluttered scenes and provide an extensive study with image
and motion segmentation methods. We show an impressive average detection rate
of over 86% on zero-shot objects.",1207,72
['cs.LG'],Causal Influence Detection for Improving Efficiency in Reinforcement Learning,"Many reinforcement learning (RL) environments consist of independent entities
that interact sparsely. In such environments, RL agents have only limited
influence over other entities in any particular situation. Our idea in this
work is that learning can be efficiently guided by knowing when and what the
agent can influence with its actions. To achieve this, we introduce a measure
of situation-dependent causal influence based on conditional mutual information
and show that it can reliably detect states of influence. We then propose
several ways to integrate this measure into RL algorithms to improve
exploration and off-policy learning. All modified algorithms show strong
increases in data efficiency on robotic manipulation tasks.",738,77
['cs.CV'],Fully Convolutional Scene Graph Generation,"This paper presents a fully convolutional scene graph generation (FCSGG)
model that detects objects and relations simultaneously. Most of the scene
graph generation frameworks use a pre-trained two-stage object detector, like
Faster R-CNN, and build scene graphs using bounding box features. Such pipeline
usually has a large number of parameters and low inference speed. Unlike these
approaches, FCSGG is a conceptually elegant and efficient bottom-up approach
that encodes objects as bounding box center points, and relationships as 2D
vector fields which are named as Relation Affinity Fields (RAFs). RAFs encode
both semantic and spatial features, and explicitly represent the relationship
between a pair of objects by the integral on a sub-region that points from
subject to object. FCSGG only utilizes visual features and still generates
strong results for scene graph generation. Comprehensive experiments on the
Visual Genome dataset demonstrate the efficacy, efficiency, and
generalizability of the proposed method. FCSGG achieves highly competitive
results on recall and zero-shot recall with significantly reduced inference
time.",1140,42
['cs.LG'],Learning Graph-Structured Sum-Product Networks for Probabilistic Semantic Maps,"We introduce Graph-Structured Sum-Product Networks (GraphSPNs), a
probabilistic approach to structured prediction for problems where dependencies
between latent variables are expressed in terms of arbitrary, dynamic graphs.
While many approaches to structured prediction place strict constraints on the
interactions between inferred variables, many real-world problems can be only
characterized using complex graph structures of varying size, often
contaminated with noise when obtained from real data. Here, we focus on one
such problem in the domain of robotics. We demonstrate how GraphSPNs can be
used to bolster inference about semantic, conceptual place descriptions using
noisy topological relations discovered by a robot exploring large-scale office
spaces. Through experiments, we show that GraphSPNs consistently outperform the
traditional approach based on undirected graphical models, successfully
disambiguating information in global semantic maps built from uncertain, noisy
local evidence. We further exploit the probabilistic nature of the model to
infer marginal distributions over semantic descriptions of as yet unexplored
places and detect spatial environment configurations that are novel and
incongruent with the known evidence.",1250,78
"['stat.ML', '62-09, 62H15, 62C05']",Optimal statistical decision for Gaussian graphical model selection,"Gaussian graphical model is a graphical representation of the dependence
structure for a Gaussian random vector. It is recognized as a powerful tool in
different applied fields such as bioinformatics, error-control codes, speech
language, information retrieval and others. Gaussian graphical model selection
is a statistical problem to identify the Gaussian graphical model from a sample
of a given size. Different approaches for Gaussian graphical model selection
are suggested in the literature. One of them is based on considering the family
of individual conditional independence tests. The application of this approach
leads to the construction of a variety of multiple testing statistical
procedures for Gaussian graphical model selection. An important characteristic
of these procedures is its error rate for a given sample size. In existing
literature great attention is paid to the control of error rates for incorrect
edge inclusion (Type I error). However, in graphical model selection it is also
important to take into account error rates for incorrect edge exclusion (Type
II error). To deal with this issue we consider the graphical model selection
problem in the framework of the multiple decision theory. The quality of
statistical procedures is measured by a risk function with additive losses.
Additive losses allow both types of errors to be taken into account. We
construct the tests of a Neyman structure for individual hypotheses and combine
them to obtain a multiple decision statistical procedure. We show that the
obtained procedure is optimal in the sense that it minimizes the linear
combination of expected numbers of Type I and Type II errors in the class of
unbiased multiple decision procedures.",1726,67
"['cs.CV', 'cs.AI', 'cs.LG', '68T30, 68U10', 'I.2']",What's in the box? Explaining the black-box model through an evaluation of its interpretable features,"Algorithms are powerful and necessary tools behind a large part of the
information we use every day. However, they may introduce new sources of bias,
discrimination and other unfair practices that affect people who are unaware of
it. Greater algorithm transparency is indispensable to provide more credible
and reliable services. Moreover, requiring developers to design transparent
algorithm-driven applications allows them to keep the model accessible and
human understandable, increasing the trust of end users. In this paper we
present EBAnO, a new engine able to produce prediction-local explanations for a
black-box model exploiting interpretable feature perturbations. EBAnO exploits
the hypercolumns representation together with the cluster analysis to identify
a set of interpretable features of images. Furthermore two indices have been
proposed to measure the influence of input features on the final prediction
made by a CNN model. EBAnO has been preliminarily tested on a set of
heterogeneous images. The results highlight the effectiveness of EBAnO in
explaining the CNN classification through the evaluation of interpretable
features influence.",1159,101
"['cs.CV', 'cs.LG', 'eess.IV', 'eess.SP']",SPI-GAN: Towards Single-Pixel Imaging through Generative Adversarial Network,"Single-pixel imaging is a novel imaging scheme that has gained popularity due
to its huge computational gain and potential for a low-cost alternative to
imaging beyond the visible spectrum. The traditional reconstruction methods
struggle to produce a clear recovery when one limits the number of illumination
patterns from a spatial light modulator. As a remedy, several
deep-learning-based solutions have been proposed which lack good generalization
ability due to the architectural setup and loss functions. In this paper, we
propose a generative adversarial network-based reconstruction framework for
single-pixel imaging, referred to as SPI-GAN. Our method can reconstruct images
with 17.92 dB PSNR and 0.487 SSIM, even if the sampling ratio drops to 5%. This
facilitates much faster reconstruction making our method suitable for
single-pixel video. Furthermore, our ResNet-like architecture for the generator
leads to useful representation learning that allows us to reconstruct
completely unseen objects. The experimental results demonstrate that SPI-GAN
achieves significant performance gain, e.g. near 3dB PSNR gain, over the
current state-of-the-art method.",1166,76
"['cs.LG', 'cs.CV', 'stat.ML']",Noise Homogenization via Multi-Channel Wavelet Filtering for High-Fidelity Sample Generation in GANs,"In the generator of typical Generative Adversarial Networks (GANs), a noise
is inputted to generate fake samples via a series of convolutional operations.
However, current noise generation models merely relies on the information from
the pixel space, which increases the difficulty to approach the target
distribution. Fortunately, the long proven wavelet transformation is able to
decompose multiple spectral information from the images. In this work, we
propose a novel multi-channel wavelet-based filtering method for GANs, to cope
with this problem. When embedding a wavelet deconvolution layer in the
generator, the resultant GAN, called WaveletGAN, takes advantage of the wavelet
deconvolution to learn a filtering with multiple channels, which can
efficiently homogenize the generated noise via an averaging operation, so as to
generate high-fidelity samples. We conducted benchmark experiments on the
Fashion-MNIST, KMNIST and SVHN datasets through an open GAN benchmark tool. The
results show that WaveletGAN has excellent performance in generating
high-fidelity samples, thanks to the smallest FIDs obtained on these datasets.",1136,100
"['cs.LG', 'stat.ML']",A Hybrid PAC Reinforcement Learning Algorithm,"This paper offers a new hybrid probably approximately correct (PAC)
reinforcement learning (RL) algorithm for Markov decision processes (MDPs) that
intelligently maintains favorable features of its parents. The designed
algorithm, referred to as the Dyna-Delayed Q-learning (DDQ) algorithm, combines
model-free and model-based learning approaches while outperforming both in most
cases. The paper includes a PAC analysis of the DDQ algorithm and a derivation
of its sample complexity. Numerical results are provided to support the claim
regarding the new algorithm's sample efficiency compared to its parents as well
as the best known model-free and model-based algorithms in application.",688,45
"['cs.CV', 'cs.LG', 'eess.IV']",Slot Based Image Augmentation System for Object Detection,"Object Detection has been a significant topic in computer vision. As the
continuous development of Deep Learning, many advanced academic and industrial
outcomes are established on localising and classifying the target objects, such
as instance segmentation, video tracking and robotic vision. As the core
concept of Deep Learning, Deep Neural Networks (DNNs) and associated training
are highly integrated with task-driven modelling, having great effects on
accurate detection. The main focus of improving detection performance is
proposing DNNs with extra layers and novel topological connections to extract
the desired features from input data. However, training these models can be
computationally expensive and laborious progress as the complicated model
architecture and enormous parameters. Besides, the dataset is another reason
causing this issue and low detection accuracy, because of insufficient data
samples or difficult instances. To address these training difficulties, this
thesis presents two different approaches to improve the detection performance
in the relatively light-weight way. As the intrinsic feature of data-driven in
deep learning, the first approach is ""slot-based image augmentation"" to enrich
the dataset with extra foreground and background combinations. Instead of the
commonly used image flipping method, the proposed system achieved similar mAP
improvement with less extra images which decrease training time. This proposed
augmentation system has extra flexibility adapting to various scenarios and the
performance-driven analysis provides an alternative aspect of conducting image
augmentation",1630,57
['cs.CV'],Automated Segmentation of Retinal Layers from Optical Coherent Tomography Images Using Geodesic Distance,"Optical coherence tomography (OCT) is a non-invasive imaging technique that
can produce images of the eye at the microscopic level. OCT image segmentation
to localise retinal layer boundaries is a fundamental procedure for diagnosing
and monitoring the progression of retinal and optical nerve disorders. In this
paper, we introduce a novel and accurate geodesic distance method (GDM) for OCT
segmentation of both healthy and pathological images in either two- or
three-dimensional spaces. The method uses a weighted geodesic distance by an
exponential function, taking into account both horizontal and vertical
intensity variations. The weighted geodesic distance is efficiently calculated
from an Eikonal equation via the fast sweeping method. The segmentation is then
realised by solving an ordinary differential equation with the geodesic
distance. The results of the GDM are compared with manually segmented retinal
layer boundaries/surfaces. Extensive experiments demonstrate that the proposed
GDM is robust to complex retinal structures with large curvatures and
irregularities and it outperforms the parametric active contour algorithm as
well as the graph theoretic based approaches for delineating the retinal layers
in both healthy and pathological images.",1267,104
['cs.CV'],Camera-based vehicle velocity estimation from monocular video,"This paper documents the winning entry at the CVPR2017 vehicle velocity
estimation challenge. Velocity estimation is an emerging task in autonomous
driving which has not yet been thoroughly explored. The goal is to estimate the
relative velocity of a specific vehicle from a sequence of images. In this
paper, we present a light-weight approach for directly regressing vehicle
velocities from their trajectories using a multilayer perceptron. Another
contribution is an explorative study of features for monocular vehicle velocity
estimation. We find that light-weight trajectory based features outperform
depth and motion cues extracted from deep ConvNets, especially for far-distance
predictions where current disparity and optical flow estimators are challenged
significantly. Our light-weight approach is real-time capable on a single CPU
and outperforms all competing entries in the velocity estimation challenge. On
the test set, we report an average error of 1.12 m/s which is comparable to a
(ground-truth) system that combines LiDAR and radar techniques to achieve an
error of around 0.71 m/s.",1102,61
['cs.CV'],Neighbourhood Watch: Referring Expression Comprehension via Language-guided Graph Attention Networks,"The task in referring expression comprehension is to localise the object
instance in an image described by a referring expression phrased in natural
language. As a language-to-vision matching task, the key to this problem is to
learn a discriminative object feature that can adapt to the expression used. To
avoid ambiguity, the expression normally tends to describe not only the
properties of the referent itself, but also its relationships to its
neighbourhood. To capture and exploit this important information we propose a
graph-based, language-guided attention mechanism. Being composed of node
attention component and edge attention component, the proposed graph attention
mechanism explicitly represents inter-object relationships, and properties with
a flexibility and power impossible with competing approaches. Furthermore, the
proposed graph attention mechanism enables the comprehension decision to be
visualisable and explainable. Experiments on three referring expression
comprehension datasets show the advantage of the proposed approach.",1053,100
"['cs.LG', 'stat.ML']",Simultaneous imputation and disease classification in incomplete medical datasets using Multigraph Geometric Matrix Completion (MGMC),"Large-scale population-based studies in medicine are a key resource towards
better diagnosis, monitoring, and treatment of diseases. They also serve as
enablers of clinical decision support systems, in particular Computer Aided
Diagnosis (CADx) using machine learning (ML). Numerous ML approaches for CADx
have been proposed in literature. However, these approaches assume full data
availability, which is not always feasible in clinical data. To account for
missing data, incomplete data samples are either removed or imputed, which
could lead to data bias and may negatively affect classification performance.
As a solution, we propose an end-to-end learning of imputation and disease
prediction of incomplete medical datasets via Multigraph Geometric Matrix
Completion (MGMC). MGMC uses multiple recurrent graph convolutional networks,
where each graph represents an independent population model based on a key
clinical meta-feature like age, sex, or cognitive function. Graph signal
aggregation from local patient neighborhoods, combined with multigraph signal
fusion via self-attention, has a regularizing effect on both matrix
reconstruction and classification performance. Our proposed approach is able to
impute class relevant features as well as perform accurate classification on
two publicly available medical datasets. We empirically show the superiority of
our proposed approach in terms of classification and imputation performance
when compared with state-of-the-art approaches. MGMC enables disease prediction
in multimodal and incomplete medical datasets. These findings could serve as
baseline for future CADx approaches which utilize incomplete datasets.",1673,133
"['cs.CV', 'eess.IV', 'q-bio.QM']",A Deep Attentive Convolutional Neural Network for Automatic Cortical Plate Segmentation in Fetal MRI,"Fetal cortical plate segmentation is essential in quantitative analysis of
fetal brain maturation and cortical folding. Manual segmentation of the
cortical plate, or manual refinement of automatic segmentations is tedious and
time-consuming. Automatic segmentation of the cortical plate, on the other
hand, is challenged by the relatively low resolution of the reconstructed fetal
brain MRI scans compared to the thin structure of the cortical plate, partial
voluming, and the wide range of variations in the morphology of the cortical
plate as the brain matures during gestation. To reduce the burden of manual
refinement of segmentations, we have developed a new and powerful deep learning
segmentation method. Our method exploits new deep attentive modules with mixed
kernel convolutions within a fully convolutional neural network architecture
that utilizes deep supervision and residual connections. We evaluated our
method quantitatively based on several performance measures and expert
evaluations. Results show that our method outperforms several state-of-the-art
deep models for segmentation, as well as a state-of-the-art multi-atlas
segmentation technique. We achieved average Dice similarity coefficient of
0.87, average Hausdorff distance of 0.96 mm, and average symmetric surface
difference of 0.28 mm on reconstructed fetal brain MRI scans of fetuses scanned
in the gestational age range of 16 to 39 weeks. With a computation time of less
than 1 minute per fetal brain, our method can facilitate and accelerate
large-scale studies on normal and altered fetal brain cortical maturation and
folding.",1612,100
"['cs.CV', 'cs.LG']",PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,"Recent research has shown that incorporating equivariance into neural network
architectures is very helpful, and there have been some works investigating the
equivariance of networks under group actions. However, as digital images and
feature maps are on the discrete meshgrid, corresponding
equivariance-preserving transformation groups are very limited. In this work,
we deal with this issue from the connection between convolutions and partial
differential operators (PDOs). In theory, assuming inputs to be smooth, we
transform PDOs and propose a system which is equivariant to a much more general
continuous group, the $n$-dimension Euclidean group. In implementation, we
discretize the system using the numerical schemes of PDOs, deriving
approximately equivariant convolutions (PDO-eConvs). Theoretically, the
approximation error of PDO-eConvs is of the quadratic order. It is the first
time that the error analysis is provided when the equivariance is approximate.
Extensive experiments on rotated MNIST and natural image classification show
that PDO-eConvs perform competitively yet use parameters much more efficiently.
Particularly, compared with Wide ResNets, our methods result in better results
using only 12.6% parameters.",1237,72
"['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']",Chest ImaGenome Dataset for Clinical Reasoning,"Despite the progress in automatic detection of radiologic findings from chest
X-ray (CXR) images in recent years, a quantitative evaluation of the
explainability of these models is hampered by the lack of locally labeled
datasets for different findings. With the exception of a few expert-labeled
small-scale datasets for specific findings, such as pneumonia and pneumothorax,
most of the CXR deep learning models to date are trained on global ""weak""
labels extracted from text reports, or trained via a joint image and
unstructured text learning strategy. Inspired by the Visual Genome effort in
the computer vision community, we constructed the first Chest ImaGenome dataset
with a scene graph data structure to describe $242,072$ images. Local
annotations are automatically produced using a joint rule-based natural
language processing (NLP) and atlas-based bounding box detection pipeline.
Through a radiologist constructed CXR ontology, the annotations for each CXR
are connected as an anatomy-centered scene graph, useful for image-level
reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$
combinations of relation annotations between $29$ CXR anatomical locations
(objects with bounding box coordinates) and their attributes, structured as a
scene graph per image, ii) over $670,000$ localized comparison relations (for
improved, worsened, or no change) between the anatomical locations across
sequential exams, as well as ii) a manually annotated gold standard scene graph
dataset from $500$ unique patients.",1542,46
"['cs.LG', 'cs.AI', 'stat.ML']",Neural Physicist: Learning Physical Dynamics from Image Sequences,"We present a novel architecture named Neural Physicist (NeurPhy) to learn
physical dynamics directly from image sequences using deep neural networks. For
any physical system, given the global system parameters, the time evolution of
states is governed by the underlying physical laws. How to learn meaningful
system representations in an end-to-end way and estimate accurate state
transition dynamics facilitating long-term prediction have been long-standing
challenges. In this paper, by leveraging recent progresses in representation
learning and state space models (SSMs), we propose NeurPhy, which uses
variational auto-encoder (VAE) to extract underlying Markovian dynamic state at
each time step, neural process (NP) to extract the global system parameters,
and a non-linear non-recurrent stochastic state space model to learn the
physical dynamic transition. We apply NeurPhy to two physical experimental
environments, i.e., damped pendulum and planetary orbits motion, and achieve
promising results. Our model can not only extract the physically meaningful
state representations, but also learn the state transition dynamics enabling
long-term predictions for unseen image sequences. Furthermore, from the
manifold dimension of the latent state space, we can easily identify the degree
of freedom (DoF) of the underlying physical systems.",1346,65
"['cs.LG', 'cs.AI']",Efficient Explanations With Relevant Sets,"Recent work proposed $\delta$-relevant inputs (or sets) as a probabilistic
explanation for the predictions made by a classifier on a given input.
$\delta$-relevant sets are significant because they serve to relate
(model-agnostic) Anchors with (model-accurate) PI- explanations, among other
explanation approaches. Unfortunately, the computation of smallest size
$\delta$-relevant sets is complete for ${NP}^{PP}$, rendering their computation
largely infeasible in practice. This paper investigates solutions for tackling
the practical limitations of $\delta$-relevant sets. First, the paper
alternatively considers the computation of subset-minimal sets. Second, the
paper studies concrete families of classifiers, including decision trees among
others. For these cases, the paper shows that the computation of subset-minimal
$\delta$-relevant sets is in NP, and can be solved with a polynomial number of
calls to an NP oracle. The experimental evaluation compares the proposed
approach with heuristic explainers for the concrete case of the classifiers
studied in the paper, and confirms the advantage of the proposed solution over
the state of the art.",1155,41
['cs.CV'],A Joint Sequence Fusion Model for Video Question Answering and Retrieval,"We present an approach named JSFusion (Joint Sequence Fusion) that can
measure semantic similarity between any pairs of multimodal sequence data (e.g.
a video clip and a language sentence). Our multimodal matching network consists
of two key components. First, the Joint Semantic Tensor composes a dense
pairwise representation of two sequence data into a 3D tensor. Then, the
Convolutional Hierarchical Decoder computes their similarity score by
discovering hidden hierarchical matches between the two sequence modalities.
Both modules leverage hierarchical attention mechanisms that learn to promote
well-matched representation patterns while prune out misaligned ones in a
bottom-up manner. Although the JSFusion is a universal model to be applicable
to any multimodal sequence data, this work focuses on video-language tasks
including multimodal retrieval and video QA. We evaluate the JSFusion model in
three retrieval and VQA tasks in LSMDC, for which our model achieves the best
performance reported so far. We also perform multiple-choice and movie
retrieval tasks for the MSR-VTT dataset, on which our approach outperforms many
state-of-the-art methods.",1162,72
"['cs.CV', 'cs.LG']",Attention-Based Query Expansion Learning,"Query expansion is a technique widely used in image search consisting in
combining highly ranked images from an original query into an expanded query
that is then reissued, generally leading to increased recall and precision. An
important aspect of query expansion is choosing an appropriate way to combine
the images into a new query. Interestingly, despite the undeniable empirical
success of query expansion, ad-hoc methods with different caveats have
dominated the landscape, and not a lot of research has been done on learning
how to do query expansion. In this paper we propose a more principled framework
to query expansion, where one trains, in a discriminative manner, a model that
learns how images should be aggregated to form the expanded query. Within this
framework, we propose a model that leverages a self-attention mechanism to
effectively learn how to transfer information between the different images
before aggregating them. Our approach obtains higher accuracy than existing
approaches on standard benchmarks. More importantly, our approach is the only
one that consistently shows high accuracy under different regimes, overcoming
caveats of existing methods.",1180,40
"['cs.LG', 'stat.ML']",FleXOR: Trainable Fractional Quantization,"Quantization based on the binary codes is gaining attention because each
quantized bit can be directly utilized for computations without dequantization
using look-up tables. Previous attempts, however, only allow for integer
numbers of quantization bits, which ends up restricting the search space for
compression ratio and accuracy. In this paper, we propose an encryption
algorithm/architecture to compress quantized weights so as to achieve
fractional numbers of bits per weight. Decryption during inference is
implemented by digital XOR-gate networks added into the neural network model
while XOR gates are described by utilizing $\tanh(x)$ for backward propagation
to enable gradient calculations. We perform experiments using MNIST, CIFAR-10,
and ImageNet to show that inserting XOR gates learns quantization/encrypted bit
decisions through training and obtains high accuracy even for fractional sub
1-bit weights. As a result, our proposed method yields smaller size and higher
model accuracy compared to binary neural networks.",1035,41
"['cs.LG', 'stat.ML']",Lifting Interpretability-Performance Trade-off via Automated Feature Engineering,"Complex black-box predictive models may have high performance, but lack of
interpretability causes problems like lack of trust, lack of stability,
sensitivity to concept drift. On the other hand, achieving satisfactory
accuracy of interpretable models require more time-consuming work related to
feature engineering. Can we train interpretable and accurate models, without
timeless feature engineering? We propose a method that uses elastic black-boxes
as surrogate models to create a simpler, less opaque, yet still accurate and
interpretable glass-box models. New models are created on newly engineered
features extracted with the help of a surrogate model. We supply the analysis
by a large-scale benchmark on several tabular data sets from the OpenML
database. There are two results 1) extracting information from complex models
may improve the performance of linear models, 2) questioning a common myth that
complex machine learning models outperform linear models.",970,80
"['cs.CV', 'cs.CL']",Image Captioning with Visual Object Representations Grounded in the Textual Modality,"We present our work in progress exploring the possibilities of a shared
embedding space between textual and visual modality. Leveraging the textual
nature of object detection labels and the hypothetical expressiveness of
extracted visual object representations, we propose an approach opposite to the
current trend, grounding of the representations in the word embedding space of
the captioning system instead of grounding words or sentences in their
associated images. Based on the previous work, we apply additional grounding
losses to the image captioning training objective aiming to force visual object
representations to create more heterogeneous clusters based on their class
label and copy a semantic structure of the word embedding space. In addition,
we provide an analysis of the learned object vector space projection and its
impact on the IC system performance. With only slight change in performance,
grounded models reach the stopping criterion during training faster than the
unconstrained model, needing about two to three times less training updates.
Additionally, an improvement in structural correlation between the word
embeddings and both original and projected object vectors suggests that the
grounding is actually mutual.",1246,84
['cs.CV'],BlockGAN: Learning 3D Object-aware Scene Representations from Unlabelled Images,"We present BlockGAN, an image generative model that learns object-aware 3D
scene representations directly from unlabelled 2D images. Current work on scene
representation learning either ignores scene background or treats the whole
scene as one object. Meanwhile, work that considers scene compositionality
treats scene objects only as image patches or 2D layers with alpha maps.
Inspired by the computer graphics pipeline, we design BlockGAN to learn to
first generate 3D features of background and foreground objects, then combine
them into 3D features for the wholes cene, and finally render them into
realistic images. This allows BlockGAN to reason over occlusion and interaction
between objects' appearance, such as shadow and lighting, and provides control
over each object's 3D pose and identity, while maintaining image realism.
BlockGAN is trained end-to-end, using only unlabelled single images, without
the need for 3D geometry, pose labels, object masks, or multiple views of the
same scene. Our experiments show that using explicit 3D features to represent
objects allows BlockGAN to learn disentangled representations both in terms of
objects (foreground and background) and their properties (pose and identity).",1226,79
"['cs.LG', 'cs.CV', 'stat.ML']",Feedback Recurrent Autoencoder for Video Compression,"Recent advances in deep generative modeling have enabled efficient modeling
of high dimensional data distributions and opened up a new horizon for solving
data compression problems. Specifically, autoencoder based learned image or
video compression solutions are emerging as strong competitors to traditional
approaches. In this work, We propose a new network architecture, based on
common and well studied components, for learned video compression operating in
low latency mode. Our method yields state of the art MS-SSIM/rate performance
on the high-resolution UVG dataset, among both learned video compression
approaches and classical video compression methods (H.265 and H.264) in the
rate range of interest for streaming applications. Additionally, we provide an
analysis of existing approaches through the lens of their underlying
probabilistic graphical models. Finally, we point out issues with temporal
consistency and color shift observed in empirical evaluation, and suggest
directions forward to alleviate those.",1024,52
['cs.CV'],Inclusive GAN: Improving Data and Minority Coverage in Generative Models,"Generative Adversarial Networks (GANs) have brought about rapid progress
towards generating photorealistic images. Yet the equitable allocation of their
modeling capacity among subgroups has received less attention, which could lead
to potential biases against underrepresented minorities if left uncontrolled.
In this work, we first formalize the problem of minority inclusion as one of
data coverage, and then propose to improve data coverage by harmonizing
adversarial training with reconstructive generation. The experiments show that
our method outperforms the existing state-of-the-art methods in terms of data
coverage on both seen and unseen data. We develop an extension that allows
explicit control over the minority subgroups that the model should ensure to
include, and validate its effectiveness at little compromise from the overall
performance on the entire dataset. Code, models, and supplemental videos are
available at GitHub.",944,72
"['cs.LG', 'cs.DS', 'stat.ML']",A Non-generative Framework and Convex Relaxations for Unsupervised Learning,"We give a novel formal theoretical framework for unsupervised learning with
two distinctive characteristics. First, it does not assume any generative model
and based on a worst-case performance metric. Second, it is comparative, namely
performance is measured with respect to a given hypothesis class. This allows
to avoid known computational hardness results and improper algorithms based on
convex relaxations. We show how several families of unsupervised learning
models, which were previously only analyzed under probabilistic assumptions and
are otherwise provably intractable, can be efficiently learned in our framework
by convex optimization.",650,75
"['cs.LG', 'stat.ML']",Graph Convolutional Gaussian Processes For Link Prediction,"Link prediction aims to reveal missing edges in a graph. We address this task
with a Gaussian process that is transformed using simplified graph convolutions
to better leverage the inductive bias of the domain. To scale the Gaussian
process model to large graphs, we introduce a variational inducing point method
that places pseudo inputs on a graph-structured domain. We evaluate our model
on eight large graphs with up to thousands of nodes and report consistent
improvements over existing Gaussian process models as well as competitive
performance when compared to state-of-the-art graph neural network approaches.",617,58
['cs.CV'],IEGAN: Multi-purpose Perceptual Quality Image Enhancement Using Generative Adversarial Network,"Despite the breakthroughs in quality of image enhancement, an end-to-end
solution for simultaneous recovery of the finer texture details and sharpness
for degraded images with low resolution is still unsolved. Some existing
approaches focus on minimizing the pixel-wise reconstruction error which
results in a high peak signal-to-noise ratio. The enhanced images fail to
provide high-frequency details and are perceptually unsatisfying, i.e., they
fail to match the quality expected in a photo-realistic image. In this paper,
we present Image Enhancement Generative Adversarial Network (IEGAN), a
versatile framework capable of inferring photo-realistic natural images for
both artifact removal and super-resolution simultaneously. Moreover, we propose
a new loss function consisting of a combination of reconstruction loss, feature
loss and an edge loss counterpart. The feature loss helps to push the output
image to the natural image manifold and the edge loss preserves the sharpness
of the output image. The reconstruction loss provides low-level semantic
information to the generator regarding the quality of the generated images
compared to the original. Our approach has been experimentally proven to
recover photo-realistic textures from heavily compressed low-resolution images
on public benchmarks and our proposed high-resolution World100 dataset.",1359,94
"['cs.LG', 'cs.CV']",A multi-task deep learning model for the classification of Age-related Macular Degeneration,"Age-related Macular Degeneration (AMD) is a leading cause of blindness.
Although the Age-Related Eye Disease Study group previously developed a 9-step
AMD severity scale for manual classification of AMD severity from color fundus
images, manual grading of images is time-consuming and expensive. Built on our
previous work DeepSeeNet, we developed a novel deep learning model for
automated classification of images into the 9-step scale. Instead of predicting
the 9-step score directly, our approach simulates the reading center grading
process. It first detects four AMD characteristics (drusen area, geographic
atrophy, increased pigment, and depigmentation), then combines these to derive
the overall 9-step score. Importantly, we applied multi-task learning
techniques, which allowed us to train classification of the four
characteristics in parallel, share representation, and prevent overfitting.
Evaluation on two image datasets showed that the accuracy of the model exceeded
the current state-of-the-art model by > 10%.",1027,91
"['stat.ML', 'cs.LG']",Manifold Density Estimation via Generalized Dequantization,"Density estimation is an important technique for characterizing distributions
given observations. Much existing research on density estimation has focused on
cases wherein the data lies in a Euclidean space. However, some kinds of data
are not well-modeled by supposing that their underlying geometry is Euclidean.
Instead, it can be useful to model such data as lying on a {\it manifold} with
some known structure. For instance, some kinds of data may be known to lie on
the surface of a sphere. We study the problem of estimating densities on
manifolds. We propose a method, inspired by the literature on ""dequantization,""
which we interpret through the lens of a coordinate transformation of an
ambient Euclidean space and a smooth manifold of interest. Using methods from
normalizing flows, we apply this method to the dequantization of smooth
manifold structures in order to model densities on the sphere, tori, and the
orthogonal group.",942,58
['cs.CV'],Human Action Recognition from Various Data Modalities: A Review,"Human Action Recognition (HAR) aims to understand human behavior and assign a
label to each action. It has a wide range of applications, and therefore has
been attracting increasing attention in the field of computer vision. Human
actions can be represented using various data modalities, such as RGB,
skeleton, depth, infrared, point cloud, event stream, audio, acceleration,
radar, and WiFi signal, which encode different sources of useful yet distinct
information and have various advantages depending on the application scenarios.
Consequently, lots of existing works have attempted to investigate different
types of approaches for HAR using various modalities. In this paper, we present
a comprehensive survey of recent progress in deep learning methods for HAR
based on the type of input data modality. Specifically, we review the current
mainstream deep learning methods for single data modalities and multiple data
modalities, including the fusion-based and the co-learning-based frameworks. We
also present comparative results on several benchmark datasets for HAR,
together with insightful observations and inspiring future research directions.",1154,63
"['cs.CV', 'cs.RO']",CP-loss: Connectivity-preserving Loss for Road Curb Detection in Autonomous Driving with Aerial Images,"Road curb detection is important for autonomous driving. It can be used to
determine road boundaries to constrain vehicles on roads, so that potential
accidents could be avoided. Most of the current methods detect road curbs
online using vehicle-mounted sensors, such as cameras or 3-D Lidars. However,
these methods usually suffer from severe occlusion issues. Especially in
highly-dynamic traffic environments, most of the field of view is occupied by
dynamic objects. To alleviate this issue, we detect road curbs offline using
high-resolution aerial images in this paper. Moreover, the detected road curbs
can be used to create high-definition (HD) maps for autonomous vehicles.
Specifically, we first predict the pixel-wise segmentation map of road curbs,
and then conduct a series of post-processing steps to extract the graph
structure of road curbs. To tackle the disconnectivity issue in the
segmentation maps, we propose an innovative connectivity-preserving loss
(CP-loss) to improve the segmentation performance. The experimental results on
a public dataset demonstrate the effectiveness of our proposed loss function.
This paper is accompanied with a demonstration video and a supplementary
document, which are available at
\texttt{\url{https://sites.google.com/view/cp-loss}}.",1290,102
"['stat.ML', 'cs.LG']",Scalable Low-Rank Tensor Learning for Spatiotemporal Traffic Data Imputation,"Missing value problem in spatiotemporal traffic data has long been a
challenging topic, in particular for large-scale and high-dimensional data with
complex missing mechanisms and diverse degrees of missingness. Recent studies
based on tensor nuclear norm have demonstrated the superiority of tensor
learning in imputation tasks by effectively characterizing the complex
correlations/dependencies in spatiotemporal data. However, despite the
promising results, these approaches do not scale well to large data tensors. In
this paper, we focus on addressing the missing data imputation problem for
large-scale spatiotemporal traffic data. To achieve both high accuracy and
efficiency, we develop a scalable tensor learning model -- Low-Tubal-Rank
Smoothing Tensor Completion (LSTC-Tubal) -- based on the existing framework of
Low-Rank Tensor Completion, which is well-suited for spatiotemporal traffic
data that is characterized by multidimensional structure of location$\times$
time of day $\times$ day. In particular, the proposed LSTC-Tubal model involves
a scalable tensor nuclear norm minimization scheme by integrating linear
unitary transformation. Therefore, tensor nuclear norm minimization can be
solved by singular value thresholding on the transformed matrix of each day
while the day-to-day correlation can be effectively preserved by the unitary
transform matrix. We compare LSTC-Tubal with state-of-the-art baseline models,
and find that LSTC-Tubal can achieve competitive accuracy with a significantly
lower computational cost. In addition, the LSTC-Tubal will also benefit other
tasks in modeling large-scale spatiotemporal traffic data, such as
network-level traffic forecasting.",1696,76
"['cs.LG', 'cs.AI']",Grammar Based Directed Testing of Machine Learning Systems,"The massive progress of machine learning has seen its application over a
variety of domains in the past decade. But how do we develop a systematic,
scalable and modular strategy to validate machine-learning systems? We present,
to the best of our knowledge, the first approach, which provides a systematic
test framework for machine-learning systems that accepts grammar-based inputs.
Our OGMA approach automatically discovers erroneous behaviours in classifiers
and leverages these erroneous behaviours to improve the respective models. OGMA
leverages inherent robustness properties present in any well trained
machine-learning model to direct test generation and thus, implementing a
scalable test generation methodology. To evaluate our OGMA approach, we have
tested it on three real world natural language processing (NLP) classifiers. We
have found thousands of erroneous behaviours in these systems. We also compare
OGMA with a random test generation approach and observe that OGMA is more
effective than such random test generation by up to 489%.",1053,58
"['cs.LG', 'math.OC']",Costate-focused models for reinforcement learning,"Many recent algorithms for reinforcement learning are model-free and founded
on the Bellman equation. Here we present a method founded on the costate
equation and models of the state dynamics. We use the costate -- the gradient
of cost with respect to state -- to improve the policy and also to ""focus"" the
model, training it to detect and mimic those features of the environment that
are most relevant to its task. We show that this method can handle difficult
time-optimal control problems, driving deterministic or stochastic mechanical
systems quickly to a target. On these tasks it works well compared to deep
deterministic policy gradient, a recent Bellman method. And because it creates
a model, the costate method can also learn from mental practice.",758,49
"['stat.ML', 'cs.LG', 'stat.CO']",On transfer learning of neural networks using bi-fidelity data for uncertainty propagation,"Due to their high degree of expressiveness, neural networks have recently
been used as surrogate models for mapping inputs of an engineering system to
outputs of interest. Once trained, neural networks are computationally
inexpensive to evaluate and remove the need for repeated evaluations of
computationally expensive models in uncertainty quantification applications.
However, given the highly parameterized construction of neural networks,
especially deep neural networks, accurate training often requires large amounts
of simulation data that may not be available in the case of computationally
expensive systems. In this paper, to alleviate this issue for uncertainty
propagation, we explore the application of transfer learning techniques using
training data generated from both high- and low-fidelity models. We explore two
strategies for coupling these two datasets during the training procedure,
namely, the standard transfer learning and the bi-fidelity weighted learning.
In the former approach, a neural network model mapping the inputs to the
outputs of interest is trained based on the low-fidelity data. The
high-fidelity data is then used to adapt the parameters of the upper layer(s)
of the low-fidelity network, or train a simpler neural network to map the
output of the low-fidelity network to that of the high-fidelity model. In the
latter approach, the entire low-fidelity network parameters are updated using
data generated via a Gaussian process model trained with a small high-fidelity
dataset. The parameter updates are performed via a variant of stochastic
gradient descent with learning rates given by the Gaussian process model. Using
three numerical examples, we illustrate the utility of these bi-fidelity
transfer learning methods where we focus on accuracy improvement achieved by
transfer learning over standard training approaches.",1866,90
"['cs.LG', 'stat.ML']",BayesGrad: Explaining Predictions of Graph Convolutional Networks,"Recent advances in graph convolutional networks have significantly improved
the performance of chemical predictions, raising a new research question: ""how
do we explain the predictions of graph convolutional networks?"" A possible
approach to answer this question is to visualize evidence substructures
responsible for the predictions. For chemical property prediction tasks, the
sample size of the training data is often small and/or a label imbalance
problem occurs, where a few samples belong to a single class and the majority
of samples belong to the other classes. This can lead to uncertainty related to
the learned parameters of the machine learning model. To address this
uncertainty, we propose BayesGrad, utilizing the Bayesian predictive
distribution, to define the importance of each node in an input graph, which is
computed efficiently using the dropout technique. We demonstrate that BayesGrad
successfully visualizes the substructures responsible for the label prediction
in the artificial experiment, even when the sample size is small. Furthermore,
we use a real dataset to evaluate the effectiveness of the visualization. The
basic idea of BayesGrad is not limited to graph-structured data and can be
applied to other data types.",1248,65
"['cs.CV', 'cs.LG', 'eess.IV']",Uncertainty-Guided Progressive GANs for Medical Image Translation,"Image-to-image translation plays a vital role in tackling various medical
imaging tasks such as attenuation correction, motion correction, undersampled
reconstruction, and denoising. Generative adversarial networks have been shown
to achieve the state-of-the-art in generating high fidelity images for these
tasks. However, the state-of-the-art GAN-based frameworks do not estimate the
uncertainty in the predictions made by the network that is essential for making
informed medical decisions and subsequent revision by medical experts and has
recently been shown to improve the performance and interpretability of the
model. In this work, we propose an uncertainty-guided progressive learning
scheme for image-to-image translation. By incorporating aleatoric uncertainty
as attention maps for GANs trained in a progressive manner, we generate images
of increasing fidelity progressively. We demonstrate the efficacy of our model
on three challenging medical image translation tasks, including PET to CT
translation, undersampled MRI reconstruction, and MRI motion artefact
correction. Our model generalizes well in three different tasks and improves
performance over state of the art under full-supervision and weak-supervision
with limited data. Code is released here:
https://github.com/ExplainableML/UncerGuidedI2I",1318,65
['cs.CV'],Graph-Based Intercategory and Intermodality Network for Multilabel Classification and Melanoma Diagnosis of Skin Lesions in Dermoscopy and Clinical Images,"The identification of melanoma involves an integrated analysis of skin lesion
images acquired using the clinical and dermoscopy modalities. Dermoscopic
images provide a detailed view of the subsurface visual structures that
supplement the macroscopic clinical images. Melanoma diagnosis is commonly
based on the 7-point visual category checklist (7PC). The 7PC contains
intrinsic relationships between categories that can aid classification, such as
shared features, correlations, and the contributions of categories towards
diagnosis. Manual classification is subjective and prone to intra- and
interobserver variability. This presents an opportunity for automated methods
to improve diagnosis. Current state-of-the-art methods focus on a single image
modality and ignore information from the other, or do not fully leverage the
complementary information from both modalities. Further, there is not a method
to exploit the intercategory relationships in the 7PC. In this study, we
address these issues by proposing a graph-based intercategory and intermodality
network (GIIN) with two modules. A graph-based relational module (GRM)
leverages intercategorical relations, intermodal relations, and prioritises the
visual structure details from dermoscopy by encoding category representations
in a graph network. The category embedding learning module (CELM) captures
representations that are specialised for each category and support the GRM. We
show that our modules are effective at enhancing classification performance
using a public dataset of dermoscopy-clinical images, and show that our method
outperforms the state-of-the-art at classifying the 7PC categories and
diagnosis.",1681,154
['cs.CV'],Depth Extraction from Video Using Non-parametric Sampling,"We describe a technique that automatically generates plausible depth maps
from videos using non-parametric depth sampling. We demonstrate our technique
in cases where past methods fail (non-translating cameras and dynamic scenes).
Our technique is applicable to single images as well as videos. For videos, we
use local motion cues to improve the inferred depth maps, while optical flow is
used to ensure temporal depth consistency. For training and evaluation, we use
a Kinect-based system to collect a large dataset containing stereoscopic videos
with known depths. We show that our depth estimation technique outperforms the
state-of-the-art on benchmark databases. Our technique can be used to
automatically convert a monoscopic video into stereo for 3D visualization, and
we demonstrate this through a variety of visually pleasing results for indoor
and outdoor scenes, including results from the feature film Charade.",923,57
['cs.CV'],Bounding Box Regression with Uncertainty for Accurate Object Detection,"Large-scale object detection datasets (e.g., MS-COCO) try to define the
ground truth bounding boxes as clear as possible. However, we observe that
ambiguities are still introduced when labeling the bounding boxes. In this
paper, we propose a novel bounding box regression loss for learning bounding
box transformation and localization variance together. Our loss greatly
improves the localization accuracies of various architectures with nearly no
additional computation. The learned localization variance allows us to merge
neighboring bounding boxes during non-maximum suppression (NMS), which further
improves the localization performance. On MS-COCO, we boost the Average
Precision (AP) of VGG-16 Faster R-CNN from 23.6% to 29.1%. More importantly,
for ResNet-50-FPN Mask R-CNN, our method improves the AP and AP90 by 1.8% and
6.2% respectively, which significantly outperforms previous state-of-the-art
bounding box refinement methods. Our code and models are available at:
github.com/yihui-he/KL-Loss",1006,70
['cs.CV'],MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features,"In this work, we tackle the problem of instance segmentation, the task of
simultaneously solving object detection and semantic segmentation. Towards this
goal, we present a model, called MaskLab, which produces three outputs: box
detection, semantic segmentation, and direction prediction. Building on top of
the Faster-RCNN object detector, the predicted boxes provide accurate
localization of object instances. Within each region of interest, MaskLab
performs foreground/background segmentation by combining semantic and direction
prediction. Semantic segmentation assists the model in distinguishing between
objects of different semantic classes including background, while the direction
prediction, estimating each pixel's direction towards its corresponding center,
allows separating instances of the same semantic class. Moreover, we explore
the effect of incorporating recent successful methods from both segmentation
and detection (i.e. atrous convolution and hypercolumn). Our proposed model is
evaluated on the COCO instance segmentation benchmark and shows comparable
performance with other state-of-art models.",1122,96
"['cs.CV', 'cs.GR']",Deep Deformation Detail Synthesis for Thin Shell Models,"In physics-based cloth animation, rich folds and detailed wrinkles are
achieved at the cost of expensive computational resources and huge labor
tuning. Data-driven techniques make efforts to reduce the computation
significantly by a database. One type of methods relies on human poses to
synthesize fitted garments which cannot be applied to general cloth. Another
type of methods adds details to the coarse meshes without such restrictions.
However, existing works usually utilize coordinate-based representations which
cannot cope with large-scale deformation, and requires dense vertex
correspondences between coarse and fine meshes. Moreover, as such methods only
add details, they require coarse meshes to be close to fine meshes, which can
be either impossible, or require unrealistic constraints when generating fine
meshes. To address these challenges, we develop a temporally and spatially
as-consistent-as-possible deformation representation (named TS-ACAP) and a
DeformTransformer network to learn the mapping from low-resolution meshes to
detailed ones. This TS-ACAP representation is designed to ensure both spatial
and temporal consistency for sequential large-scale deformations from cloth
animations. With this representation, our DeformTransformer network first
utilizes two mesh-based encoders to extract the coarse and fine features,
respectively. To transduct the coarse features to the fine ones, we leverage
the Transformer network that consists of frame-level attention mechanisms to
ensure temporal coherence of the prediction. Experimental results show that our
method is able to produce reliable and realistic animations in various datasets
at high frame rates: 10 ~ 35 times faster than physics-based simulation, with
superior detail synthesis abilities than existing methods.",1803,55
"['cs.LG', 'stat.ML']",Causal Adversarial Network for Learning Conditional and Interventional Distributions,"We propose a generative Causal Adversarial Network (CAN) for learning and
sampling from conditional and interventional distributions. In contrast to the
existing CausalGAN which requires the causal graph to be given, our proposed
framework learns the causal relations from the data and generates samples
accordingly. The proposed CAN comprises a two-fold process namely Label
Generation Network (LGN) and Conditional Image Generation Network (CIGN). The
LGN is a GAN-based architecture which learns and samples from the causal model
over labels. The sampled labels are then fed to CIGN, a conditional GAN
architecture, which learns the relationships amongst labels and pixels and
pixels themselves and generates samples based on them. This framework is
equipped with an intervention mechanism which enables. the model to generate
samples from interventional distributions. We quantitatively and qualitatively
assess the performance of CAN and empirically show that our model is able to
generate both interventional and conditional samples without having access to
the causal graph for the application of face generation on CelebA data.",1135,84
"['cs.LG', 'cs.CC', 'cs.IT', 'math.IT', 'math.OC']",Near-Optimal Data Source Selection for Bayesian Learning,"We study a fundamental problem in Bayesian learning, where the goal is to
select a set of data sources with minimum cost while achieving a certain
learning performance based on the data streams provided by the selected data
sources. First, we show that the data source selection problem for Bayesian
learning is NP-hard. We then show that the data source selection problem can be
transformed into an instance of the submodular set covering problem studied in
the literature, and provide a standard greedy algorithm to solve the data
source selection problem with provable performance guarantees. Next, we propose
a fast greedy algorithm that improves the running times of the standard greedy
algorithm, while achieving performance guarantees that are comparable to those
of the standard greedy algorithm. The fast greedy algorithm can also be applied
to solve the general submodular set covering problem with performance
guarantees. Finally, we validate the theoretical results using numerical
examples, and show that the greedy algorithms work well in practice.",1062,56
"['cs.CV', 'cs.GR']",pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis,"We have witnessed rapid progress on 3D-aware image synthesis, leveraging
recent advances in generative visual models and neural rendering. Existing
approaches however fall short in two ways: first, they may lack an underlying
3D representation or rely on view-inconsistent rendering, hence synthesizing
images that are not multi-view consistent; second, they often depend upon
representation network architectures that are not expressive enough, and their
results thus lack in image quality. We propose a novel generative model, named
Periodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), for
high-quality 3D-aware image synthesis. $\pi$-GAN leverages neural
representations with periodic activation functions and volumetric rendering to
represent scenes as view-consistent 3D representations with fine detail. The
proposed approach obtains state-of-the-art results for 3D-aware image synthesis
with multiple real and synthetic datasets.",956,86
"['cs.LG', 'cs.AI', 'cs.MA', 'stat.ML']",Multi-Agent Generative Adversarial Imitation Learning,"Imitation learning algorithms can be used to learn a policy from expert
demonstrations without access to a reward signal. However, most existing
approaches are not applicable in multi-agent settings due to the existence of
multiple (Nash) equilibria and non-stationary environments. We propose a new
framework for multi-agent imitation learning for general Markov games, where we
build upon a generalized notion of inverse reinforcement learning. We further
introduce a practical multi-agent actor-critic algorithm with good empirical
performance. Our method can be used to imitate complex behaviors in
high-dimensional environments with multiple cooperative or competing agents.",679,53
"['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']",Learning to generalize to new compositions in image understanding,"Recurrent neural networks have recently been used for learning to describe
images using natural language. However, it has been observed that these models
generalize poorly to scenes that were not observed during training, possibly
depending too strongly on the statistics of the text in the training data. Here
we propose to describe images using short structured representations, aiming to
capture the crux of a description. These structured representations allow us to
tease-out and evaluate separately two types of generalization: standard
generalization to new images with similar scenes, and generalization to new
combinations of known entities. We compare two learning approaches on the
MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,
Attend and Tell), and a simple structured prediction model on top of a deep
network. We find that the structured model generalizes to new compositions
substantially better than the LSTM, ~7 times the accuracy of predicting
structured representations. By providing a concrete method to quantify
generalization for unseen combinations, we argue that structured
representations and compositional splits are a useful benchmark for image
captioning, and advocate compositional models that capture linguistic and
visual structure.",1295,65
"['cs.CV', 'cs.GR']",Shading Annotations in the Wild,"Understanding shading effects in images is critical for a variety of vision
and graphics problems, including intrinsic image decomposition, shadow removal,
image relighting, and inverse rendering. As is the case with other vision
tasks, machine learning is a promising approach to understanding shading - but
there is little ground truth shading data available for real-world images. We
introduce Shading Annotations in the Wild (SAW), a new large-scale, public
dataset of shading annotations in indoor scenes, comprised of multiple forms of
shading judgments obtained via crowdsourcing, along with shading annotations
automatically generated from RGB-D imagery. We use this data to train a
convolutional neural network to predict per-pixel shading information in an
image. We demonstrate the value of our data and network in an application to
intrinsic images, where we can reduce decomposition artifacts produced by
existing algorithms. Our database is available at
http://opensurfaces.cs.cornell.edu/saw/.",1008,31
"['cs.LG', 'cs.AI']",Evolved Policy Gradients,"We propose a metalearning approach for learning gradient-based reinforcement
learning (RL) algorithms. The idea is to evolve a differentiable loss function,
such that an agent, which optimizes its policy to minimize this loss, will
achieve high rewards. The loss is parametrized via temporal convolutions over
the agent's experience. Because this loss is highly flexible in its ability to
take into account the agent's history, it enables fast task learning. Empirical
results show that our evolved policy gradient algorithm (EPG) achieves faster
learning on several randomized environments compared to an off-the-shelf policy
gradient method. We also demonstrate that EPG's learned loss can generalize to
out-of-distribution test time tasks, and exhibits qualitatively different
behavior from other popular metalearning algorithms.",832,24
['cs.LG'],Do not explain without context: addressing the blind spot of model explanations,"The increasing number of regulations and expectations of predictive machine
learning models, such as so called right to explanation, has led to a large
number of methods promising greater interpretability. High demand has led to a
widespread adoption of XAI techniques like Shapley values, Partial Dependence
profiles or permutational variable importance. However, we still do not know
enough about their properties and how they manifest in the context in which
explanations are created by analysts, reviewed by auditors, and interpreted by
various stakeholders. This paper highlights a blind spot which, although
critical, is often overlooked when monitoring and auditing machine learning
models: the effect of the reference data on the explanation calculation. We
discuss that many model explanations depend directly or indirectly on the
choice of the referenced data distribution. We showcase examples where small
changes in the distribution lead to drastic changes in the explanations, such
as a change in trend or, alarmingly, a conclusion. Consequently, we postulate
that obtaining robust and useful explanations always requires supporting them
with a broader context.",1174,79
['cs.LG'],Projection-free Graph-based Classifier Learning using Gershgorin Disc Perfect Alignment,"In semi-supervised graph-based binary classifier learning, a subset of known
labels $\hat{x}_i$ are used to infer unknown labels, assuming that the label
signal $x$ is smooth with respect to a similarity graph specified by a
Laplacian matrix. When restricting labels $x_i$ to binary values, the problem
is NP-hard. While a conventional semi-definite programming (SDP) relaxation can
be solved in polynomial time using, for example, the alternating direction
method of multipliers (ADMM), the complexity of iteratively projecting a
candidate matrix $M$ onto the positive semi-definite (PSD) cone ($M \succeq 0$)
remains high. In this paper, leveraging a recent linear algebraic theory called
Gershgorin disc perfect alignment (GDPA), we propose a fast projection-free
method by solving a sequence of linear programs (LP) instead. Specifically, we
first recast the SDP relaxation to its SDP dual, where a feasible solution $H
\succeq 0$ can be interpreted as a Laplacian matrix corresponding to a balanced
signed graph sans the last node. To achieve graph balance, we split the last
node into two that respectively contain the original positive and negative
edges, resulting in a new Laplacian $\bar{H}$. We repose the SDP dual for
solution $\bar{H}$, then replace the PSD cone constraint $\bar{H} \succeq 0$
with linear constraints derived from GDPA -- sufficient conditions to ensure
$\bar{H}$ is PSD -- so that the optimization becomes an LP per iteration.
Finally, we extract predicted labels from our converged LP solution $\bar{H}$.
Experiments show that our algorithm enjoyed a $40\times$ speedup on average
over the next fastest scheme while retaining comparable label prediction
performance.",1698,87
"['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",Tractable Reinforcement Learning of Signal Temporal Logic Objectives,"Signal temporal logic (STL) is an expressive language to specify time-bound
real-world robotic tasks and safety specifications. Recently, there has been an
interest in learning optimal policies to satisfy STL specifications via
reinforcement learning (RL). Learning to satisfy STL specifications often needs
a sufficient length of state history to compute reward and the next action. The
need for history results in exponential state-space growth for the learning
problem. Thus the learning problem becomes computationally intractable for most
real-world applications. In this paper, we propose a compact means to capture
state history in a new augmented state-space representation. An approximation
to the objective (maximizing probability of satisfaction) is proposed and
solved for in the new augmented state-space. We show the performance bound of
the approximate solution and compare it with the solution of an existing
technique via simulations.",951,68
"['cs.CV', 'cs.LG']",Learning to Detect Objects with a 1 Megapixel Event Camera,"Event cameras encode visual information with high temporal precision, low
data-rate, and high-dynamic range. Thanks to these characteristics, event
cameras are particularly suited for scenarios with high motion, challenging
lighting conditions and requiring low latency. However, due to the novelty of
the field, the performance of event-based systems on many vision tasks is still
lower compared to conventional frame-based solutions. The main reasons for this
performance gap are: the lower spatial resolution of event sensors, compared to
frame cameras; the lack of large-scale training datasets; the absence of well
established deep learning architectures for event-based processing. In this
paper, we address all these problems in the context of an event-based object
detection task. First, we publicly release the first high-resolution
large-scale dataset for object detection. The dataset contains more than 14
hours recordings of a 1 megapixel event camera, in automotive scenarios,
together with 25M bounding boxes of cars, pedestrians, and two-wheelers,
labeled at high frequency. Second, we introduce a novel recurrent architecture
for event-based detection and a temporal consistency loss for better-behaved
training. The ability to compactly represent the sequence of events into the
internal memory of the model is essential to achieve high accuracy. Our model
outperforms by a large margin feed-forward event-based architectures. Moreover,
our method does not require any reconstruction of intensity images from events,
showing that training directly from raw events is possible, more efficient, and
more accurate than passing through an intermediate intensity image. Experiments
on the dataset introduced in this work, for which events and gray level images
are available, show performance on par with that of highly tuned and studied
frame-based detectors.",1873,58
"['cs.CV', 'cs.AI', 'cs.LG', 'eess.IV']",Restoration of marker occluded hematoxylin and eosin stained whole slide histology images using generative adversarial networks,"It is common for pathologists to annotate specific regions of the tissue,
such as tumor, directly on the glass slide with markers. Although this practice
was helpful prior to the advent of histology whole slide digitization, it often
occludes important details which are increasingly relevant to immuno-oncology
due to recent advancements in digital pathology imaging techniques. The current
work uses a generative adversarial network with cycle loss to remove these
annotations while still maintaining the underlying structure of the tissue by
solving an image-to-image translation problem. We train our network on up to
300 whole slide images with marker inks and show that 70% of the corrected
image patches are indistinguishable from originally uncontaminated image tissue
to a human expert. This portion increases 97% when we replace the human expert
with a deep residual network. We demonstrated the fidelity of the method to the
original image by calculating the correlation between image gradient
magnitudes. We observed a revival of up to 94,000 nuclei per slide in our
dataset, the majority of which were located on tissue border.",1140,127
"['cs.LG', 'cs.AI', 'cs.IT', 'cs.NE', 'math.IT', 'physics.data-an']",Efficient Methods for Unsupervised Learning of Probabilistic Models,"In this thesis I develop a variety of techniques to train, evaluate, and
sample from intractable and high dimensional probabilistic models. Abstract
exceeds arXiv space limitations -- see PDF.",192,67
"['cs.LG', 'cs.CV']",A general approach to compute the relevance of middle-level input features,"This work proposes a novel general framework, in the context of eXplainable
Artificial Intelligence (XAI), to construct explanations for the behaviour of
Machine Learning (ML) models in terms of middle-level features. One can isolate
two different ways to provide explanations in the context of XAI: low and
middle-level explanations. Middle-level explanations have been introduced for
alleviating some deficiencies of low-level explanations such as, in the context
of image classification, the fact that human users are left with a significant
interpretive burden: starting from low-level explanations, one has to identify
properties of the overall input that are perceptually salient for the human
visual system. However, a general approach to correctly evaluate the elements
of middle-level explanations with respect ML model responses has never been
proposed in the literature.",881,74
"['cs.CV', 'cs.LG']",Unsupervised Learning of Neural Networks to Explain Neural Networks,"This paper presents an unsupervised method to learn a neural network, namely
an explainer, to interpret a pre-trained convolutional neural network (CNN),
i.e., explaining knowledge representations hidden in middle conv-layers of the
CNN. Given feature maps of a certain conv-layer of the CNN, the explainer
performs like an auto-encoder, which first disentangles the feature maps into
object-part features and then inverts object-part features back to features of
higher conv-layers of the CNN. More specifically, the explainer contains
interpretable conv-layers, where each filter disentangles the representation of
a specific object part from chaotic input feature maps. As a paraphrase of CNN
features, the disentangled representations of object parts help people
understand the logic inside the CNN. We also learn the explainer to use
object-part features to reconstruct features of higher CNN layers, in order to
minimize loss of information during the feature disentanglement. More
crucially, we learn the explainer via network distillation without using any
annotations of sample labels, object parts, or textures for supervision. We
have applied our method to different types of CNNs for evaluation, and
explainers have significantly boosted the interpretability of CNN features.",1287,67
['cs.CV'],Capsule Network is Not More Robust than Convolutional Network,"The Capsule Network is widely believed to be more robust than Convolutional
Networks. However, there are no comprehensive comparisons between these two
networks, and it is also unknown which components in the CapsNet affect its
robustness. In this paper, we first carefully examine the special designs in
CapsNet that differ from that of a ConvNet commonly used for image
classification. The examination reveals five major new/different components in
CapsNet: a transformation process, a dynamic routing layer, a squashing
function, a marginal loss other than cross-entropy loss, and an additional
class-conditional reconstruction loss for regularization. Along with these
major differences, we conduct comprehensive ablation studies on three kinds of
robustness, including affine transformation, overlapping digits, and semantic
representation. The study reveals that some designs, which are thought critical
to CapsNet, actually can harm its robustness, i.e., the dynamic routing layer
and the transformation process, while others are beneficial for the robustness.
Based on these findings, we propose enhanced ConvNets simply by introducing the
essential components behind the CapsNet's success. The proposed simple ConvNets
can achieve better robustness than the CapsNet.",1275,61
"['cs.CV', 'cs.CL', 'cs.LG']",Component Analysis for Visual Question Answering Architectures,"Recent research advances in Computer Vision and Natural Language Processing
have introduced novel tasks that are paving the way for solving AI-complete
problems. One of those tasks is called Visual Question Answering (VQA). A VQA
system must take an image and a free-form, open-ended natural language question
about the image, and produce a natural language answer as the output. Such a
task has drawn great attention from the scientific community, which generated a
plethora of approaches that aim to improve the VQA predictive accuracy. Most of
them comprise three major components: (i) independent representation learning
of images and questions; (ii) feature fusion so the model can use information
from both sources to answer visual questions; and (iii) the generation of the
correct answer in natural language. With so many approaches being recently
introduced, it became unclear the real contribution of each component for the
ultimate performance of the model. The main goal of this paper is to provide a
comprehensive analysis regarding the impact of each component in VQA models.
Our extensive set of experiments cover both visual and textual elements, as
well as the combination of these representations in form of fusion and
attention mechanisms. Our major contribution is to identify core components for
training VQA models so as to maximize their predictive performance.",1384,62
"['cs.LG', 'cs.AI', 'cs.MA', 'cs.RO', 'stat.ML', '93C85, 68T40, 68T05', 'I.2.11; I.2.9; I.2.6']",Multi-Agent Connected Autonomous Driving using Deep Reinforcement Learning,"The capability to learn and adapt to changes in the driving environment is
crucial for developing autonomous driving systems that are scalable beyond
geo-fenced operational design domains. Deep Reinforcement Learning (RL)
provides a promising and scalable framework for developing adaptive learning
based solutions. Deep RL methods usually model the problem as a (Partially
Observable) Markov Decision Process in which an agent acts in a stationary
environment to learn an optimal behavior policy. However, driving involves
complex interaction between multiple, intelligent (artificial or human) agents
in a highly non-stationary environment. In this paper, we propose the use of
Partially Observable Markov Games(POSG) for formulating the connected
autonomous driving problems with realistic assumptions. We provide a taxonomy
of multi-agent learning environments based on the nature of tasks, nature of
agents and the nature of the environment to help in categorizing various
autonomous driving problems that can be addressed under the proposed
formulation. As our main contributions, we provide MACAD-Gym, a Multi-Agent
Connected, Autonomous Driving agent learning platform for furthering research
in this direction. Our MACAD-Gym platform provides an extensible set of
Connected Autonomous Driving (CAD) simulation environments that enable the
research and development of Deep RL- based integrated sensing, perception,
planning and control algorithms for CAD systems with unlimited operational
design domain under realistic, multi-agent settings. We also share the
MACAD-Agents that were trained successfully using the MACAD-Gym platform to
learn control policies for multiple vehicle agents in a partially observable,
stop-sign controlled, 3-way urban intersection environment with raw (camera)
sensor observations.",1820,74
"['cs.CV', 'cs.LG']",Dynamic Clone Transformer for Efficient Convolutional Neural Netwoks,"Convolutional networks (ConvNets) have shown impressive capability to solve
various vision tasks. Nevertheless, the trade-off between performance and
efficiency is still a challenge for a feasible model deployment on
resource-constrained platforms. In this paper, we introduce a novel concept
termed multi-path fully connected pattern (MPFC) to rethink the
interdependencies of topology pattern, accuracy and efficiency for ConvNets.
Inspired by MPFC, we further propose a dual-branch module named dynamic clone
transformer (DCT) where one branch generates multiple replicas from inputs and
another branch reforms those clones through a series of difference vectors
conditional on inputs itself to produce more variants. This operation allows
the self-expansion of channel-wise information in a data-driven way with little
computational cost while providing sufficient learning capacity, which is a
potential unit to replace computationally expensive pointwise convolution as an
expansion layer in the bottleneck structure.",1023,68
"['cs.CV', 'cs.LG']",OFF-ApexNet on Micro-expression Recognition System,"When a person attempts to conceal an emotion, the genuine emotion is manifest
as a micro-expression. Exploration of automatic facial micro-expression
recognition systems is relatively new in the computer vision domain. This is
due to the difficulty in implementing optimal feature extraction methods to
cope with the subtlety and brief motion characteristics of the expression. Most
of the existing approaches extract the subtle facial movements based on
hand-crafted features. In this paper, we address the micro-expression
recognition task with a convolutional neural network (CNN) architecture, which
well integrates the features extracted from each video. A new feature
descriptor, Optical Flow Features from Apex frame Network (OFF-ApexNet) is
introduced. This feature descriptor combines the optical ow guided context with
the CNN. Firstly, we obtain the location of the apex frame from each video
sequence as it portrays the highest intensity of facial motion among all
frames. Then, the optical ow information are attained from the apex frame and a
reference frame (i.e., onset frame). Finally, the optical flow features are fed
into a pre-designed CNN model for further feature enhancement as well as to
carry out the expression classification. To evaluate the effectiveness of
OFF-ApexNet, comprehensive evaluations are conducted on three public
spontaneous micro-expression datasets (i.e., SMIC, CASME II and SAMM). The
promising recognition result suggests that the proposed method can optimally
describe the significant micro-expression details. In particular, we report
that, in a multi-database with leave-one-subject-out cross-validation
experimental protocol, the recognition performance reaches 74.60% of
recognition accuracy and F-measure of 71.04%. We also note that this is the
first work that performs cross-dataset validation on three databases in this
domain.",1883,50
"['cs.LG', 'stat.ML']",Automatic Remaining Useful Life Estimation Framework with Embedded Convolutional LSTM as the Backbone,"An essential task in predictive maintenance is the prediction of the
Remaining Useful Life (RUL) through the analysis of multivariate time series.
Using the sliding window method, Convolutional Neural Network (CNN) and
conventional Recurrent Neural Network (RNN) approaches have produced impressive
results on this matter, due to their ability to learn optimized features.
However, sequence information is only partially modeled by CNN approaches. Due
to the flatten mechanism in conventional RNNs, like Long Short Term Memories
(LSTM), the temporal information within the window is not fully preserved. To
exploit the multi-level temporal information, many approaches are proposed
which combine CNN and RNN models. In this work, we propose a new LSTM variant
called embedded convolutional LSTM (ECLSTM). In ECLSTM a group of different 1D
convolutions is embedded into the LSTM structure. Through this, the temporal
information is preserved between and within windows. Since the hyper-parameters
of models require careful tuning, we also propose an automated prediction
framework based on the Bayesian optimization with hyperband optimizer, which
allows for efficient optimization of the network architecture. Finally, we show
the superiority of our proposed ECLSTM approach over the state-of-the-art
approaches on several widely used benchmark data sets for RUL Estimation.",1374,101
"['cs.LG', 'cs.AI', 'eess.SP']",Designing ECG Monitoring Healthcare System with Federated Transfer Learning and Explainable AI,"Deep learning play a vital role in classifying different arrhythmias using
the electrocardiography (ECG) data. Nevertheless, training deep learning models
normally requires a large amount of data and it can lead to privacy concerns.
Unfortunately, a large amount of healthcare data cannot be easily collected
from a single silo. Additionally, deep learning models are like black-box, with
no explainability of the predicted results, which is often required in clinical
healthcare. This limits the application of deep learning in real-world health
systems. In this paper, we design a new explainable artificial intelligence
(XAI) based deep learning framework in a federated setting for ECG-based
healthcare applications. The federated setting is used to solve issues such as
data availability and privacy concerns. Furthermore, the proposed framework
setting effectively classifies arrhythmia's using an autoencoder and a
classifier, both based on a convolutional neural network (CNN). Additionally,
we propose an XAI-based module on top of the proposed classifier to explain the
classification results, which help clinical practitioners make quick and
reliable decisions. The proposed framework was trained and tested using the
MIT-BIH Arrhythmia database. The classifier achieved accuracy up to 94% and 98%
for arrhythmia detection using noisy and clean data, respectively, with
five-fold cross-validation.",1408,94
"['cs.LG', 'cs.AI', 'stat.ML']",Human-centric Transfer Learning Explanation via Knowledge Graph [Extended Abstract],"Transfer learning which aims at utilizing knowledge learned from one problem
(source domain) to solve another different but related problem (target domain)
has attracted wide research attentions. However, the current transfer learning
methods are mostly uninterpretable, especially to people without ML expertise.
In this extended abstract, we brief introduce two knowledge graph (KG) based
frameworks towards human understandable transfer learning explanation. The
first one explains the transferability of features learned by Convolutional
Neural Network (CNN) from one domain to another through pre-training and
fine-tuning, while the second justifies the model of a target domain predicted
by models from multiple source domains in zero-shot learning (ZSL). Both
methods utilize KG and its reasoning capability to provide rich and human
understandable explanations to the transfer procedure.",895,83
['cs.CV'],Co-salient Object Detection Based on Deep Saliency Networks and Seed Propagation over an Integrated Graph,"This paper presents a co-salient object detection method to find common
salient regions in a set of images. We utilize deep saliency networks to
transfer co-saliency prior knowledge and better capture high-level semantic
information, and the resulting initial co-saliency maps are enhanced by seed
propagation steps over an integrated graph. The deep saliency networks are
trained in a supervised manner to avoid online weakly supervised learning and
exploit them not only to extract high-level features but also to produce both
intra- and inter-image saliency maps. Through a refinement step, the initial
co-saliency maps can uniformly highlight co-salient regions and locate accurate
object boundaries. To handle input image groups inconsistent in size, we
propose to pool multi-regional descriptors including both within-segment and
within-group information. In addition, the integrated multilayer graph is
constructed to find the regions that the previous steps may not detect by seed
propagation with low-level descriptors. In this work, we utilize the useful
complementary components of high-, low-level information, and several
learning-based steps. Our experiments have demonstrated that the proposed
approach outperforms comparable co-saliency detection methods on widely used
public databases and can also be directly applied to co-segmentation tasks.",1361,105
['cs.CV'],Spatio-temporal Person Retrieval via Natural Language Queries,"In this paper, we address the problem of spatio-temporal person retrieval
from multiple videos using a natural language query, in which we output a tube
(i.e., a sequence of bounding boxes) which encloses the person described by the
query. For this problem, we introduce a novel dataset consisting of videos
containing people annotated with bounding boxes for each second and with five
natural language descriptions. To retrieve the tube of the person described by
a given natural language query, we design a model that combines methods for
spatio-temporal human detection and multimodal retrieval. We conduct
comprehensive experiments to compare a variety of tube and text representations
and multimodal retrieval methods, and present a strong baseline in this task as
well as demonstrate the efficacy of our tube representation and multimodal
feature embedding technique. Finally, we demonstrate the versatility of our
model by applying it to two other important tasks.",971,61
['cs.CV'],A Fusion Approach for Multi-Frame Optical Flow Estimation,"To date, top-performing optical flow estimation methods only take pairs of
consecutive frames into account. While elegant and appealing, the idea of using
more than two frames has not yet produced state-of-the-art results. We present
a simple, yet effective fusion approach for multi-frame optical flow that
benefits from longer-term temporal cues. Our method first warps the optical
flow from previous frames to the current, thereby yielding multiple plausible
estimates. It then fuses the complementary information carried by these
estimates into a new optical flow field. At the time of writing, our method
ranks first among published results in the MPI Sintel and KITTI 2015
benchmarks. Our models will be available on https://github.com/NVlabs/PWC-Net.",757,57
"['cs.CV', 'cs.LG', 'cs.RO']",Crowdsourced 3D Mapping: A Combined Multi-View Geometry and Self-Supervised Learning Approach,"The ability to efficiently utilize crowdsourced visual data carries immense
potential for the domains of large scale dynamic mapping and autonomous
driving. However, state-of-the-art methods for crowdsourced 3D mapping assume
prior knowledge of camera intrinsics. In this work, we propose a framework that
estimates the 3D positions of semantically meaningful landmarks such as traffic
signs without assuming known camera intrinsics, using only monocular color
camera and GPS. We utilize multi-view geometry as well as deep learning based
self-calibration, depth, and ego-motion estimation for traffic sign
positioning, and show that combining their strengths is important for
increasing the map coverage. To facilitate research on this task, we construct
and make available a KITTI based 3D traffic sign ground truth positioning
dataset. Using our proposed framework, we achieve an average single-journey
relative and absolute positioning accuracy of 39cm and 1.26m respectively, on
this dataset.",997,93
['cs.CV'],COLA-Net: Collaborative Attention Network for Image Restoration,"Local and non-local attention-based methods have been well studied in various
image restoration tasks while leading to promising performance. However, most
of the existing methods solely focus on one type of attention mechanism (local
or non-local). Furthermore, by exploiting the self-similarity of natural
images, existing pixel-wise non-local attention operations tend to give rise to
deviations in the process of characterizing long-range dependence due to image
degeneration. To overcome these problems, in this paper we propose a novel
collaborative attention network (COLA-Net) for image restoration, as the first
attempt to combine local and non-local attention mechanisms to restore image
content in the areas with complex textures and with highly repetitive details
respectively. In addition, an effective and robust patch-wise non-local
attention model is developed to capture long-range feature correspondences
through 3D patches. Extensive experiments on synthetic image denoising, real
image denoising and compression artifact reduction tasks demonstrate that our
proposed COLA-Net is able to achieve state-of-the-art performance in both peak
signal-to-noise ratio and visual perception, while maintaining an attractive
computational complexity. The source code is available on
https://github.com/MC-E/COLA-Net.",1325,63
"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']",NEAT: Neural Attention Fields for End-to-End Autonomous Driving,"Efficient reasoning about the semantic, spatial, and temporal structure of a
scene is a crucial prerequisite for autonomous driving. We present NEural
ATtention fields (NEAT), a novel representation that enables such reasoning for
end-to-end imitation learning models. NEAT is a continuous function which maps
locations in Bird's Eye View (BEV) scene coordinates to waypoints and
semantics, using intermediate attention maps to iteratively compress
high-dimensional 2D image features into a compact representation. This allows
our model to selectively attend to relevant regions in the input while ignoring
information irrelevant to the driving task, effectively associating the images
with the BEV representation. In a new evaluation setting involving adverse
environmental conditions and challenging scenarios, NEAT outperforms several
strong baselines and achieves driving scores on par with the privileged CARLA
expert used to generate its training data. Furthermore, visualizing the
attention maps for models with NEAT intermediate representations provides
improved interpretability.",1088,63
"['cs.LG', 'stat.ML']",Continuous-Time Model-Based Reinforcement Learning,"Model-based reinforcement learning (MBRL) approaches rely on discrete-time
state transition models whereas physical systems and the vast majority of
control tasks operate in continuous-time. To avoid time-discretization
approximation of the underlying process, we propose a continuous-time MBRL
framework based on a novel actor-critic method. Our approach also infers the
unknown state evolution differentials with Bayesian neural ordinary
differential equations (ODE) to account for epistemic uncertainty. We implement
and test our method on a new ODE-RL suite that explicitly solves
continuous-time control systems. Our experiments illustrate that the model is
robust against irregular and noisy data, is sample-efficient, and can solve
control problems which pose challenges to discrete-time MBRL methods.",808,50
['cs.CV'],Infant brain MRI segmentation with dilated convolution pyramid downsampling and self-attention,"In this paper, we propose a dual aggregation network to adaptively aggregate
different information in infant brain MRI segmentation. More precisely, we
added two modules based on 3D-UNet to better model information at different
levels and locations. The dilated convolution pyramid downsampling module is
mainly to solve the problem of loss of spatial information on the downsampling
process, and it can effectively save details while reducing the resolution. The
self-attention module can integrate the remote dependence on the feature maps
in two dimensions of spatial and channel, effectively improving the
representation ability and discriminating ability of the model. Our results are
compared to the winners of iseg2017's first evaluation, the DICE ratio of WM
and GM increased by 0.7%, and CSF is comparable.In the latest evaluation of the
iseg-2019 cross-dataset challenge,we achieve the first place in the DICE of WM
and GM, and the DICE of CSF is second.",964,94
"['cs.CV', 'cs.LG']",Intra-clip Aggregation for Video Person Re-identification,"Video-based person re-identification has drawn massive attention in recent
years due to its extensive applications in video surveillance. While deep
learning-based methods have led to significant progress, these methods are
limited by ineffectively using complementary information, which is blamed on
necessary data augmentation in the training process. Data augmentation has been
widely used to mitigate the over-fitting trap and improve the ability of
network representation. However, the previous methods adopt image-based data
augmentation scheme to individually process the input frames, which corrupts
the complementary information between consecutive frames and causes performance
degradation. Extensive experiments on three benchmark datasets demonstrate that
our framework outperforms the most recent state-of-the-art methods. We also
perform cross-dataset validation to prove the generality of our method.",915,57
['cs.CV'],Adversarial Feedback Loop,"Thanks to their remarkable generative capabilities, GANs have gained great
popularity, and are used abundantly in state-of-the-art methods and
applications. In a GAN based model, a discriminator is trained to learn the
real data distribution. To date, it has been used only for training purposes,
where it's utilized to train the generator to provide real-looking outputs. In
this paper we propose a novel method that makes an explicit use of the
discriminator in test-time, in a feedback manner in order to improve the
generator results. To the best of our knowledge it is the first time a
discriminator is involved in test-time. We claim that the discriminator holds
significant information on the real data distribution, that could be useful for
test-time as well, a potential that has not been explored before.
  The approach we propose does not alter the conventional training stage. At
test-time, however, it transfers the output from the generator into the
discriminator, and uses feedback modules (convolutional blocks) to translate
the features of the discriminator layers into corrections to the features of
the generator layers, which are used eventually to get a better generator
result. Our method can contribute to both conditional and unconditional GANs.
As demonstrated by our experiments, it can improve the results of
state-of-the-art networks for super-resolution, and image generation.",1405,25
['cs.LG'],Searching for Accurate Binary Neural Architectures,"Binary neural networks have attracted tremendous attention due to the
efficiency for deploying them on mobile devices. Since the weak expression
ability of binary weights and features, their accuracy is usually much lower
than that of full-precision (i.e. 32-bit) models. Here we present a new frame
work for automatically searching for compact but accurate binary neural
networks. In practice, number of channels in each layer will be encoded into
the search space and optimized using the evolutionary algorithm. Experiments
conducted on benchmark datasets and neural architectures demonstrate that our
searched binary networks can achieve the performance of full-precision models
with acceptable increments on model sizes and calculations.",741,50
['cs.CV'],Learning Predicates as Functions to Enable Few-shot Scene Graph Prediction,"Scene graph prediction --- classifying the set of objects and predicates in a
visual scene --- requires substantial training data. However, most predicates
only occur a handful of times making them difficult to learn. We introduce the
first scene graph prediction model that supports few-shot learning of
predicates. Existing scene graph generation models represent objects using
pretrained object detectors or word embeddings that capture semantic object
information at the cost of encoding information about which relationships they
afford. So, these object representations are unable to generalize to new
few-shot relationships. We introduce a framework that induces object
representations that are structured according to their visual relationships.
Unlike past methods, our framework embeds objects that afford similar
relationships closer together. This property allows our model to perform well
in the few-shot setting. For example, applying the 'riding' predicate
transformation to 'person' modifies the representation towards objects like
'skateboard' and 'horse' that enable riding. We generate object representations
by learning predicates trained as message passing functions within a new graph
convolution framework. The object representations are used to build few-shot
predicate classifiers for rare predicates with as few as 1 labeled example. We
achieve a 5-shot performance of 22.70 recall@50, a 3.7 increase when compared
to strong transfer learning baselines.",1479,74
"['cs.CV', 'cs.NE', 'eess.IV']",An Internal Clock Based Space-time Neural Network for Motion Speed Recognition,"In this work we present a novel internal clock based space-time neural
network for motion speed recognition. The developed system has a spike train
encoder, a Spiking Neural Network (SNN) with internal clocking behaviors, a
pattern transformation block and a Network Dynamic Dependent Plasticity (NDDP)
learning block. The core principle is that the developed SNN will automatically
tune its network pattern frequency (internal clock frequency) to recognize
human motions in a speed domain. We employed both cartoons and real-world
videos as training benchmarks, results demonstrate that our system can not only
recognize motions with considerable speed differences (e.g. run, walk, jump,
wonder(think) and standstill), but also motions with subtle speed gaps such as
run and fast walk. The inference accuracy can be up to 83.3% (cartoon videos)
and 75% (real-world videos). Meanwhile, the system only requires six video
datasets in the learning stage and with up to 42 training trials. Hardware
performance estimation indicates that the training time is 0.84-4.35s and power
consumption is 33.26-201mW (based on an ARM Cortex M4 processor). Therefore,
our system takes unique learning advantages of the requirement of the small
dataset, quick learning and low power performance, which shows great potentials
for edge or scalable AI-based applications.",1352,78
['cs.CV'],Towards Purely Unsupervised Disentanglement of Appearance and Shape for Person Images Generation,"There have been a fairly of research interests in exploring the
disentanglement of appearance and shape from human images. Most existing
endeavours pursuit this goal by either using training images with annotations
or regulating the training process with external clues such as human skeleton,
body segmentation or cloth patches etc. In this paper, we aim to address this
challenge in a more unsupervised manner---we do not require any annotation nor
any external task-specific clues. To this end, we formulate an
encoder-decoder-like network to extract both the shape and appearance features
from input images at the same time, and train the parameters by three losses:
feature adversarial loss, color consistency loss and reconstruction loss. The
feature adversarial loss mainly impose little to none mutual information
between the extracted shape and appearance features, while the color
consistency loss is to encourage the invariance of person appearance
conditioned on different shapes. More importantly, our unsupervised
(Unsupervised learning has many interpretations in different tasks. To be
clear, in this paper, we refer unsupervised learning as learning without
task-specific human annotations, pairs or any form of weak supervision.)
framework utilizes learned shape features as masks which are applied to the
input itself in order to obtain clean appearance features. Without using fixed
input human skeleton, our network better preserves the conditional human
posture while requiring less supervision. Experimental results on DeepFashion
and Market1501 demonstrate that the proposed method achieves clean
disentanglement and is able to synthesis novel images of comparable quality
with state-of-the-art weakly-supervised or even supervised methods.",1764,96
['cs.CV'],Multi-View Matching Network for 6D Pose Estimation,"Applications that interact with the real world such as augmented reality or
robot manipulation require a good understanding of the location and pose of the
surrounding objects. In this paper, we present a new approach to estimate the 6
Degree of Freedom (DoF) or 6D pose of objects from a single RGB image. Our
approach can be paired with an object detection and segmentation method to
estimate, refine and track the pose of the objects by matching the input image
with rendered images.",486,50
['cs.LG'],Constrained Policy Optimization,"For many applications of reinforcement learning it can be more convenient to
specify both a reward function and constraints, rather than trying to design
behavior through the reward function. For example, systems that physically
interact with or around humans should satisfy safety constraints. Recent
advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015,
Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in
high-dimensional control, but do not consider the constrained setting.
  We propose Constrained Policy Optimization (CPO), the first general-purpose
policy search algorithm for constrained reinforcement learning with guarantees
for near-constraint satisfaction at each iteration. Our method allows us to
train neural network policies for high-dimensional control while making
guarantees about policy behavior all throughout training. Our guarantees are
based on a new theoretical result, which is of independent interest: we prove a
bound relating the expected returns of two policies to an average divergence
between them. We demonstrate the effectiveness of our approach on simulated
robot locomotion tasks where the agent must satisfy constraints motivated by
safety.",1230,31
['cs.CV'],Assisting Scene Graph Generation with Self-Supervision,"Research in scene graph generation has quickly gained traction in the past
few years because of its potential to help in downstream tasks like visual
question answering, image captioning, etc. Many interesting approaches have
been proposed to tackle this problem. Most of these works have a pre-trained
object detection model as a preliminary feature extractor. Therefore, getting
object bounding box proposals from the object detection model is relatively
cheaper. We take advantage of this ready availability of bounding box
annotations produced by the pre-trained detector. We propose a set of three
novel yet simple self-supervision tasks and train them as auxiliary multi-tasks
to the main model. While comparing, we train the base-model from scratch with
these self-supervision tasks, we achieve state-of-the-art results in all the
metrics and recall settings. We also resolve some of the confusion between two
types of relationships: geometric and possessive, by training the model with
the proposed self-supervision losses. We use the benchmark dataset, Visual
Genome to conduct our experiments and show our results.",1124,54
['cs.CV'],Object-centric Sampling for Fine-grained Image Classification,"This paper proposes to go beyond the state-of-the-art deep convolutional
neural network (CNN) by incorporating the information from object detection,
focusing on dealing with fine-grained image classification. Unfortunately, CNN
suffers from over-fiting when it is trained on existing fine-grained image
classification benchmarks, which typically only consist of less than a few tens
of thousands training images. Therefore, we first construct a large-scale
fine-grained car recognition dataset that consists of 333 car classes with more
than 150 thousand training images. With this large-scale dataset, we are able
to build a strong baseline for CNN with top-1 classification accuracy of 81.6%.
One major challenge in fine-grained image classification is that many classes
are very similar to each other while having large within-class variation. One
contributing factor to the within-class variation is cluttered image
background. However, the existing CNN training takes uniform window sampling
over the image, acting as blind on the location of the object of interest. In
contrast, this paper proposes an \emph{object-centric sampling} (OCS) scheme
that samples image windows based on the object location information. The
challenge in using the location information lies in how to design powerful
object detector and how to handle the imperfectness of detection results. To
that end, we design a saliency-aware object detection approach specific for the
setting of fine-grained image classification, and the uncertainty of detection
results are naturally handled in our OCS scheme. Our framework is demonstrated
to be very effective, improving top-1 accuracy to 89.3% (from 81.6%) on the
large-scale fine-grained car classification dataset.",1744,61
"['cs.LG', 'cs.AI', 'cs.CV']",Game-theoretic Understanding of Adversarially Learned Features,"This paper aims to understand adversarial attacks and defense from a new
perspecitve, i.e., the signal-processing behavior of DNNs. We novelly define
the multi-order interaction in game theory, which satisfies six properties.
With the multi-order interaction, we discover that adversarial attacks mainly
affect high-order interactions to fool the DNN. Furthermore, we find that the
robustness of adversarially trained DNNs comes from category-specific low-order
interactions. Our findings provide more insights into and make a revision of
previous understanding for the shape bias of adversarially learned features.
Besides, the multi-order interaction can also explain the recoverability of
adversarial examples.",713,62
"['cs.LG', 'cs.AI']",Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration,"Intrinsically motivated goal exploration algorithms enable machines to
discover repertoires of policies that produce a diversity of effects in complex
environments. These exploration algorithms have been shown to allow real world
robots to acquire skills such as tool use in high-dimensional continuous state
and action spaces. However, they have so far assumed that self-generated goals
are sampled in a specifically engineered feature space, limiting their
autonomy. In this work, we propose to use deep representation learning
algorithms to learn an adequate goal space. This is a developmental 2-stage
approach: first, in a perceptual learning stage, deep learning algorithms use
passive raw sensor observations of world changes to learn a corresponding
latent space; then goal exploration happens in a second stage by sampling goals
in this latent space. We present experiments where a simulated robot arm
interacts with an object, and we show that exploration algorithms using such
learned representations can match the performance obtained using engineered
representations.",1080,81
"['cs.LG', 'stat.ML']",Efficient Model-Free Reinforcement Learning Using Gaussian Process,"Efficient Reinforcement Learning usually takes advantage of demonstration or
good exploration strategy. By applying posterior sampling in model-free RL
under the hypothesis of GP, we propose Gaussian Process Posterior Sampling
Reinforcement Learning(GPPSTD) algorithm in continuous state space, giving
theoretical justifications and empirical results. We also provide theoretical
and empirical results that various demonstration could lower expected
uncertainty and benefit posterior sampling exploration. In this way, we
combined the demonstration and exploration process together to achieve a more
efficient reinforcement learning.",633,66
"['cs.LG', 'stat.ML']",Intelligent Condition Based Monitoring Techniques for Bearing Fault Diagnosis,"In recent years, intelligent condition-based monitor-ing of rotary machinery
systems has become a major researchfocus of machine fault diagnosis. In
condition-based monitoring,it is challenging to form a large-scale
well-annotated datasetdue to the expense of data acquisition and costly
annotation.The generated data have a large number of redundant featureswhich
degraded the performance of the machine learning models.To overcome this, we
have utilized the advantages of minimumredundancy maximum relevance (mRMR) and
transfer learningwith a deep learning model. In this work,mRMRis combinedwith
deep learning and deep transfer learning framework toimprove the fault
diagnostics performance in terms of accuracyand computational complexity.
ThemRMRreduces the redundantinformation from data and increases the deep
learning perfor-mance, whereas transfer learning, reduces a large amount of
datadependency for training the model. In the proposed work, twoframeworks,
i.e.,mRMRwith deep learning andmRMRwith deeptransfer learning, have explored
and validated on CWRU andIMS rolling element bearings datasets. The analysis
shows thatthe proposed frameworks can obtain better diagnostic accuracycompared
to existing methods and can handle the data with alarge number of features more
quickly.",1291,77
"['cs.CV', 'cs.LG', 'cs.MA', 'cs.MM', 'cs.RO']",Graph Neural Networks for 3D Multi-Object Tracking,"3D Multi-object tracking (MOT) is crucial to autonomous systems. Recent work
often uses a tracking-by-detection pipeline, where the feature of each object
is extracted independently to compute an affinity matrix. Then, the affinity
matrix is passed to the Hungarian algorithm for data association. A key process
of this pipeline is to learn discriminative features for different objects in
order to reduce confusion during data association. To that end, we propose two
innovative techniques: (1) instead of obtaining the features for each object
independently, we propose a novel feature interaction mechanism by introducing
Graph Neural Networks; (2) instead of obtaining the features from either 2D or
3D space as in prior work, we propose a novel joint feature extractor to learn
appearance and motion features from 2D and 3D space. Through experiments on the
KITTI dataset, our proposed method achieves state-of-the-art 3D MOT
performance. Our project website is at
http://www.xinshuoweng.com/projects/GNN3DMOT.",1015,50
['cs.CV'],Neural Design Network: Graphic Layout Generation with Constraints,"Graphic design is essential for visual communication with layouts being
fundamental to composing attractive designs. Layout generation differs from
pixel-level image synthesis and is unique in terms of the requirement of mutual
relations among the desired components. We propose a method for design layout
generation that can satisfy user-specified constraints. The proposed neural
design network (NDN) consists of three modules. The first module predicts a
graph with complete relations from a graph with user-specified relations. The
second module generates a layout from the predicted graph. Finally, the third
module fine-tunes the predicted layout. Quantitative and qualitative
experiments demonstrate that the generated layouts are visually similar to real
design layouts. We also construct real designs based on predicted layouts for a
better understanding of the visual quality. Finally, we demonstrate a practical
application on layout recommendation.",960,65
['cs.CV'],G-TAD: Sub-Graph Localization for Temporal Action Detection,"Temporal action detection is a fundamental yet challenging task in video
understanding. Video context is a critical cue to effectively detect actions,
but current works mainly focus on temporal context, while neglecting semantic
context as well as other important context properties. In this work, we propose
a graph convolutional network (GCN) model to adaptively incorporate multi-level
semantic context into video features and cast temporal action detection as a
sub-graph localization problem. Specifically, we formulate video snippets as
graph nodes, snippet-snippet correlations as edges, and actions associated with
context as target sub-graphs. With graph convolution as the basic operation, we
design a GCN block called GCNeXt, which learns the features of each node by
aggregating its context and dynamically updates the edges in the graph. To
localize each sub-graph, we also design an SGAlign layer to embed each
sub-graph into the Euclidean space. Extensive experiments show that G-TAD is
capable of finding effective video context without extra supervision and
achieves state-of-the-art performance on two detection benchmarks. On
ActivityNet-1.3, it obtains an average mAP of 34.09%; on THUMOS14, it reaches
51.6% at IoU@0.5 when combined with a proposal processing method. G-TAD code is
publicly available at https://github.com/frostinassiky/gtad.",1363,59
['cs.CV'],Transforming Multi-Concept Attention into Video Summarization,"Video summarization is among challenging tasks in computer vision, which aims
at identifying highlight frames or shots over a lengthy video input. In this
paper, we propose an novel attention-based framework for video summarization
with complex video data. Unlike previous works which only apply attention
mechanism on the correspondence between frames, our multi-concept video
self-attention (MC-VSA) model is presented to identify informative regions
across temporal and concept video features, which jointly exploit context
diversity over time and space for summarization purposes. Together with
consistency between video and summary enforced in our framework, our model can
be applied to both labeled and unlabeled data, making our method preferable to
real-world applications. Extensive and complete experiments on two benchmarks
demonstrate the effectiveness of our model both quantitatively and
qualitatively, and confirms its superiority over the stateof-the-arts.",972,61
"['cs.CV', 'cs.LG']",An Improved Self-supervised GAN via Adversarial Training,"We propose to improve unconditional Generative Adversarial Networks (GAN) by
training the self-supervised learning with the adversarial process. In
particular, we apply self-supervised learning via the geometric transformation
on input images and assign the pseudo-labels to these transformed images. (i)
In addition to the GAN task, which distinguishes data (real) versus generated
(fake) samples, we train the discriminator to predict the correct pseudo-labels
of real transformed samples (classification task). Importantly, we find out
that simultaneously training the discriminator to classify the fake class from
the pseudo-classes of real samples for the classification task will improve the
discriminator and subsequently lead better guides to train generator. (ii) The
generator is trained by attempting to confuse the discriminator for not only
the GAN task but also the classification task. For the classification task, the
generator tries to confuse the discriminator recognizing the transformation of
its output as one of the real transformed classes. Especially, we exploit that
when the generator creates samples that result in a similar loss (via
cross-entropy) as that of the real ones, the training is more stable and the
generator distribution tends to match better the data distribution. When
integrating our techniques into a state-of-the-art Auto-Encoder (AE) based-GAN
model, they help to significantly boost the model's performance and also
establish new state-of-the-art Fr\'echet Inception Distance (FID) scores in the
literature of unconditional GAN for CIFAR-10 and STL-10 datasets.",1609,56
"['cs.LG', 'cs.CR', 'cs.NE', 'cs.NI', 'stat.ML']",A Compendium on Network and Host based Intrusion Detection Systems,"The techniques of deep learning have become the state of the art methodology
for executing complicated tasks from various domains of computer vision,
natural language processing, and several other areas. Due to its rapid
development and promising benchmarks in those fields, researchers started
experimenting with this technique to perform in the area of, especially in
intrusion detection related tasks. Deep learning is a subset and a natural
extension of classical Machine learning and an evolved model of neural
networks. This paper contemplates and discusses all the methodologies related
to the leading edge Deep learning and Neural network models purposing to the
arena of Intrusion Detection Systems.",708,66
"['cs.LG', 'cs.AI']",Representation based and Attention augmented Meta learning,"Deep learning based computer vision fails to work when labeled images are
scarce. Recently, Meta learning algorithm has been confirmed as a promising way
to improve the ability of learning from few images for computer vision.
However, previous Meta learning approaches expose problems:
  1) they ignored the importance of attention mechanism for the Meta learner;
  2) they didn't give the Meta learner the ability of well using the past
knowledge which can help to express images into high representations, resulting
in that the Meta learner has to solve few shot learning task directly from the
original high dimensional RGB images.
  In this paper, we argue that the attention mechanism and the past knowledge
are crucial for the Meta learner, and the Meta learner should be trained on
high representations of the RGB images instead of directly on the original
ones. Based on these arguments, we propose two methods: Attention augmented
Meta Learning (AML) and Representation based and Attention augmented Meta
Learning(RAML). The method AML aims to improve the Meta learner's attention
ability by explicitly embedding an attention model into its network. The method
RAML aims to give the Meta learner the ability of leveraging the past learned
knowledge to reduce the dimension of the original input data by expressing it
into high representations, and help the Meta learner to perform well. Extensive
experiments demonstrate the effectiveness of the proposed models, with
state-of-the-art few shot learning performances on several few shot learning
benchmarks. The source code of our proposed methods will be released soon to
facilitate further studies on those aforementioned problem.",1690,58
"['cs.CV', 'cs.AI']",An Efficient Method for the Classification of Croplands in Scarce-Label Regions,"Two of the main challenges for cropland classification by satellite
time-series images are insufficient ground-truth data and inaccessibility of
high-quality hyperspectral images for under-developed areas. Unlabeled
medium-resolution satellite images are abundant, but how to benefit from them
is an open question. We will show how to leverage their potential for cropland
classification using self-supervised tasks. Self-supervision is an approach
where we provide simple training signals for the samples, which are apparent
from the data's structure. Hence, they are cheap to acquire and explain a
simple concept about the data. We introduce three self-supervised tasks for
cropland classification. They reduce epistemic uncertainty, and the resulting
model shows superior accuracy in a wide range of settings compared to SVM and
Random Forest. Subsequently, we use the self-supervised tasks to perform
unsupervised domain adaptation and benefit from the labeled samples in other
regions. It is crucial to know what information to transfer to avoid degrading
the performance. We show how to automate the information selection and transfer
process in cropland classification even when the source and target areas have a
very different feature distribution. We improved the model by about 24%
compared to a baseline architecture without any labeled sample in the target
domain. Our method is amenable to gradual improvement, works with
medium-resolution satellite images, and does not require complicated models.
Code and data are available.",1541,79
['cs.CV'],Trait of Gait: A Survey on Gait Biometrics,"Gait analysis is the study of the systematic methods that assess and quantify
animal locomotion. The research on gait analysis has considerably evolved
through time. It was an ancient art, and it still finds its application today
in modern science and medicine. This paper describes how one's gait can be used
as a biometric. It shall diversely cover salient research done within the field
and explain the nuances and advances in each type of gait analysis. The
prominent methods of gait recognition from the early era to the state of the
art are covered. This survey also reviews the various gait datasets. The
overall aim of this study is to provide a concise roadmap for anyone who wishes
to do research in the field of gait biometrics.",739,42
"['cs.LG', 'cs.AI', 'stat.ML']",Iterative Policy-Space Expansion in Reinforcement Learning,"Humans and animals solve a difficult problem much more easily when they are
presented with a sequence of problems that starts simple and slowly increases
in difficulty. We explore this idea in the context of reinforcement learning.
Rather than providing the agent with an externally provided curriculum of
progressively more difficult tasks, the agent solves a single task utilizing a
decreasingly constrained policy space. The algorithm we propose first learns to
categorize features into positive and negative before gradually learning a more
refined policy. Experimental results in Tetris demonstrate superior learning
rate of our approach when compared to existing algorithms.",680,58
['cs.CV'],QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection,"While general object detection with deep learning has achieved great success
in the past few years, the performance and efficiency of detecting small
objects are far from satisfactory. The most common and effective way to promote
small object detection is to use high-resolution images or feature maps.
However, both approaches induce costly computation since the computational cost
grows squarely as the size of images and features increases. To get the best of
two worlds, we propose QueryDet that uses a novel query mechanism to accelerate
the inference speed of feature-pyramid based object detectors. The pipeline
composes two steps: it first predicts the coarse locations of small objects on
low-resolution features and then computes the accurate detection results using
high-resolution features sparsely guided by those coarse positions. In this
way, we can not only harvest the benefit of high-resolution feature maps but
also avoid useless computation for the background area. On the popular COCO
dataset, the proposed method improves the detection mAP by 1.0 and mAP-small by
2.0, and the high-resolution inference speed is improved to 3.0x on average. On
VisDrone dataset, which contains more small objects, we create a new
state-of-the-art while gaining a 2.3x high-resolution acceleration on average.
Code is available at: https://github.com/ChenhongyiYang/QueryDet-PyTorch",1386,87
"['cs.CV', 'cs.RO']",KITTI-CARLA: a KITTI-like dataset generated by CARLA Simulator,"KITTI-CARLA is a dataset built from the CARLA v0.9.10 simulator using a
vehicle with sensors identical to the KITTI dataset. The vehicle thus has a
Velodyne HDL64 LiDAR positioned in the middle of the roof and two color cameras
similar to Point Grey Flea 2. The positions of the LiDAR and cameras are the
same as the setup used in KITTI. The objective of this dataset is to test
approaches of semantic segmentation LiDAR and/or images, odometry LiDAR and/or
image in synthetic data and to compare with the results obtained on real data
like KITTI. This dataset thus makes it possible to improve transfer learning
methods from a synthetic dataset to a real dataset. We created 7 sequences with
5000 frames in each sequence in the 7 maps of CARLA providing different
environments (city, suburban area, mountain, rural area, highway...). The
dataset is available at: http://npm3d.fr",879,62
"['stat.ML', 'cs.AI', 'cs.LG']",Markov Decision Processes with Continuous Side Information,"We consider a reinforcement learning (RL) setting in which the agent
interacts with a sequence of episodic MDPs. At the start of each episode the
agent has access to some side-information or context that determines the
dynamics of the MDP for that episode. Our setting is motivated by applications
in healthcare where baseline measurements of a patient at the start of a
treatment episode form the context that may provide information about how the
patient might respond to treatment decisions. We propose algorithms for
learning in such Contextual Markov Decision Processes (CMDPs) under an
assumption that the unobserved MDP parameters vary smoothly with the observed
context. We also give lower and upper PAC bounds under the smoothness
assumption. Because our lower bound has an exponential dependence on the
dimension, we consider a tractable linear setting where the context is used to
create linear combinations of a finite set of MDPs. For the linear setting, we
give a PAC learning algorithm based on KWIK learning techniques.",1035,58
['cs.CV'],Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts,"The rapid progress in 3D scene understanding has come with growing demand for
data; however, collecting and annotating 3D scenes (e.g. point clouds) are
notoriously hard. For example, the number of scenes (e.g. indoor rooms) that
can be accessed and scanned might be limited; even given sufficient data,
acquiring 3D labels (e.g. instance masks) requires intensive human labor. In
this paper, we explore data-efficient learning for 3D point cloud. As a first
step towards this direction, we propose Contrastive Scene Contexts, a 3D
pre-training method that makes use of both point-level correspondences and
spatial contexts in a scene. Our method achieves state-of-the-art results on a
suite of benchmarks where training data or labels are scarce. Our study reveals
that exhaustive labelling of 3D point clouds might be unnecessary; and
remarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89%
(instance segmentation) and 96% (semantic segmentation) of the baseline
performance that uses full annotations.",1026,79
"['cs.LG', 'cs.CV', 'cs.NE']",SNN: Stacked Neural Networks,"It has been proven that transfer learning provides an easy way to achieve
state-of-the-art accuracies on several vision tasks by training a simple
classifier on top of features obtained from pre-trained neural networks. The
goal of this work is to generate better features for transfer learning from
multiple publicly available pre-trained neural networks. To this end, we
propose a novel architecture called Stacked Neural Networks which leverages the
fast training time of transfer learning while simultaneously being much more
accurate. We show that using a stacked NN architecture can result in up to 8%
improvements in accuracy over state-of-the-art techniques using only one
pre-trained network for transfer learning. A second aim of this work is to make
network fine- tuning retain the generalizability of the base network to unseen
tasks. To this end, we propose a new technique called ""joint fine-tuning"" that
is able to give accuracies comparable to finetuning the same network
individually over two datasets. We also show that a jointly finetuned network
generalizes better to unseen tasks when compared to a network finetuned over a
single task.",1157,28
['cs.CV'],Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and View Synthesis,"Recent neural rendering methods have demonstrated accurate view interpolation
by predicting volumetric density and color with a neural network. Although such
volumetric representations can be supervised on static and dynamic scenes,
existing methods implicitly bake the complete scene light transport into a
single neural network for a given scene, including surface modeling,
bidirectional scattering distribution functions, and indirect lighting effects.
In contrast to traditional rendering pipelines, this prohibits changing surface
reflectance, illumination, or composing other objects in the scene.
  In this work, we explicitly model the light transport between scene surfaces
and we rely on traditional integration schemes and the rendering equation to
reconstruct a scene. The proposed method allows BSDF recovery with unknown
light conditions and classic light transports such as pathtracing. By learning
decomposed transport with surface representations established in conventional
rendering methods, the method naturally facilitates editing shape, reflectance,
lighting and scene composition. The method outperforms NeRV for relighting
under known lighting conditions, and produces realistic reconstructions for
relit and edited scenes. We validate the proposed approach for scene editing,
relighting and reflectance estimation learned from synthetic and captured views
on a subset of NeRV's datasets.",1413,87
"['cs.LG', 'stat.ML', 'I.2.6']",A framework for reinforcement learning with autocorrelated actions,"The subject of this paper is reinforcement learning. Policies are considered
here that produce actions based on states and random elements autocorrelated in
subsequent time instants. Consequently, an agent learns from experiments that
are distributed over time and potentially give better clues to policy
improvement. Also, physical implementation of such policies, e.g. in robotics,
is less problematic, as it avoids making robots shake. This is in opposition to
most RL algorithms which add white noise to control causing unwanted shaking of
the robots. An algorithm is introduced here that approximately optimizes the
aforementioned policy. Its efficiency is verified for four simulated learning
control problems (Ant, HalfCheetah, Hopper, and Walker2D) against three other
methods (PPO, SAC, ACER). The algorithm outperforms others in three of these
problems.",863,66
"['cs.LG', 'stat.ML']",Supervised Feature Subset Selection and Feature Ranking for Multivariate Time Series without Feature Extraction,"We introduce supervised feature ranking and feature subset selection
algorithms for multivariate time series (MTS) classification. Unlike most
existing supervised/unsupervised feature selection algorithms for MTS our
techniques do not require a feature extraction step to generate a
one-dimensional feature vector from the time series. Instead it is based on
directly computing similarity between individual time series and assessing how
well the resulting cluster structure matches the labels. The techniques are
amenable to heterogeneous MTS data, where the time series measurements may have
different sampling resolutions, and to multi-modal data.",650,111
"['stat.ML', 'physics.bio-ph', 'physics.chem-ph', 'physics.comp-ph', 'q-bio.BM']",Variational Encoding of Complex Dynamics,"Often the analysis of time-dependent chemical and biophysical systems
produces high-dimensional time-series data for which it can be difficult to
interpret which individual features are most salient. While recent work from
our group and others has demonstrated the utility of time-lagged co-variate
models to study such systems, linearity assumptions can limit the compression
of inherently nonlinear dynamics into just a few characteristic components.
Recent work in the field of deep learning has led to the development of
variational autoencoders (VAE), which are able to compress complex datasets
into simpler manifolds. We present the use of a time-lagged VAE, or variational
dynamics encoder (VDE), to reduce complex, nonlinear processes to a single
embedding with high fidelity to the underlying dynamics. We demonstrate how the
VDE is able to capture nontrivial dynamics in a variety of examples, including
Brownian dynamics and atomistic protein folding. Additionally, we demonstrate a
method for analyzing the VDE model, inspired by saliency mapping, to determine
what features are selected by the VDE model to describe dynamics. The VDE
presents an important step in applying techniques from deep learning to more
accurately model and interpret complex biophysics.",1275,40
['cs.CV'],"Artist, Style And Year Classification Using Face Recognition And Clustering With Convolutional Neural Networks","Artist, year and style classification of fine-art paintings are generally
achieved using standard image classification methods, image segmentation, or
more recently, convolutional neural networks (CNNs). This works aims to use
newly developed face recognition methods such as FaceNet that use CNNs to
cluster fine-art paintings using the extracted faces in the paintings, which
are found abundantly. A dataset consisting of over 80,000 paintings from over
1000 artists is chosen, and three separate face recognition and clustering
tasks are performed. The produced clusters are analyzed by the file names of
the paintings and the clusters are named by their majority artist, year range,
and style. The clusters are further analyzed and their performance metrics are
calculated. The study shows promising results as the artist, year, and styles
are clustered with an accuracy of 58.8, 63.7, and 81.3 percent, while the
clusters have an average purity of 63.1, 72.4, and 85.9 percent.",982,110
['cs.LG'],The Sobolev Regularization Effect of Stochastic Gradient Descent,"The multiplicative structure of parameters and input data in the first layer
of neural networks is explored to build connection between the landscape of the
loss function with respect to parameters and the landscape of the model
function with respect to input data. By this connection, it is shown that flat
minima regularize the gradient of the model function, which explains the good
generalization performance of flat minima. Then, we go beyond the flatness and
consider high-order moments of the gradient noise, and show that Stochastic
Gradient Dascent (SGD) tends to impose constraints on these moments by a linear
stability analysis of SGD around global minima. Together with the
multiplicative structure, we identify the Sobolev regularization effect of SGD,
i.e. SGD regularizes the Sobolev seminorms of the model function with respect
to the input data. Finally, bounds for generalization error and adversarial
robustness are provided for solutions found by SGD under assumptions of the
data distribution.",1015,64
"['cs.LG', 'cs.CL', 'stat.ML']",Weighed Domain-Invariant Representation Learning for Cross-domain Sentiment Analysis,"Cross-domain sentiment analysis is currently a hot topic in the research and
engineering areas. One of the most popular frameworks in this field is the
domain-invariant representation learning (DIRL) paradigm, which aims to learn a
distribution-invariant feature representation across domains. However, in this
work, we find out that applying DIRL may harm domain adaptation when the label
distribution $\rm{P}(\rm{Y})$ changes across domains. To address this problem,
we propose a modification to DIRL, obtaining a novel weighted domain-invariant
representation learning (WDIRL) framework. We show that it is easy to transfer
existing SOTA DIRL models to WDIRL. Empirical studies on extensive cross-domain
sentiment analysis tasks verified our statements and showed the effectiveness
of our proposed solution.",810,84
['cs.CV'],Deep Semantics-Aware Photo Adjustment,"Automatic photo adjustment is to mimic the photo retouching style of
professional photographers and automatically adjust photos to the learned
style. There have been many attempts to model the tone and the color adjustment
globally with low-level color statistics. Also, spatially varying photo
adjustment methods have been studied by exploiting high-level features and
semantic label maps. Those methods are semantics-aware since the color mapping
is dependent on the high-level semantic context. However, their performance is
limited to the pre-computed hand-crafted features and it is hard to reflect
user's preference to the adjustment. In this paper, we propose a deep neural
network that models the semantics-aware photo adjustment. The proposed network
exploits bilinear models that are the multiplicative interaction of the color
and the contexual features. As the contextual features we propose the semantic
adjustment map, which discovers the inherent photo retouching presets that are
applied according to the scene context. The proposed method is trained using a
robust loss with a scene parsing task. The experimental results show that the
proposed method outperforms the existing method both quantitatively and
qualitatively. The proposed method also provides users a way to retouch the
photo by their own likings by giving customized adjustment maps.",1365,37
['cs.CV'],Stereo Correspondence and Reconstruction of Endoscopic Data Challenge,"The stereo correspondence and reconstruction of endoscopic data sub-challenge
was organized during the Endovis challenge at MICCAI 2019 in Shenzhen, China.
The task was to perform dense depth estimation using 7 training datasets and 2
test sets of structured light data captured using porcine cadavers. These were
provided by a team at Intuitive Surgical. 10 teams participated in the
challenge day. This paper contains 3 additional methods which were submitted
after the challenge finished as well as a supplemental section from these teams
on issues they found with the dataset.",580,69
"['cs.LG', 'cs.CV', 'stat.ML']",Improved Network Robustness with Adversary Critic,"Ideally, what confuses neural network should be confusing to humans. However,
recent experiments have shown that small, imperceptible perturbations can
change the network prediction. To address this gap in perception, we propose a
novel approach for learning robust classifier. Our main idea is: adversarial
examples for the robust classifier should be indistinguishable from the regular
data of the adversarial target. We formulate a problem of learning robust
classifier in the framework of Generative Adversarial Networks (GAN), where the
adversarial attack on classifier acts as a generator, and the critic network
learns to distinguish between regular and adversarial images. The classifier
cost is augmented with the objective that its adversarial examples should
confuse the adversary critic. To improve the stability of the adversarial
mapping, we introduce adversarial cycle-consistency constraint which ensures
that the adversarial mapping of the adversarial examples is close to the
original. In the experiments, we show the effectiveness of our defense. Our
method surpasses in terms of robustness networks trained with adversarial
training. Additionally, we verify in the experiments with human annotators on
MTurk that adversarial examples are indeed visually confusing. Codes for the
project are available at https://github.com/aam-at/adversary_critic.",1367,49
['cs.CV'],A Pretrained DenseNet Encoder for Brain Tumor Segmentation,"This article presents a convolutional neural network for the automatic
segmentation of brain tumors in multimodal 3D MR images based on a U-net
architecture.We evaluate the use of a densely connected convolutional network
encoder (DenseNet) which was pretrained on the ImageNet data set. We detail two
network architectures that can take into account multiple 3D images as inputs.
This work aims to identify if a generic pretrained network can be used for very
specific medical applications where the target data differ both in the number
of spatial dimensions as well as in the number of inputs channels. Moreover in
order to regularize this transfer learning task we only train the decoder part
of the U-net architecture. We evaluate the effectiveness of the proposed
approach on the BRATS 2018 segmentation challenge where we obtained dice scores
of 0.79, 0.90, 0.85 and 95/% Hausdorff distance of 2.9mm, 3.95mm, and 6.48mm
for enhanced tumor core, whole tumor and tumor core respectively on the
validation set. This scores degrades to 0.77, 0.88, 0.78 and 95 /% Hausdorff
distance of 3.6mm, 5.72mm, and 5.83mm on the testing set.",1133,58
"['cs.CV', 'cs.DS']",Missing Slice Recovery for Tensors Using a Low-rank Model in Embedded Space,"Let us consider a case where all of the elements in some continuous slices
are missing in tensor data.
  In this case, the nuclear-norm and total variation regularization methods
usually fail to recover the missing elements.
  The key problem is capturing some delay/shift-invariant structure.
  In this study, we consider a low-rank model in an embedded space of a tensor.
  For this purpose, we extend a delay embedding for a time series to a
""multi-way delay-embedding transform"" for a tensor, which takes a given
incomplete tensor as the input and outputs a higher-order incomplete Hankel
tensor.
  The higher-order tensor is then recovered by Tucker-based low-rank tensor
factorization.
  Finally, an estimated tensor can be obtained by using the inverse multi-way
delay embedding transform of the recovered higher-order tensor.
  Our experiments showed that the proposed method successfully recovered
missing slices for some color images and functional magnetic resonance images.",985,75
"['cs.CV', 'cs.AI']",Few-Shot Batch Incremental Road Object Detection via Detector Fusion,"Incremental few-shot learning has emerged as a new and challenging area in
deep learning, whose objective is to train deep learning models using very few
samples of new class data, and none of the old class data. In this work we
tackle the problem of batch incremental few-shot road object detection using
data from the India Driving Dataset (IDD). Our approach, DualFusion, combines
object detectors in a manner that allows us to learn to detect rare objects
with very limited data, all without severely degrading the performance of the
detector on the abundant classes. In the IDD OpenSet incremental few-shot
detection task, we achieve a mAP50 score of 40.0 on the base classes and an
overall mAP50 score of 38.8, both of which are the highest to date. In the COCO
batch incremental few-shot detection task, we achieve a novel AP score of 9.9,
surpassing the state-of-the-art novel class performance on the same by over 6.6
times.",933,68
"['stat.ML', 'cs.LG', '62M10, 62H30, 62G99']",Spectral embedding for dynamic networks with stability guarantees,"We consider the problem of embedding a dynamic network, to obtain
time-evolving vector representations of each node, which can then be used to
describe the changes in behaviour of a single node, one or more communities, or
the entire graph. Given this open-ended remit, we wish to guarantee stability
in the spatio-temporal positioning of the nodes: assigning the same position,
up to noise, to nodes behaving similarly at a given time (cross-sectional
stability) and a constant position, up to noise, to a single node behaving
similarly across different times (longitudinal stability). These properties are
defined formally within a generic dynamic latent position model. By showing how
this model can be recast as a multilayer random dot product graph, we
demonstrate that unfolded adjacency spectral embedding satisfies both stability
conditions, allowing, for example, spatio-temporal clustering under the dynamic
stochastic block model. We also show how alternative methods, such as omnibus,
independent or time-averaged spectral embedding, lack one or the other form of
stability.",1086,65
"['cs.LG', 'eess.SP', 'I.2.8']",Revisiting the Application of Feature Selection Methods to Speech Imagery BCI Datasets,"Brain-computer interface (BCI) aims to establish and improve human and
computer interactions. There has been an increasing interest in designing new
hardware devices to facilitate the collection of brain signals through various
technologies, such as wet and dry electroencephalogram (EEG) and functional
near-infrared spectroscopy (fNIRS) devices. The promising results of machine
learning methods have attracted researchers to apply these methods to their
data. However, some methods can be overlooked simply due to their inferior
performance against a particular dataset. This paper shows how relatively
simple yet powerful feature selection/ranking methods can be applied to speech
imagery datasets and generate significant results. To do so, we introduce two
approaches, horizontal and vertical settings, to use any feature selection and
ranking methods to speech imagery BCI datasets. Our primary goal is to improve
the resulting classification accuracies from support vector machines,
$k$-nearest neighbour, decision tree, linear discriminant analysis and long
short-term memory recurrent neural network classifiers. Our experimental
results show that using a small subset of channels, we can retain and, in most
cases, improve the resulting classification accuracies regardless of the
classifier.",1303,86
['cs.CV'],Towards 3D Visualization of Video from Frames,"We explain theoretically how to reconstruct the 3D scene from successive
frames in order to see the video in 3D. To do this, features, associated to
moving rigid objects in 3D, are extracted in frames and matched. The vanishing
point computed in frame corresponding to the direction of moving object is used
for 3D positioning of the 3D structure of the moving object. First experiments
are conducted and the obtained results are shown and publicly available. They
demonstrate the feasibility of our method. We conclude this paper by future
works in order to improve this method tacking into account non-rigid objects
and the case of moving camera.",648,45
['cs.CV'],A Survey on Deep Learning in Medical Image Analysis,"Deep learning algorithms, in particular convolutional networks, have rapidly
become a methodology of choice for analyzing medical images. This paper reviews
the major deep learning concepts pertinent to medical image analysis and
summarizes over 300 contributions to the field, most of which appeared in the
last year. We survey the use of deep learning for image classification, object
detection, segmentation, registration, and other tasks and provide concise
overviews of studies per application area. Open challenges and directions for
future research are discussed.",570,51
"['stat.ML', 'cs.LG']",Weight-of-evidence 2.0 with shrinkage and spline-binning,"In many practical applications, such as fraud detection, credit risk modeling
or medical decision making, classification models for assigning instances to a
predefined set of classes are required to be both precise as well as
interpretable. Linear modeling methods such as logistic regression are often
adopted, since they offer an acceptable balance between precision and
interpretability. Linear methods, however, are not well equipped to handle
categorical predictors with high-cardinality or to exploit non-linear relations
in the data. As a solution, data preprocessing methods such as
weight-of-evidence are typically used for transforming the predictors. The
binning procedure that underlies the weight-of-evidence approach, however, has
been little researched and typically relies on ad-hoc or expert driven
procedures. The objective in this paper, therefore, is to propose a formalized,
data-driven and powerful method.
  To this end, we explore the discretization of continuous variables through
the binning of spline functions, which allows for capturing non-linear effects
in the predictor variables and yields highly interpretable predictors taking
only a small number of discrete values. Moreover, we extend upon the
weight-of-evidence approach and propose to estimate the proportions using
shrinkage estimators. Together, this offers an improved ability to exploit both
non-linear and categorical predictors for achieving increased classification
precision, while maintaining interpretability of the resulting model and
decreasing the risk of overfitting.
  We present the results of a series of experiments in a fraud detection
setting, which illustrate the effectiveness of the presented approach. We
facilitate reproduction of the presented results and adoption of the proposed
approaches by providing both the dataset and the code for implementing the
experiments and the presented approach.",1910,56
"['cs.LG', 'stat.ML']",Conservative Optimistic Policy Optimization via Multiple Importance Sampling,"Reinforcement Learning (RL) has been able to solve hard problems such as
playing Atari games or solving the game of Go, with a unified approach. Yet
modern deep RL approaches are still not widely used in real-world applications.
One reason could be the lack of guarantees on the performance of the
intermediate executed policies, compared to an existing (already working)
baseline policy. In this paper, we propose an online model-free algorithm that
solves conservative exploration in the policy optimization problem. We show
that the regret of the proposed approach is bounded by
$\tilde{\mathcal{O}}(\sqrt{T})$ for both discrete and continuous parameter
spaces.",664,76
"['cs.LG', 'cs.DC', 'stat.ML']",Sparse GPU Kernels for Deep Learning,"Scientific workloads have traditionally exploited high levels of sparsity to
accelerate computation and reduce memory requirements. While deep neural
networks can be made sparse, achieving practical speedups on GPUs is difficult
because these applications have relatively moderate levels of sparsity that are
not sufficient for existing sparse kernels to outperform their dense
counterparts. In this work, we study sparse matrices from deep learning
applications and identify favorable properties that can be exploited to
accelerate computation. Based on these insights, we develop high-performance
GPU kernels for two sparse matrix operations widely applicable in neural
networks: sparse matrix-dense matrix multiplication and sampled dense-dense
matrix multiplication. Our kernels reach 27% of single-precision peak on Nvidia
V100 GPUs. Using our kernels, we demonstrate sparse Transformer and MobileNet
models that achieve 1.2-2.1x speedups and up to 12.8x memory savings without
sacrificing accuracy.",1004,36
"['cs.LG', 'cs.AI', 'cs.CV', 'cs.RO', 'cs.SY', 'eess.SY']",Hierarchical Neural Dynamic Policies,"We tackle the problem of generalization to unseen configurations for dynamic
tasks in the real world while learning from high-dimensional image input. The
family of nonlinear dynamical system-based methods have successfully
demonstrated dynamic robot behaviors but have difficulty in generalizing to
unseen configurations as well as learning from image inputs. Recent works
approach this issue by using deep network policies and reparameterize actions
to embed the structure of dynamical systems but still struggle in domains with
diverse configurations of image goals, and hence, find it difficult to
generalize. In this paper, we address this dichotomy by leveraging embedding
the structure of dynamical systems in a hierarchical deep policy learning
framework, called Hierarchical Neural Dynamical Policies (H-NDPs). Instead of
fitting deep dynamical systems to diverse data directly, H-NDPs form a
curriculum by learning local dynamical system-based policies on small regions
in state-space and then distill them into a global dynamical system-based
policy that operates only from high-dimensional images. H-NDPs additionally
provide smooth trajectories, a strong safety benefit in the real world. We
perform extensive experiments on dynamic tasks both in the real world (digit
writing, scooping, and pouring) and simulation (catching, throwing, picking).
We show that H-NDPs are easily integrated with both imitation as well as
reinforcement learning setups and achieve state-of-the-art results. Video
results are at https://shikharbahl.github.io/hierarchical-ndps/",1570,36
['cs.LG'],Using Multilevel Circulant Matrix Approximate to Speed Up Kernel Logistic Regression,"Kernel logistic regression (KLR) is a classical nonlinear classifier in
statistical machine learning. Newton method with quadratic convergence rate can
solve KLR problem more effectively than the gradient method. However, an
obvious limitation of Newton method for training large-scale problems is the
$O(n^{3})$ time complexity and $O(n^{2})$ space complexity, where $n$ is the
number of training instances. In this paper, we employ the multilevel circulant
matrix (MCM) approximate kernel matrix to save in storage space and accelerate
the solution of the KLR. Combined with the characteristics of MCM and our
ingenious design, we propose an MCM approximate Newton iterative method. We
first simplify the Newton direction according to the semi-positivity of the
kernel matrix and then perform a two-step approximation of the Newton direction
by using MCM. Our method reduces the time complexity of each iteration to $O(n
\log n)$ by using the multidimensional fast Fourier transform (mFFT). In
addition, the space complexity can be reduced to $O(n)$ due to the built-in
periodicity of MCM. Experimental results on some large-scale binary and
multi-classification problems show that our method makes KLR scalable for
large-scale problems, with less memory consumption, and converges to test
accuracy without sacrifice in a shorter time.",1337,84
"['cs.CV', 'cs.LG', 'stat.ML']",Estimating Bicycle Route Attractivity from Image Data,"This master thesis focuses on practical application of Convolutional Neural
Network models on the task of road labeling with bike attractivity score. We
start with an abstraction of real world locations into nodes and scored edges
in partially annotated dataset. We enhance information available about each
edge with photographic data from Google Street View service and with additional
neighborhood information from Open Street Map database. We teach a model on
this enhanced dataset and experiment with ImageNet Large Scale Visual
Recognition Competition. We try different dataset enhancing techniques as well
as various model architectures to improve road scoring. We also make use of
transfer learning to use features from a task with rich dataset of ImageNet
into our task with smaller number of images, to prevent model overfitting.",838,53
"['cs.LG', 'eess.SP']",Anomaly Detection in Time Series with Triadic Motif Fields and Application in Atrial Fibrillation ECG Classification,"In the time-series analysis, the time series motifs and the order patterns in
time series can reveal general temporal patterns and dynamic features. Triadic
Motif Field (TMF) is a simple and effective time-series image encoding method
based on triadic time series motifs. Electrocardiography (ECG) signals are
time-series data widely used to diagnose various cardiac anomalies. The TMF
images contain the features characterizing the normal and Atrial Fibrillation
(AF) ECG signals. Considering the quasi-periodic characteristics of ECG
signals, the dynamic features can be extracted from the TMF images with the
transfer learning pre-trained convolutional neural network (CNN) models. With
the extracted features, the simple classifiers, such as the Multi-Layer
Perceptron (MLP), the logistic regression, and the random forest, can be
applied for accurate anomaly detection. With the test dataset of the PhysioNet
Challenge 2017 database, the TMF classification model with the VGG16 transfer
learning model and MLP classifier demonstrates the best performance with the
95.50% ROC-AUC and 88.43% F1 score in the AF classification. Besides, the TMF
classification model can identify AF patients in the test dataset with high
precision. The feature vectors extracted from the TMF images show clear
patient-wise clustering with the t-distributed Stochastic Neighbor Embedding
technique. Above all, the TMF classification model has very good clinical
interpretability. The patterns revealed by symmetrized Gradient-weighted Class
Activation Mapping have a clear clinical interpretation at the beat and rhythm
levels.",1611,116
"['cs.LG', 'cs.AI']",Decision Transformer: Reinforcement Learning via Sequence Modeling,"We introduce a framework that abstracts Reinforcement Learning (RL) as a
sequence modeling problem. This allows us to draw upon the simplicity and
scalability of the Transformer architecture, and associated advances in
language modeling such as GPT-x and BERT. In particular, we present Decision
Transformer, an architecture that casts the problem of RL as conditional
sequence modeling. Unlike prior approaches to RL that fit value functions or
compute policy gradients, Decision Transformer simply outputs the optimal
actions by leveraging a causally masked Transformer. By conditioning an
autoregressive model on the desired return (reward), past states, and actions,
our Decision Transformer model can generate future actions that achieve the
desired return. Despite its simplicity, Decision Transformer matches or exceeds
the performance of state-of-the-art model-free offline RL baselines on Atari,
OpenAI Gym, and Key-to-Door tasks.",939,66
['stat.ML'],A Birth and Death Process for Bayesian Network Structure Inference,"Bayesian networks (BNs) are graphical models that are useful for representing
high-dimensional probability distributions. There has been a great deal of
interest in recent years in the NP-hard problem of learning the structure of a
BN from observed data. Typically, one assigns a score to various structures and
the search becomes an optimization problem that can be approached with either
deterministic or stochastic methods. In this paper, we walk through the space
of graphs by modeling the appearance and disappearance of edges as a birth and
death process and compare our novel approach to the popular Metropolis-Hastings
search strategy. We give empirical evidence that the birth and death process
has superior mixing properties.",735,66
['cs.CV'],Unpaired Image Captioning by Language Pivoting,"Image captioning is a multimodal task involving computer vision and natural
language processing, where the goal is to learn a mapping from the image to its
natural language description. In general, the mapping function is learned from
a training set of image-caption pairs. However, for some language, large scale
image-caption paired corpus might not be available. We present an approach to
this unpaired image captioning problem by language pivoting. Our method can
effectively capture the characteristics of an image captioner from the pivot
language (Chinese) and align it to the target language (English) using another
pivot-target (Chinese-English) sentence parallel corpus. We evaluate our method
on two image-to-English benchmark datasets: MSCOCO and Flickr30K. Quantitative
comparisons against several baseline approaches demonstrate the effectiveness
of our method.",875,46
['cs.CV'],Geometric Multi-Model Fitting with a Convex Relaxation Algorithm,"We propose a novel method to fit and segment multi-structural data via convex
relaxation. Unlike greedy methods --which maximise the number of inliers-- this
approach efficiently searches for a soft assignment of points to models by
minimising the energy of the overall classification. Our approach is similar to
state-of-the-art energy minimisation techniques which use a global energy.
However, we deal with the scaling factor (as the number of models increases) of
the original combinatorial problem by relaxing the solution. This relaxation
brings two advantages: first, by operating in the continuous domain we can
parallelize the calculations. Second, it allows for the use of different
metrics which results in a more general formulation.
  We demonstrate the versatility of our technique on two different problems of
estimating structure from images: plane extraction from RGB-D data and
homography estimation from pairs of images. In both cases, we report accurate
results on publicly available datasets, in most of the cases outperforming the
state-of-the-art.",1070,64
['cs.LG'],Adaptive Explainable Continual Learning Framework for Regression Problems with Focus on Power Forecasts,"Compared with traditional deep learning techniques, continual learning
enables deep neural networks to learn continually and adaptively. Deep neural
networks have to learn new tasks and overcome forgetting the knowledge obtained
from the old tasks as the amount of data keeps increasing in applications. In
this article, two continual learning scenarios will be proposed to describe the
potential challenges in this context. Besides, based on our previous work
regarding the CLeaR framework, which is short for continual learning for
regression tasks, the work will be further developed to enable models to extend
themselves and learn data successively. Research topics are related but not
limited to developing continual deep learning algorithms, strategies for
non-stationarity detection in data streams, explainable and visualizable
artificial intelligence, etc. Moreover, the framework- and algorithm-related
hyperparameters should be dynamically updated in applications. Forecasting
experiments will be conducted based on power generation and consumption data
collected from real-world applications. A series of comprehensive evaluation
metrics and visualization tools can help analyze the experimental results. The
proposed framework is expected to be generally applied to other constantly
changing scenarios.",1315,103
"['cs.CV', 'cs.HC', 'cs.LG', 'cs.SE']",Object Detection for Graphical User Interface: Old Fashioned or Deep Learning or a Combination?,"Detecting Graphical User Interface (GUI) elements in GUI images is a
domain-specific object detection task. It supports many software engineering
tasks, such as GUI animation and testing, GUI search and code generation.
Existing studies for GUI element detection directly borrow the mature methods
from computer vision (CV) domain, including old fashioned ones that rely on
traditional image processing features (e.g., canny edge, contours), and deep
learning models that learn to detect from large-scale GUI data. Unfortunately,
these CV methods are not originally designed with the awareness of the unique
characteristics of GUIs and GUI elements and the high localization accuracy of
the GUI element detection task. We conduct the first large-scale empirical
study of seven representative GUI element detection methods on over 50k GUI
images to understand the capabilities, limitations and effective designs of
these methods. This study not only sheds the light on the technical challenges
to be addressed but also informs the design of new GUI element detection
methods. We accordingly design a new GUI-specific old-fashioned method for
non-text GUI element detection which adopts a novel top-down coarse-to-fine
strategy, and incorporate it with the mature deep learning model for GUI text
detection.Our evaluation on 25,000 GUI images shows that our method
significantly advances the start-of-the-art performance in GUI element
detection.",1444,95
"['cs.LG', 'cs.IR']","An Analytical Study on Behavior of Clusters Using K Means, EM and K* Means Algorithm","Clustering is an unsupervised learning method that constitutes a cornerstone
of an intelligent data analysis process. It is used for the exploration of
inter-relationships among a collection of patterns, by organizing them into
homogeneous clusters. Clustering has been dynamically applied to a variety of
tasks in the field of Information Retrieval (IR). Clustering has become one of
the most active area of research and the development. Clustering attempts to
discover the set of consequential groups where those within each group are more
closely related to one another than the others assigned to different groups.
The resultant clusters can provide a structure for organizing large bodies of
text for efficient browsing and searching. There exists a wide variety of
clustering algorithms that has been intensively studied in the clustering
problem. Among the algorithms that remain the most common and effectual, the
iterative optimization clustering algorithms have been demonstrated reasonable
performance for clustering, e.g. the Expectation Maximization (EM) algorithm
and its variants, and the well known k-means algorithm. This paper presents an
analysis on how partition method clustering techniques - EM, K -means and K*
Means algorithm work on heartspect dataset with below mentioned features -
Purity, Entropy, CPU time, Cluster wise analysis, Mean value analysis and inter
cluster distance. Thus the paper finally provides the experimental results of
datasets for five clusters to strengthen the results that the quality of the
behavior in clusters in EM algorithm is far better than k-means algorithm and
k*means algorithm.",1640,84
"['cs.LG', 'stat.ML']",Modeling Customer Engagement from Partial Observations,"It is of high interest for a company to identify customers expected to bring
the largest profit in the upcoming period. Knowing as much as possible about
each customer is crucial for such predictions. However, their demographic data,
preferences, and other information that might be useful for building loyalty
programs is often missing. Additionally, modeling relations among different
customers as a network can be beneficial for predictions at an individual
level, as similar customers tend to have similar purchasing patterns. We
address this problem by proposing a robust framework for structured regression
on deficient data in evolving networks with a supervised representation
learning based on neural features embedding. The new method is compared to
several unstructured and structured alternatives for predicting customer
behavior (e.g. purchasing frequency and customer ticket) on user networks
generated from customer databases of two companies from different industries.
The obtained results show $4\%$ to $130\%$ improvement in accuracy over
alternatives when all customer information is known. Additionally, the
robustness of our method is demonstrated when up to $80\%$ of demographic
information was missing where it was up to several folds more accurate as
compared to alternatives that are either ignoring cases with missing values or
learn their feature representation in an unsupervised manner.",1416,54
"['cs.CV', 'cs.LG', 'stat.ML']","Learning Graphical Models of Images, Videos and Their Spatial Transformations","Mixtures of Gaussians, factor analyzers (probabilistic PCA) and hidden Markov
models are staples of static and dynamic data modeling and image and video
modeling in particular. We show how topographic transformations in the input,
such as translation and shearing in images, can be accounted for in these
models by including a discrete transformation variable. The resulting models
perform clustering, dimensionality reduction and time-series analysis in a way
that is invariant to transformations in the input. Using the EM algorithm,
these transformation-invariant models can be fit to static data and time
series. We give results on filtering microscopy images, face and facial pose
clustering, handwritten digit modeling and recognition, video clustering,
object tracking, and removal of distractions from video sequences.",826,77
['cs.LG'],BiTe-GCN: A New GCN Architecture via BidirectionalConvolution of Topology and Features on Text-Rich Networks,"Graph convolutional networks (GCNs), aiming to integrate high-order
neighborhood information through stacked graph convolution layers, have
demonstrated remarkable power in many network analysis tasks. However,
topological limitations, including over-smoothing and local topology homophily,
limit its capability to represent networks. Existing studies only perform
feature convolution on network topology, which inevitably introduces unbalance
between topology and features. Considering that in real world, the information
network consists of not only the node-level citation information but also the
local text-sequence information. We propose BiTe-GCN, a novel GCN architecture
with bidirectional convolution of both topology and features on text-rich
networks to solve these limitations. We first transform the original text-rich
network into an augmented bi-typed heterogeneous network, capturing both the
global node-level information and the local text-sequence information from
texts. We then introduce discriminative convolution mechanisms to performs
convolutions of both topology and features simultaneously. Extensive
experiments on text-rich networks demonstrate that our new architecture
outperforms state-of-the-art by a breakout improvement. Moreover, this
architecture can also be applied to several e-commerce searching scenes such as
JD searching. The experiments on the JD dataset validate the superiority of the
proposed architecture over the related methods.",1479,108
"['cs.LG', 'stat.ML']",PushNet: Efficient and Adaptive Neural Message Passing,"Message passing neural networks have recently evolved into a state-of-the-art
approach to representation learning on graphs. Existing methods perform
synchronous message passing along all edges in multiple subsequent rounds and
consequently suffer from various shortcomings: Propagation schemes are
inflexible since they are restricted to $k$-hop neighborhoods and insensitive
to actual demands of information propagation. Further, long-range dependencies
cannot be modeled adequately and learned representations are based on
correlations of fixed locality. These issues prevent existing methods from
reaching their full potential in terms of prediction performance. Instead, we
consider a novel asynchronous message passing approach where information is
pushed only along the most relevant edges until convergence. Our proposed
algorithm can equivalently be formulated as a single synchronous message
passing iteration using a suitable neighborhood function, thus sharing the
advantages of existing methods while addressing their central issues. The
resulting neural network utilizes a node-adaptive receptive field derived from
meaningful sparse node neighborhoods. In addition, by learning and combining
node representations over differently sized neighborhoods, our model is able to
capture correlations on multiple scales. We further propose variants of our
base model with different inductive bias. Empirical results are provided for
semi-supervised node classification on five real-world datasets following a
rigorous evaluation protocol. We find that our models outperform competitors on
all datasets in terms of accuracy with statistical significance. In some cases,
our models additionally provide faster runtime.",1723,54
['cs.LG'],Simple Deep Random Model Ensemble,"Representation learning and unsupervised learning are two central topics of
machine learning and signal processing. Deep learning is one of the most
effective unsupervised representation learning approach. The main contributions
of this paper to the topics are as follows. (i) We propose to view the
representative deep learning approaches as special cases of the knowledge reuse
framework of clustering ensemble. (ii) We propose to view sparse coding when
used as a feature encoder as the consensus function of clustering ensemble, and
view dictionary learning as the training process of the base clusterings of
clustering ensemble. (ii) Based on the above two views, we propose a very
simple deep learning algorithm, named deep random model ensemble (DRME). It is
a stack of random model ensembles. Each random model ensemble is a special
k-means ensemble that discards the expectation-maximization optimization of
each base k-means but only preserves the default initialization method of the
base k-means. (iv) We propose to select the most powerful representation among
the layers by applying DRME to clustering where the single-linkage is used as
the clustering algorithm. Moreover, the DRME based clustering can also detect
the number of the natural clusters accurately. Extensive experimental
comparisons with 5 representation learning methods on 19 benchmark data sets
demonstrate the effectiveness of DRME.",1415,33
"['cs.CV', 'cs.LG', 'eess.IV']",Object-Centric Image Generation from Layouts,"Despite recent impressive results on single-object and single-domain image
generation, the generation of complex scenes with multiple objects remains
challenging. In this paper, we start with the idea that a model must be able to
understand individual objects and relationships between objects in order to
generate complex scenes well. Our layout-to-image-generation method, which we
call Object-Centric Generative Adversarial Network (or OC-GAN), relies on a
novel Scene-Graph Similarity Module (SGSM). The SGSM learns representations of
the spatial relationships between objects in the scene, which lead to our
model's improved layout-fidelity. We also propose changes to the conditioning
mechanism of the generator that enhance its object instance-awareness. Apart
from improving image quality, our contributions mitigate two failure modes in
previous approaches: (1) spurious objects being generated without corresponding
bounding boxes in the layout, and (2) overlapping bounding boxes in the layout
leading to merged objects in images. Extensive quantitative evaluation and
ablation studies demonstrate the impact of our contributions, with our model
outperforming previous state-of-the-art approaches on both the COCO-Stuff and
Visual Genome datasets. Finally, we address an important limitation of
evaluation metrics used in previous works by introducing SceneFID -- an
object-centric adaptation of the popular Fr{\'e}chet Inception Distance metric,
that is better suited for multi-object images.",1504,44
['cs.CV'],Attentive Action and Context Factorization,"We propose a method for human action recognition, one that can localize the
spatiotemporal regions that `define' the actions. This is a challenging task
due to the subtlety of human actions in video and the co-occurrence of
contextual elements. To address this challenge, we utilize conjugate samples of
human actions, which are video clips that are contextually similar to human
action samples but do not contain the action. We introduce a novel attentional
mechanism that can spatially and temporally separate human actions from the
co-occurring contextual factors. The separation of the action and context
factors is weakly supervised, eliminating the need for laboriously detailed
annotation of these two factors in training samples. Our method can be used to
build human action classifiers with higher accuracy and better
interpretability. Experiments on several human action recognition datasets
demonstrate the quantitative and qualitative benefits of our approach.",972,42
"['cs.LG', 'cs.AI', 'cs.RO']",Transferring Autonomous Driving Knowledge on Simulated and Real Intersections,"We view intersection handling on autonomous vehicles as a reinforcement
learning problem, and study its behavior in a transfer learning setting. We
show that a network trained on one type of intersection generally is not able
to generalize to other intersections. However, a network that is pre-trained on
one intersection and fine-tuned on another performs better on the new task
compared to training in isolation. This network also retains knowledge of the
prior task, even though some forgetting occurs. Finally, we show that the
benefits of fine-tuning hold when transferring simulated intersection handling
knowledge to a real autonomous vehicle.",651,77
"['cs.LG', 'cs.CL']",Large-scale Simple Question Answering with Memory Networks,"Training large-scale question answering systems is complicated because
training sources usually cover a small portion of the range of possible
questions. This paper studies the impact of multitask and transfer learning for
simple question answering; a setting for which the reasoning required to answer
is quite easy, as long as one can retrieve the correct evidence given a
question, which can be difficult in large-scale conditions. To this end, we
introduce a new dataset of 100k questions that we use in conjunction with
existing benchmarks. We conduct our study within the framework of Memory
Networks (Weston et al., 2015) because this perspective allows us to eventually
scale up to more complex reasoning, and show that Memory Networks can be
successfully trained to achieve excellent performance.",805,58
"['cs.CV', 'cs.LG']",Kronecker Attention Networks,"Attention operators have been applied on both 1-D data like texts and
higher-order data such as images and videos. Use of attention operators on
high-order data requires flattening of the spatial or spatial-temporal
dimensions into a vector, which is assumed to follow a multivariate normal
distribution. This not only incurs excessive requirements on computational
resources, but also fails to preserve structures in data. In this work, we
propose to avoid flattening by assuming the data follow matrix-variate normal
distributions. Based on this new view, we develop Kronecker attention operators
(KAOs) that operate on high-order tensor data directly. More importantly, the
proposed KAOs lead to dramatic reductions in computational resources.
Experimental results show that our methods reduce the amount of required
computational resources by a factor of hundreds, with larger factors for
higher-dimensional and higher-order data. Results also show that networks with
KAOs outperform models without attention, while achieving competitive
performance as those with original attention operators.",1097,28
['cs.CV'],Real-Time Anomaly Detection With HMOF Feature,"Anomaly detection is a challenging problem in intelligent video surveillance.
Most existing methods are computation consuming, which cannot satisfy the
real-time requirement. In this paper, we propose a real-time anomaly detection
framework with low computational complexity and high efficiency. A new feature,
named Histogram of Magnitude Optical Flow (HMOF), is proposed to capture the
motion of video patches. Compared with existing feature descriptors, HMOF is
more sensitive to motion magnitude and more efficient to distinguish anomaly
information. The HMOF features are computed for foreground patches, and are
reconstructed by the auto-encoder for better clustering. Then, we use Gaussian
Mixture Model (GMM) Classifiers to distinguish anomalies from normal activities
in videos. Experimental results show that our framework outperforms
state-of-the-art methods, and can reliably detect anomalies in real-time.",918,45
"['cs.LG', 'cs.CV', 'eess.IV']",GMM-Based Generative Adversarial Encoder Learning,"While GAN is a powerful model for generating images, its inability to infer a
latent space directly limits its use in applications requiring an encoder. Our
paper presents a simple architectural setup that combines the generative
capabilities of GAN with an encoder. We accomplish this by combining the
encoder with the discriminator using shared weights, then training them
simultaneously using a new loss term. We model the output of the encoder latent
space via a GMM, which leads to both good clustering using this latent space
and improved image generation by the GAN. Our framework is generic and can be
easily plugged into any GAN strategy. In particular, we demonstrate it both
with Vanilla GAN and Wasserstein GAN, where in both it leads to an improvement
in the generated images in terms of both the IS and FID scores. Moreover, we
show that our encoder learns a meaningful representation as its clustering
results are competitive with the current GAN-based state-of-the-art in
clustering.",999,49
"['cs.CV', 'cs.AI', 'cs.LG', 'cs.MA', 'cs.RO']",RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting,"Motion forecasting plays a significant role in various domains (e.g.,
autonomous driving, human-robot interaction), which aims to predict future
motion sequences given a set of historical observations. However, the observed
elements may be of different levels of importance. Some information may be
irrelevant or even distracting to the forecasting in certain situations. To
address this issue, we propose a generic motion forecasting framework (named
RAIN) with dynamic key information selection and ranking based on a hybrid
attention mechanism. The general framework is instantiated to handle
multi-agent trajectory prediction and human motion forecasting tasks,
respectively. In the former task, the model learns to recognize the relations
between agents with a graph representation and to determine their relative
significance. In the latter task, the model learns to capture the temporal
proximity and dependency in long-term human motions. We also propose an
effective double-stage training pipeline with an alternating training strategy
to optimize the parameters in different modules of the framework. We validate
the framework on both synthetic simulations and motion forecasting benchmarks
in different domains, demonstrating that our method not only achieves
state-of-the-art forecasting performance, but also provides interpretable and
reasonable hybrid attention weights.",1385,74
"['cs.LG', 'cs.AI', 'stat.ML']",Offline Meta-Reinforcement Learning with Advantage Weighting,"This paper introduces the offline meta-reinforcement learning (offline
meta-RL) problem setting and proposes an algorithm that performs well in this
setting. Offline meta-RL is analogous to the widely successful supervised
learning strategy of pre-training a model on a large batch of fixed,
pre-collected data (possibly from various tasks) and fine-tuning the model to a
new task with relatively little data. That is, in offline meta-RL, we
meta-train on fixed, pre-collected data from several tasks in order to adapt to
a new task with a very small amount (less than 5 trajectories) of data from the
new task. By nature of being offline, algorithms for offline meta-RL can
utilize the largest possible pool of training data available and eliminate
potentially unsafe or costly data collection during meta-training. This setting
inherits the challenges of offline RL, but it differs significantly because
offline RL does not generally consider a) transfer to new tasks or b) limited
data from the test task, both of which we face in offline meta-RL. Targeting
the offline meta-RL setting, we propose Meta-Actor Critic with Advantage
Weighting (MACAW), an optimization-based meta-learning algorithm that uses
simple, supervised regression objectives for both the inner and outer loop of
meta-training. On offline variants of common meta-RL benchmarks, we empirically
find that this approach enables fully offline meta-reinforcement learning and
achieves notable gains over prior methods.",1487,60
"['cs.LG', 'cs.CV', 'cs.MA']",Represented Value Function Approach for Large Scale Multi Agent Reinforcement Learning,"In this paper, we consider the problem of large scale multi agent
reinforcement learning. Firstly, we studied the representation problem of the
pairwise value function to reduce the complexity of the interactions among
agents. Secondly, we adopt a l2-norm trick to ensure the trivial term of the
approximated value function is bounded. Thirdly, experimental results on battle
game demonstrate the effectiveness of the proposed approach.",436,86
"['cs.CV', 'cs.AI']",Handwriting Profiling using Generative Adversarial Networks,"Handwriting is a skill learned by humans from a very early age. The ability
to develop one's own unique handwriting as well as mimic another person's
handwriting is a task learned by the brain with practice. This paper deals with
this very problem where an intelligent system tries to learn the handwriting of
an entity using Generative Adversarial Networks (GANs). We propose a modified
architecture of DCGAN (Radford, Metz, and Chintala 2015) to achieve this. We
also discuss about applying reinforcement learning techniques to achieve faster
learning. Our algorithm hopes to give new insights in this area and its uses
include identification of forged documents, signature verification, computer
generated art, digitization of documents among others. Our early implementation
of the algorithm illustrates a good performance with MNIST datasets.",847,59
"['cs.CV', 'cs.AI', 'cs.LG', 'eess.SP', 'stat.ML']",MEx: Multi-modal Exercises Dataset for Human Activity Recognition,"MEx: Multi-modal Exercises Dataset is a multi-sensor, multi-modal dataset,
implemented to benchmark Human Activity Recognition(HAR) and Multi-modal Fusion
algorithms. Collection of this dataset was inspired by the need for recognising
and evaluating quality of exercise performance to support patients with
Musculoskeletal Disorders(MSD). We select 7 exercises regularly recommended for
MSD patients by physiotherapists and collected data with four sensors a
pressure mat, a depth camera and two accelerometers. The dataset contains three
data modalities; numerical time-series data, video data and pressure sensor
data posing interesting research challenges when reasoning for HAR and Exercise
Quality Assessment. This paper presents our evaluation of the dataset on number
of standard classification algorithms for the HAR task by comparing different
feature representation algorithms for each sensor. These results set a
reference performance for each individual sensor that expose their strengths
and weaknesses for the future tasks. In addition we visualise pressure mat data
to explore the potential of the sensor to capture exercise performance quality.
With the recent advancement in multi-modal fusion, we also believe MEx is a
suitable dataset to benchmark not only HAR algorithms, but also fusion
algorithms of heterogeneous data types in multiple application domains.",1379,65
"['cs.CV', 'cs.AI', 'stat.ML']",Towards High Resolution Video Generation with Progressive Growing of Sliced Wasserstein GANs,"The extension of image generation to video generation turns out to be a very
difficult task, since the temporal dimension of videos introduces an extra
challenge during the generation process. Besides, due to the limitation of
memory and training stability, the generation becomes increasingly challenging
with the increase of the resolution/duration of videos. In this work, we
exploit the idea of progressive growing of Generative Adversarial Networks
(GANs) for higher resolution video generation. In particular, we begin to
produce video samples of low-resolution and short-duration, and then
progressively increase both resolution and duration alone (or jointly) by
adding new spatiotemporal convolutional layers to the current networks.
Starting from the learning on a very raw-level spatial appearance and temporal
movement of the video distribution, the proposed progressive method learns
spatiotemporal information incrementally to generate higher resolution videos.
Furthermore, we introduce a sliced version of Wasserstein GAN (SWGAN) loss to
improve the distribution learning on the video data of high-dimension and
mixed-spatiotemporal distribution. SWGAN loss replaces the distance between
joint distributions by that of one-dimensional marginal distributions, making
the loss easier to compute. We evaluate the proposed model on our collected
face video dataset of 10,900 videos to generate photorealistic face videos of
256x256x32 resolution. In addition, our model also reaches a record inception
score of 14.57 in unsupervised action recognition dataset UCF-101.",1580,92
"['cs.CV', 'cs.GR', 'cs.LG', 'stat.ML']",Realistic Image Generation using Region-phrase Attention,"The Generative Adversarial Network (GAN) has recently been applied to
generate synthetic images from text. Despite significant advances, most current
state-of-the-art algorithms are regular-grid region based; when attention is
used, it is mainly applied between individual regular-grid regions and a word.
These approaches are sufficient to generate images that contain a single object
in its foreground, such as a ""bird"" or ""flower"". However, natural languages
often involve complex foreground objects and the background may also constitute
a variable portion of the generated image. Therefore, the regular-grid based
image attention weights may not necessarily concentrate on the intended
foreground region(s), which in turn, results in an unnatural looking image.
Additionally, individual words such as ""a"", ""blue"" and ""shirt"" do not
necessarily provide a full visual context unless they are applied together. For
this reason, in our paper, we proposed a novel method in which we introduced an
additional set of attentions between true-grid regions and word phrases. The
true-grid region is derived using a set of auxiliary bounding boxes. These
auxiliary bounding boxes serve as superior location indicators to where the
alignment and attention should be drawn with the word phrases. Word phrases are
derived from analysing Part-of-Speech (POS) results. We perform experiments on
this novel network architecture using the Microsoft Common Objects in Context
(MSCOCO) dataset and the model generates $256 \times 256$ conditioned on a
short sentence description. Our proposed approach is capable of generating more
realistic images compared with the current state-of-the-art algorithms.",1688,56
['cs.CV'],A Solution for Crime Scene Reconstruction using Time-of-Flight Cameras,"In this work, we propose a method for three-dimensional (3D) reconstruction
of wide crime scene, based on a Simultaneous Localization and Mapping (SLAM)
approach. We used a Kinect V2 Time-of-Flight (TOF) RGB-D camera to provide
colored dense point clouds at a 30 Hz frequency. This device is moved freely (6
degrees of freedom) during the scene exploration. The implemented SLAM solution
aligns successive point clouds using an 3D keypoints description and matching
approach. This type of approach exploits both colorimetric and geometrical
information, and permits reconstruction under poor illumination conditions. Our
solution has been tested for indoor crime scene and outdoor archaeological site
reconstruction, returning a mean error around one centimeter. It is less
precise than environmental laser scanner solution, but more practical and
portable as well as less cumbersome. Also, the hardware is definitively
cheaper.",928,70
"['cs.LG', 'cs.AI', 'cs.NE']",Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning,"Reinforcement learning, evolutionary algorithms and imitation learning are
three principal methods to deal with continuous control tasks. Reinforcement
learning is sample efficient, yet sensitive to hyper-parameters setting and
needs efficient exploration; Evolutionary algorithms are stable, but with low
sample efficiency; Imitation learning is both sample efficient and stable,
however it requires the guidance of expert data. In this paper, we propose
Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning,
a scalable framework that combines advantages of the three methods mentioned
above. The core of this framework is a dual-actors and single critic
reinforcement learning agent. This agent can recruit high-fitness actors from
the population of evolutionary algorithms, which instructs itself to learn from
experience replay buffer. At the same time, low-fitness actors in the
evolutionary population can imitate behavior patterns of the reinforcement
learning agent and improve their adaptability. Reinforcement and imitation
learners in this framework can be replaced with any off-policy actor-critic
reinforcement learner or data-driven imitation learner. We evaluate RIM on a
series of benchmarks for continuous control tasks in Mujoco. The experimental
results show that RIM outperforms prior evolutionary or reinforcement learning
methods. The performance of RIM's components is significantly better than
components of previous evolutionary reinforcement learning algorithm, and the
recruitment using soft update enables reinforcement learning agent to learn
faster than that using hard update.",1632,71
"['cs.LG', 'cs.AI', 'cs.CV', 'cs.RO', 'stat.ML']",Sparse Graphical Memory for Robust Planning,"To operate effectively in the real world, agents should be able to act from
high-dimensional raw sensory input such as images and achieve diverse goals
across long time-horizons. Current deep reinforcement and imitation learning
methods can learn directly from high-dimensional inputs but do not scale well
to long-horizon tasks. In contrast, classical graphical methods like A* search
are able to solve long-horizon tasks, but assume that the state space is
abstracted away from raw sensory input. Recent works have attempted to combine
the strengths of deep learning and classical planning; however, dominant
methods in this domain are still quite brittle and scale poorly with the size
of the environment. We introduce Sparse Graphical Memory (SGM), a new data
structure that stores states and feasible transitions in a sparse memory. SGM
aggregates states according to a novel two-way consistency objective, adapting
classic state aggregation criteria to goal-conditioned RL: two states are
redundant when they are interchangeable both as goals and as starting states.
Theoretically, we prove that merging nodes according to two-way consistency
leads to an increase in shortest path lengths that scales only linearly with
the merging threshold. Experimentally, we show that SGM significantly
outperforms current state of the art methods on long horizon, sparse-reward
visual navigation tasks. Project video and code are available at
https://mishalaskin.github.io/sgm/",1471,43
['cs.CV'],Weakly Supervised Domain-Specific Color Naming Based on Attention,"The majority of existing color naming methods focuses on the eleven basic
color terms of the English language. However, in many applications, different
sets of color names are used for the accurate description of objects. Labeling
data to learn these domain-specific color names is an expensive and laborious
task. Therefore, in this article we aim to learn color names from weakly
labeled data. For this purpose, we add an attention branch to the color naming
network. The attention branch is used to modulate the pixel-wise color naming
predictions of the network. In experiments, we illustrate that the attention
branch correctly identifies the relevant regions. Furthermore, we show that our
method obtains state-of-the-art results for pixel-wise and image-wise
classification on the EBAY dataset and is able to learn color names for various
domains.",854,65
"['cs.CV', 'cs.CL']",Question Relevance in Visual Question Answering,"Free-form and open-ended Visual Question Answering systems solve the problem
of providing an accurate natural language answer to a question pertaining to an
image. Current VQA systems do not evaluate if the posed question is relevant to
the input image and hence provide nonsensical answers when posed with
irrelevant questions to an image. In this paper, we solve the problem of
identifying the relevance of the posed question to an image. We address the
problem as two sub-problems. We first identify if the question is visual or
not. If the question is visual, we then determine if it's relevant to the image
or not. For the second problem, we generate a large dataset from existing
visual question answering datasets in order to enable the training of complex
architectures and model the relevance of a visual question to an image. We also
compare the results of our Long Short-Term Memory Recurrent Neural Network
based models to Logistic Regression, XGBoost and multi-layer perceptron based
approaches to the problem.",1023,47
['cs.CV'],Towards Accurate Pixel-wise Object Tracking by Attention Retrieval,"The encoding of the target in object tracking moves from the coarse
bounding-box to fine-grained segmentation map recently. Revisiting de facto
real-time approaches that are capable of predicting mask during tracking, we
observed that they usually fork a light branch from the backbone network for
segmentation. Although efficient, directly fusing backbone features without
considering the negative influence of background clutter tends to introduce
false-negative predictions, lagging the segmentation accuracy. To mitigate this
problem, we propose an attention retrieval network (ARN) to perform soft
spatial constraints on backbone features. We first build a look-up-table (LUT)
with the ground-truth mask in the starting frame, and then retrieves the LUT to
obtain an attention map for spatial constraints. Moreover, we introduce a
multi-resolution multi-stage segmentation network (MMS) to further weaken the
influence of background clutter by reusing the predicted mask to filter
backbone features. Our approach set a new state-of-the-art on recent pixel-wise
object tracking benchmark VOT2020 while running at 40 fps. Notably, the
proposed model surpasses SiamMask by 11.7/4.2/5.5 points on VOT2020, DAVIS2016,
and DAVIS2017, respectively. We will release our code at
https://github.com/researchmm/TracKit.",1313,66
['cs.CV'],End-to-end feature fusion siamese network for adaptive visual tracking,"According to observations, different visual objects have different salient
features in different scenarios. Even for the same object, its salient shape
and appearance features may change greatly from time to time in a long-term
tracking task. Motivated by them, we proposed an end-to-end feature fusion
framework based on Siamese network, named FF-Siam, which can effectively fuse
different features for adaptive visual tracking. The framework consists of four
layers. A feature extraction layer is designed to extract the different
features of the target region and search region. The extracted features are
then put into a weight generation layer to obtain the channel weights, which
indicate the importance of different feature channels. Both features and the
channel weights are utilized in a template generation layer to generate a
discriminative template. Finally, the corresponding response maps created by
the convolution of the search region features and the template are applied with
a fusion layer to obtain the final response map for locating the target.
Experimental results demonstrate that the proposed framework achieves
state-of-the-art performance on the popular Temple-Color, OTB50 and UAV123
benchmarks.",1223,70
"['cs.CV', 'cs.CL', 'cs.LG', 'stat.ML']",Visual Reasoning with Multi-hop Feature Modulation,"Recent breakthroughs in computer vision and natural language processing have
spurred interest in challenging multi-modal tasks such as visual
question-answering and visual dialogue. For such tasks, one successful approach
is to condition image-based convolutional network computation on language via
Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and
shifting. We propose to generate the parameters of FiLM layers going up the
hierarchy of a convolutional network in a multi-hop fashion rather than all at
once, as in prior work. By alternating between attending to the language input
and generating FiLM layer parameters, this approach is better able to scale to
settings with longer input sequences such as dialogue. We demonstrate that
multi-hop FiLM generation achieves state-of-the-art for the short input
sequence task ReferIt --- on-par with single-hop FiLM generation --- while also
significantly outperforming prior state-of-the-art and single-hop FiLM
generation on the GuessWhat?! visual dialogue task.",1038,50
"['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']",Near real-time map building with multi-class image set labelling and classification of road conditions using convolutional neural networks,"Weather is an important factor affecting transportation and road safety. In
this paper, we leverage state-of-the-art convolutional neural networks in
labelling images taken by street and highway cameras located across across
North America. Road camera snapshots were used in experiments with multiple
deep learning frameworks to classify images by road condition. The training
data for these experiments used images labelled as dry, wet, snow/ice, poor,
and offline. The experiments tested different configurations of six
convolutional neural networks (VGG-16, ResNet50, Xception, InceptionResNetV2,
EfficientNet-B0 and EfficientNet-B4) to assess their suitability to this
problem. The precision, accuracy, and recall were measured for each framework
configuration. In addition, the training sets were varied both in overall size
and by size of individual classes. The final training set included 47,000
images labelled using the five aforementioned classes. The EfficientNet-B4
framework was found to be most suitable to this problem, achieving validation
accuracy of 90.6%, although EfficientNet-B0 achieved an accuracy of 90.3% with
half the execution time. It was observed that VGG-16 with transfer learning
proved to be very useful for data acquisition and pseudo-labelling with limited
hardware resources, throughout this project. The EfficientNet-B4 framework was
then placed into a real-time production environment, where images could be
classified in real-time on an ongoing basis. The classified images were then
used to construct a map showing real-time road conditions at various camera
locations across North America. The choice of these frameworks and our analysis
take into account unique requirements of real-time map building functions. A
detailed analysis of the process of semi-automated dataset labelling using
these frameworks is also presented in this paper.",1880,138
['cs.CV'],UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking,"In recent years, numerous effective multi-object tracking (MOT) methods are
developed because of the wide range of applications. Existing performance
evaluations of MOT methods usually separate the object tracking step from the
object detection step by using the same fixed object detection results for
comparisons. In this work, we perform a comprehensive quantitative study on the
effects of object detection accuracy to the overall MOT performance, using the
new large-scale University at Albany DETection and tRACking (UA-DETRAC)
benchmark dataset. The UA-DETRAC benchmark dataset consists of 100 challenging
video sequences captured from real-world traffic scenes (over 140,000 frames
with rich annotations, including occlusion, weather, vehicle category,
truncation, and vehicle bounding boxes) for object detection, object tracking
and MOT system. We evaluate complete MOT systems constructed from combinations
of state-of-the-art object detection and object tracking methods. Our analysis
shows the complex effects of object detection accuracy on MOT system
performance. Based on these observations, we propose new evaluation tools and
metrics for MOT systems that consider both object detection and object tracking
for comprehensive analysis.",1251,79
['cs.CV'],"Deep Learning for Medical Image Processing: Overview, Challenges and Future","Healthcare sector is totally different from other industry. It is on high
priority sector and people expect highest level of care and services regardless
of cost. It did not achieve social expectation even though it consume huge
percentage of budget. Mostly the interpretations of medical data is being done
by medical expert. In terms of image interpretation by human expert, it is
quite limited due to its subjectivity, the complexity of the image, extensive
variations exist across different interpreters, and fatigue. After the success
of deep learning in other real world application, it is also providing exciting
solutions with good accuracy for medical imaging and is seen as a key method
for future applications in health secotr. In this chapter, we discussed state
of the art deep learning architecture and its optimization used for medical
image segmentation and classification. In the last section, we have discussed
the challenges deep learning based methods for medical imaging and open
research issue.",1016,75
"['cs.LG', 'cs.SD', 'eess.AS', 'stat.ML']",Dirichlet Mixture Model based VQ Performance Prediction for Line Spectral Frequency,"In this paper, we continue our previous work on the Dirichlet mixture model
(DMM)-based VQ to derive the performance bound of the LSF VQ. The LSF
parameters are transformed into the $\Delta$LSF domain and the underlying
distribution of the $\Delta$LSF parameters are modelled by a DMM with finite
number of mixture components. The quantization distortion, in terms of the mean
squared error (MSE), is calculated with the high rate theory. The mapping
relation between the perceptually motivated log spectral distortion (LSD) and
the MSE is empirically approximated by a polynomial. With this mapping
function, the minimum required bit rate for transparent coding of the LSF is
estimated.",687,83
"['stat.ML', 'cs.AI', 'cs.LG']",Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability,"Explaining AI systems is fundamental both to the development of high
performing models and to the trust placed in them by their users. The Shapley
framework for explainability has strength in its general applicability combined
with its precise, rigorous foundation: it provides a common, model-agnostic
language for AI explainability and uniquely satisfies a set of intuitive
mathematical axioms. However, Shapley values are too restrictive in one
significant regard: they ignore all causal structure in the data. We introduce
a less restrictive framework, Asymmetric Shapley values (ASVs), which are
rigorously founded on a set of axioms, applicable to any AI system, and
flexible enough to incorporate any causal structure known to be respected by
the data. We demonstrate that ASVs can (i) improve model explanations by
incorporating causal information, (ii) provide an unambiguous test for unfair
discrimination in model predictions, (iii) enable sequentially incremental
explanations in time-series models, and (iv) support feature-selection studies
without the need for model retraining.",1093,92
['cs.CV'],Cascade Region Proposal and Global Context for Deep Object Detection,"Deep region-based object detector consists of a region proposal step and a
deep object recognition step. In this paper, we make significant improvements
on both of the two steps. For region proposal we propose a novel lightweight
cascade structure which can effectively improve RPN proposal quality. For
object recognition we re-implement global context modeling with a few
modications and obtain a performance boost (4.2% mAP gain on the ILSVRC 2016
validation set). Besides, we apply the idea of pre-training extensively and
show its importance in both steps. Together with common training and testing
tricks, we improve Faster R-CNN baseline by a large margin. In particular, we
obtain 87.9% mAP on the PASCAL VOC 2012 test set, 65.3% on the ILSVRC 2016 test
set and 36.8% on the COCO test-std set.",801,68
"['cs.CV', 'cs.LG']",DADA: Differentiable Automatic Data Augmentation,"Data augmentation (DA) techniques aim to increase data variability, and thus
train deep networks with better generalisation. The pioneering AutoAugment
automated the search for optimal DA policies with reinforcement learning.
However, AutoAugment is extremely computationally expensive, limiting its wide
applicability. Followup works such as Population Based Augmentation (PBA) and
Fast AutoAugment improved efficiency, but their optimization speed remains a
bottleneck. In this paper, we propose Differentiable Automatic Data
Augmentation (DADA) which dramatically reduces the cost. DADA relaxes the
discrete DA policy selection to a differentiable optimization problem via
Gumbel-Softmax. In addition, we introduce an unbiased gradient estimator,
RELAX, leading to an efficient and effective one-pass optimization strategy to
learn an efficient and accurate DA policy. We conduct extensive experiments on
CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate
the value of Auto DA in pre-training for downstream detection problems. Results
show our DADA is at least one order of magnitude faster than the
state-of-the-art while achieving very comparable accuracy. The code is
available at https://github.com/VDIGPKU/DADA.",1247,48
"['cs.LG', 'cs.CV', 'cs.NE', 'eess.IV']",RT3D: Achieving Real-Time Execution of 3D Convolutional Neural Networks on Mobile Devices,"Mobile devices are becoming an important carrier for deep learning tasks, as
they are being equipped with powerful, high-end mobile CPUs and GPUs. However,
it is still a challenging task to execute 3D Convolutional Neural Networks
(CNNs) targeting for real-time performance, besides high inference accuracy.
The reason is more complex model structure and higher model dimensionality
overwhelm the available computation/storage resources on mobile devices. A
natural way may be turning to deep learning weight pruning techniques. However,
the direct generalization of existing 2D CNN weight pruning methods to 3D CNNs
is not ideal for fully exploiting mobile parallelism while achieving high
inference accuracy.
  This paper proposes RT3D, a model compression and mobile acceleration
framework for 3D CNNs, seamlessly integrating neural network weight pruning and
compiler code generation techniques. We propose and investigate two structured
sparsity schemes i.e., the vanilla structured sparsity and kernel group
structured (KGS) sparsity that are mobile acceleration friendly. The vanilla
sparsity removes whole kernel groups, while KGS sparsity is a more fine-grained
structured sparsity that enjoys higher flexibility while exploiting full
on-device parallelism. We propose a reweighted regularization pruning algorithm
to achieve the proposed sparsity schemes. The inference time speedup due to
sparsity is approaching the pruning rate of the whole model FLOPs (floating
point operations). RT3D demonstrates up to 29.1$\times$ speedup in end-to-end
inference time comparing with current mobile frameworks supporting 3D CNNs,
with moderate 1%-1.5% accuracy loss. The end-to-end inference time for 16 video
frames could be within 150 ms, when executing representative C3D and R(2+1)D
models on a cellphone. For the first time, real-time execution of 3D CNNs is
achieved on off-the-shelf mobiles.",1898,89
['cs.CV'],Drop-DTW: Aligning Common Signal Between Sequences While Dropping Outliers,"In this work, we consider the problem of sequence-to-sequence alignment for
signals containing outliers. Assuming the absence of outliers, the standard
Dynamic Time Warping (DTW) algorithm efficiently computes the optimal alignment
between two (generally) variable-length sequences. While DTW is robust to
temporal shifts and dilations of the signal, it fails to align sequences in a
meaningful way in the presence of outliers that can be arbitrarily interspersed
in the sequences. To address this problem, we introduce Drop-DTW, a novel
algorithm that aligns the common signal between the sequences while
automatically dropping the outlier elements from the matching. The entire
procedure is implemented as a single dynamic program that is efficient and
fully differentiable. In our experiments, we show that Drop-DTW is a robust
similarity measure for sequence retrieval and demonstrate its effectiveness as
a training loss on diverse applications. With Drop-DTW, we address temporal
step localization on instructional videos, representation learning from noisy
videos, and cross-modal representation learning for audio-visual retrieval and
localization. In all applications, we take a weakly- or unsupervised approach
and demonstrate state-of-the-art results under these settings.",1283,74
"['cs.CV', 'eess.IV']",The Image Local Autoregressive Transformer,"Recently, AutoRegressive (AR) models for the whole image generation empowered
by transformers have achieved comparable or even better performance to
Generative Adversarial Networks (GANs). Unfortunately, directly applying such
AR models to edit/change local image regions, may suffer from the problems of
missing global information, slow inference speed, and information leakage of
local guidance. To address these limitations, we propose a novel model -- image
Local Autoregressive Transformer (iLAT), to better facilitate the locally
guided image synthesis. Our iLAT learns the novel local discrete
representations, by the newly proposed local autoregressive (LA) transformer of
the attention mask and convolution mechanism. Thus iLAT can efficiently
synthesize the local image regions by key guidance information. Our iLAT is
evaluated on various locally guided image syntheses, such as pose-guided person
image synthesis and face editing. Both the quantitative and qualitative results
show the efficacy of our model.",1020,42
['cs.CV'],Unsupervised Eyeglasses Removal in the Wild,"Eyeglasses removal is challenging in removing different kinds of eyeglasses,
e.g., rimless glasses, full-rim glasses and sunglasses, and recovering
appropriate eyes. Due to the large visual variants, the conventional methods
lack scalability. Most existing works focus on the frontal face images in the
controlled environment, such as the laboratory, and need to design specific
systems for different eyeglass types. To address the limitation, we propose a
unified eyeglass removal model called Eyeglasses Removal Generative Adversarial
Network (ERGAN), which could handle different types of glasses in the wild. The
proposed method does not depend on the dense annotation of eyeglasses location
but benefits from the large-scale face images with weak annotations.
Specifically, we study the two relevant tasks simultaneously, i.e., removing
and wearing eyeglasses. Given two facial images with and without eyeglasses,
the proposed model learns to swap the eye area in two faces. The generation
mechanism focuses on the eye area and invades the difficulty of generating a
new face. In the experiment, we show the proposed method achieves a competitive
removal quality in terms of realism and diversity. Furthermore, we evaluate
ERGAN on several subsequent tasks, such as face verification and facial
expression recognition. The experiment shows that our method could serve as a
pre-processing method for these tasks.",1416,43
"['cs.LG', 'cs.AI']",Predictive Multi-level Patient Representations from Electronic Health Records,"The advent of the Internet era has led to an explosive growth in the
Electronic Health Records (EHR) in the past decades. The EHR data can be
regarded as a collection of clinical events, including laboratory results,
medication records, physiological indicators, etc, which can be used for
clinical outcome prediction tasks to support constructions of intelligent
health systems. Learning patient representation from these clinical events for
the clinical outcome prediction is an important but challenging step. Most
related studies transform EHR data of a patient into a sequence of clinical
events in temporal order and then use sequential models to learn patient
representations for outcome prediction. However, clinical event sequence
contains thousands of event types and temporal dependencies. We further make an
observation that clinical events occurring in a short period are not
constrained by any temporal order but events in a long term are influenced by
temporal dependencies. The multi-scale temporal property makes it difficult for
traditional sequential models to capture the short-term co-occurrence and the
long-term temporal dependencies in clinical event sequences. In response to the
above challenges, this paper proposes a Multi-level Representation Model (MRM).
MRM first uses a sparse attention mechanism to model the short-term
co-occurrence, then uses interval-based event pooling to remove redundant
information and reduce sequence length and finally predicts clinical outcomes
through Long Short-Term Memory (LSTM). Experiments on real-world datasets
indicate that our proposed model largely improves the performance of clinical
outcome prediction tasks using EHR data.",1697,77
"['cs.LG', 'q-bio.MN', 'q-bio.QM']",PaccMann: Prediction of anticancer compound sensitivity with multi-modal attention-based neural networks,"We present a novel approach for the prediction of anticancer compound
sensitivity by means of multi-modal attention-based neural networks (PaccMann).
In our approach, we integrate three key pillars of drug sensitivity, namely,
the molecular structure of compounds, transcriptomic profiles of cancer cells
as well as prior knowledge about interactions among proteins within cells. Our
models ingest a drug-cell pair consisting of SMILES encoding of a compound and
the gene expression profile of a cancer cell and predicts an IC50 sensitivity
value. Gene expression profiles are encoded using an attention-based encoding
mechanism that assigns high weights to the most informative genes. We present
and study three encoders for SMILES string of compounds: 1) bidirectional
recurrent 2) convolutional 3) attention-based encoders. We compare our devised
models against a baseline model that ingests engineered fingerprints to
represent the molecular structure. We demonstrate that using our
attention-based encoders, we can surpass the baseline model. The use of
attention-based encoders enhance interpretability and enable us to identify
genes, bonds and atoms that were used by the network to make a prediction.",1209,104
['cs.CV'],A General Gaussian Heatmap Labeling for Arbitrary-Oriented Object Detection,"Recently, many arbitrary-oriented object detection (AOOD) methods have been
proposed and attracted widespread attention in many fields. However, most of
them are based on anchor-boxes or standard Gaussian heatmaps. Such label
assignment strategy may not only fail to reflect the shape and direction
characteristics of arbitrary-oriented objects, but also have high
parameter-tuning efforts. In this paper, a novel AOOD method called General
Gaussian Heatmap Labeling (GGHL) is proposed. Specifically, an anchor-free
object-adaptation label assignment (OLA) strategy is presented to define the
positive candidates based on two-dimensional (2-D) oriented Gaussian heatmaps,
which reflect the shape and direction features of arbitrary-oriented objects.
Based on OLA, an oriented-bounding-box (OBB) representation component (ORC) is
developed to indicate OBBs and adjust the Gaussian center prior weights to fit
the characteristics of different objects adaptively through neural network
learning. Moreover, a joint-optimization loss (JOL) with area normalization and
dynamic confidence weighting is designed to refine the misalign optimal results
of different subtasks. Extensive experiments on public datasets demonstrate
that the proposed GGHL improves the AOOD performance with low parameter-tuning
and time costs. Furthermore, it is generally applicable to most AOOD methods to
improve their performance including lightweight models on embedded platforms.",1455,75
"['cs.LG', 'cs.AI', 'stat.ML']",A Theoretical Analysis of Contrastive Unsupervised Representation Learning,"Recent empirical works have successfully used unlabeled data to learn feature
representations that are broadly useful in downstream classification tasks.
Several of these methods are reminiscent of the well-known word2vec embedding
algorithm: leveraging availability of pairs of semantically ""similar"" data
points and ""negative samples,"" the learner forces the inner product of
representations of similar pairs with each other to be higher on average than
with negative samples. The current paper uses the term contrastive learning for
such algorithms and presents a theoretical framework for analyzing them by
introducing latent classes and hypothesizing that semantically similar points
are sampled from the same latent class. This framework allows us to show
provable guarantees on the performance of the learned representations on the
average classification task that is comprised of a subset of the same set of
latent classes. Our generalization bound also shows that learned
representations can reduce (labeled) sample complexity on downstream tasks. We
conduct controlled experiments in both the text and image domains to support
the theory.",1148,74
['cs.CV'],Image/Video Deep Anomaly Detection: A Survey,"The considerable significance of Anomaly Detection (AD) problem has recently
drawn the attention of many researchers. Consequently, the number of proposed
methods in this research field has been increased steadily. AD strongly
correlates with the important computer vision and image processing tasks such
as image/video anomaly, irregularity and sudden event detection. More recently,
Deep Neural Networks (DNNs) offer a high performance set of solutions, but at
the expense of a heavy computational cost. However, there is a noticeable gap
between the previously proposed methods and an applicable real-word approach.
Regarding the raised concerns about AD as an ongoing challenging problem,
notably in images and videos, the time has come to argue over the pitfalls and
prospects of methods have attempted to deal with visual AD tasks. Hereupon, in
this survey we intend to conduct an in-depth investigation into the
images/videos deep learning based AD methods. We also discuss current
challenges and future research directions thoroughly.",1042,44
"['cs.CV', 'cs.GR']",Differentiable Rendering: A Survey,"Deep neural networks (DNNs) have shown remarkable performance improvements on
vision-related tasks such as object detection or image segmentation. Despite
their success, they generally lack the understanding of 3D objects which form
the image, as it is not always possible to collect 3D information about the
scene or to easily annotate it. Differentiable rendering is a novel field which
allows the gradients of 3D objects to be calculated and propagated through
images. It also reduces the requirement of 3D data collection and annotation,
while enabling higher success rate in various applications. This paper reviews
existing literature and discusses the current state of differentiable
rendering, its applications and open research problems.",746,34
"['cs.CV', 'cs.SD', 'eess.AS']",Rethinking CNN Models for Audio Classification,"In this paper, we show that ImageNet-Pretrained standard deep CNN models can
be used as strong baseline networks for audio classification. Even though there
is a significant difference between audio Spectrogram and standard ImageNet
image samples, transfer learning assumptions still hold firmly. To understand
what enables the ImageNet pretrained models to learn useful audio
representations, we systematically study how much of pretrained weights is
useful for learning spectrograms. We show (1) that for a given standard model
using pretrained weights is better than using randomly initialized weights (2)
qualitative results of what the CNNs learn from the spectrograms by visualizing
the gradients. Besides, we show that even though we use the pretrained model
weights for initialization, there is variance in performance in various output
runs of the same model. This variance in performance is due to the random
initialization of linear classification layer and random mini-batch orderings
in multiple runs. This brings significant diversity to build stronger ensemble
models with an overall improvement in accuracy. An ensemble of ImageNet
pretrained DenseNet achieves 92.89% validation accuracy on the ESC-50 dataset
and 87.42% validation accuracy on the UrbanSound8K dataset which is the current
state-of-the-art on both of these datasets.",1349,46
['cs.CV'],Measuring the Accuracy of Object Detectors and Trackers,"The accuracy of object detectors and trackers is most commonly evaluated by
the Intersection over Union (IoU) criterion. To date, most approaches are
restricted to axis-aligned or oriented boxes and, as a consequence, many
datasets are only labeled with boxes. Nevertheless, axis-aligned or oriented
boxes cannot accurately capture an object's shape. To address this, a number of
densely segmented datasets has started to emerge in both the object detection
and the object tracking communities. However, evaluating the accuracy of object
detectors and trackers that are restricted to boxes on densely segmented data
is not straightforward. To close this gap, we introduce the relative
Intersection over Union (rIoU) accuracy measure. The measure normalizes the IoU
with the optimal box for the segmentation to generate an accuracy measure that
ranges between 0 and 1 and allows a more precise measurement of accuracies.
Furthermore, it enables an efficient and easy way to understand scenes and the
strengths and weaknesses of an object detection or tracking approach. We
display how the new measure can be efficiently calculated and present an
easy-to-use evaluation framework. The framework is tested on the DAVIS and the
VOT2016 segmentations and has been made available to the community.",1291,55
"['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']",Compositional Attention Networks for Interpretability in Natural Language Question Answering,"MAC Net is a compositional attention network designed for Visual Question
Answering. We propose a modified MAC net architecture for Natural Language
Question Answering. Question Answering typically requires Language
Understanding and multi-step Reasoning. MAC net's unique architecture - the
separation between memory and control, facilitates data-driven iterative
reasoning. This makes it an ideal candidate for solving tasks that involve
logical reasoning. Our experiments with 20 bAbI tasks demonstrate the value of
MAC net as a data-efficient and interpretable architecture for Natural Language
Question Answering. The transparent nature of MAC net provides a highly
granular view of the reasoning steps taken by the network in answering a query.",750,92
"['cs.CV', 'cs.LG', 'stat.ML']",Explainable Deep One-Class Classification,"Deep one-class classification variants for anomaly detection learn a mapping
that concentrates nominal samples in feature space causing anomalies to be
mapped away. Because this transformation is highly non-linear, finding
interpretations poses a significant challenge. In this paper we present an
explainable deep one-class classification method, Fully Convolutional Data
Description (FCDD), where the mapped samples are themselves also an explanation
heatmap. FCDD yields competitive detection performance and provides reasonable
explanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet.
On MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps,
FCDD sets a new state of the art in the unsupervised setting. Our method can
incorporate ground-truth anomaly maps during training and using even a few of
these (~5) improves performance significantly. Finally, using FCDD's
explanations we demonstrate the vulnerability of deep one-class classification
models to spurious image features such as image watermarks.",1054,41
['cs.CV'],Towards Head Motion Compensation Using Multi-Scale Convolutional Neural Networks,"Head pose estimation and tracking is useful in variety of medical
applications. With the advent of RGBD cameras like Kinect, it has become
feasible to do markerless tracking by estimating the head pose directly from
the point clouds. One specific medical application is robot assisted
transcranial magnetic stimulation (TMS) where any patient motion is compensated
with the help of a robot. For increased patient comfort, it is important to
track the head without markers. In this regard, we address the head pose
estimation problem using two different approaches. In the first approach, we
build upon the more traditional approach of model based head tracking, where a
head model is morphed according to the particular head to be tracked and the
morphed model is used to track the head in the point cloud streams. In the
second approach, we propose a new multi-scale convolutional neural network
architecture for more accurate pose regression. Additionally, we outline a
systematic data set acquisition strategy using a head phantom mounted on the
robot and ground-truth labels generated using a highly accurate tracking
system.",1129,80
"['cs.LG', 'stat.ML']",Relational Constraints for Metric Learning on Relational Data,"Most of metric learning approaches are dedicated to be applied on data
described by feature vectors, with some notable exceptions such as times
series, trees or graphs. The objective of this paper is to propose a metric
learning algorithm that specifically considers relational data. The proposed
approach can take benefit from both the topological structure of the data and
supervised labels. For selecting relative constraints representing the
relational information, we introduce a link-strength function that measures the
strength of relationship links between entities by the side-information of
their common parents. We show the performance of the proposed method with two
different classical metric learning algorithms, which are ITML (Information
Theoretic Metric Learning) and LSML (Least Squares Metric Learning), and test
on several real-world datasets. Experimental results show that using relational
information improves the quality of the learned metric.",968,61
['stat.ML'],An Expectation Conditional Maximization approach for Gaussian graphical models,"Bayesian graphical models are a useful tool for understanding dependence
relationships among many variables, particularly in situations with external
prior information. In high-dimensional settings, the space of possible graphs
becomes enormous, rendering even state-of-the-art Bayesian stochastic search
computationally infeasible. We propose a deterministic alternative to estimate
Gaussian and Gaussian copula graphical models using an Expectation Conditional
Maximization (ECM) algorithm, extending the EM approach from Bayesian variable
selection to graphical model estimation. We show that the ECM approach enables
fast posterior exploration under a sequence of mixture priors, and can
incorporate multiple sources of information.",736,78
"['cs.LG', 'cs.AI', 'cs.MA']",SHAQ: Incorporating Shapley Value Theory into Q-Learning for Multi-Agent Reinforcement Learning,"Value factorisation proves to be a very useful technique in multi-agent
reinforcement learning (MARL), but the underlying mechanism is not yet fully
understood. This paper explores a theoretic basis for value factorisation. We
generalise the Shapley value in the coalitional game theory to a Markov convex
game (MCG) and use it to guide value factorisation in MARL. We show that the
generalised Shapley value possesses several features such as (1) accurate
estimation of the maximum global value, (2) fairness in the factorisation of
the global value, and (3) being sensitive to dummy agents. The proposed theory
yields a new learning algorithm called Sharpley Q-learning (SHAQ), which
inherits the important merits of ordinary Q-learning but extends it to MARL. In
comparison with prior-arts, SHAQ has a much weaker assumption (MCG) that is
more compatible with real-world problems, but has superior explainability and
performance in many cases. We demonstrated SHAQ and verified the theoretic
claims on Predator-Prey and StarCraft Multi-Agent Challenge (SMAC).",1062,95
['cs.CV'],Referring Segmentation in Images and Videos with Cross-Modal Self-Attention Network,"We consider the problem of referring segmentation in images and videos with
natural language. Given an input image (or video) and a referring expression,
the goal is to segment the entity referred by the expression in the image or
video. In this paper, we propose a cross-modal self-attention (CMSA) module to
utilize fine details of individual words and the input image or video, which
effectively captures the long-range dependencies between linguistic and visual
features. Our model can adaptively focus on informative words in the referring
expression and important regions in the visual input. We further propose a
gated multi-level fusion (GMLF) module to selectively integrate self-attentive
cross-modal features corresponding to different levels of visual features. This
module controls the feature fusion of information flow of features at different
levels with high-level and low-level semantic information related to different
attentive words. Besides, we introduce cross-frame self-attention (CFSA) module
to effectively integrate temporal information in consecutive frames which
extends our method in the case of referring segmentation in videos. Experiments
on benchmark datasets of four referring image datasets and two actor and action
video segmentation datasets consistently demonstrate that our proposed approach
outperforms existing state-of-the-art methods.",1378,83
['cs.CV'],Point Cloud Upsampling and Normal Estimation using Deep Learning for Robust Surface Reconstruction,"The reconstruction of real-world surfaces is on high demand in various
applications. Most existing reconstruction approaches apply 3D scanners for
creating point clouds which are generally sparse and of low density. These
points clouds will be triangulated and used for visualization in combination
with surface normals estimated by geometrical approaches. However, the quality
of the reconstruction depends on the density of the point cloud and the
estimation of the surface normals. In this paper, we present a novel deep
learning architecture for point cloud upsampling that enables subsequent stable
and smooth surface reconstruction. A noisy point cloud of low density with
corresponding point normals is used to estimate a point cloud with higher
density and appendant point normals. To this end, we propose a compound loss
function that encourages the network to estimate points that lie on a surface
including normals accurately predicting the orientation of the surface. Our
results show the benefit of estimating normals together with point positions.
The resulting point cloud is smoother, more complete, and the final surface
reconstruction is much closer to ground truth.",1184,98
"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']",Ego-Pose Estimation and Forecasting as Real-Time PD Control,"We propose the use of a proportional-derivative (PD) control based policy
learned via reinforcement learning (RL) to estimate and forecast 3D human pose
from egocentric videos. The method learns directly from unsegmented egocentric
videos and motion capture data consisting of various complex human motions
(e.g., crouching, hopping, bending, and motion transitions). We propose a
video-conditioned recurrent control technique to forecast physically-valid and
stable future motions of arbitrary length. We also introduce a value function
based fail-safe mechanism which enables our method to run as a single pass
algorithm over the video data. Experiments with both controlled and in-the-wild
data show that our approach outperforms previous art in both quantitative
metrics and visual quality of the motions, and is also robust enough to
transfer directly to real-world scenarios. Additionally, our time analysis
shows that the combined use of our pose estimation and forecasting can run at
30 FPS, making it suitable for real-time applications.",1046,59
"['cs.LG', 'stat.ML']",Learning with Correntropy-induced Losses for Regression with Mixture of Symmetric Stable Noise,"In recent years, correntropy and its applications in machine learning have
been drawing continuous attention owing to its merits in dealing with
non-Gaussian noise and outliers. However, theoretical understanding of
correntropy, especially in the statistical learning context, is still limited.
In this study, within the statistical learning framework, we investigate
correntropy based regression in the presence of non-Gaussian noise or outliers.
Motivated by the practical way of generating non-Gaussian noise or outliers, we
introduce mixture of symmetric stable noise, which include Gaussian noise,
Cauchy noise, and their mixture as special cases, to model non-Gaussian noise
or outliers. We demonstrate that under the mixture of symmetric stable noise
assumption, correntropy based regression can learn the conditional mean
function or the conditional median function well without resorting to the
finite-variance or even the finite first-order moment condition on the noise.
In particular, for the above two cases, we establish asymptotic optimal
learning rates for correntropy based regression estimators that are
asymptotically of type $\mathcal{O}(n^{-1})$. These results justify the
effectiveness of the correntropy based regression estimators in dealing with
outliers as well as non-Gaussian noise. We believe that the present study
completes our understanding towards correntropy based regression from a
statistical learning viewpoint, and may also shed some light on robust
statistical learning for regression.",1524,94
"['cs.CV', 'cs.GR']",Tackling problems of marker-based augmented reality under water,"Underwater sites are a harsh environment for augmented reality applications.
Obstacles that must be battled include poor visibility conditions, difficult
navigation, and hard manipulation with devices under water. This chapter
focuses on the problem of localizing a device under water using markers. It
discusses various filters that enhance and improve images recorded under water,
and their impact on marker-based tracking. It presents various combinations of
10 image improving algorithms and 4 marker detecting algorithms, and tests
their performance in real situations. All solutions are designed to run
real-time on mobile devices to provide a solid basis for augmented reality.
Usability of this solution is evaluated on locations in Mediterranean Sea. It
is shown that image improving algorithms with carefully chosen parameters can
reduce the problems with visibility under water and improve the detection of
markers. The best results are obtained with marker detecting algorithms that
are specifically designed for underwater environments.",1049,63
"['cs.LG', 'cs.AI']",ProcessTransformer: Predictive Business Process Monitoring with Transformer Network,"Predictive business process monitoring focuses on predicting future
characteristics of a running process using event logs. The foresight into
process execution promises great potentials for efficient operations, better
resource management, and effective customer services. Deep learning-based
approaches have been widely adopted in process mining to address the
limitations of classical algorithms for solving multiple problems, especially
the next event and remaining-time prediction tasks. Nevertheless, designing a
deep neural architecture that performs competitively across various tasks is
challenging as existing methods fail to capture long-range dependencies in the
input sequences and perform poorly for lengthy process traces. In this paper,
we propose ProcessTransformer, an approach for learning high-level
representations from event logs with an attention-based network. Our model
incorporates long-range memory and relies on a self-attention mechanism to
establish dependencies between a multitude of event sequences and corresponding
outputs. We evaluate the applicability of our technique on nine real event
logs. We demonstrate that the transformer-based model outperforms several
baselines of prior techniques by obtaining on average above 80% accuracy for
the task of predicting the next activity. Our method also perform
competitively, compared to baselines, for the tasks of predicting event time
and remaining time of a running case",1454,83
['cs.CV'],CSIFT Based Locality-constrained Linear Coding for Image Classification,"In the past decade, SIFT descriptor has been witnessed as one of the most
robust local invariant feature descriptors and widely used in various vision
tasks. Most traditional image classification systems depend on the
luminance-based SIFT descriptors, which only analyze the gray level variations
of the images. Misclassification may happen since their color contents are
ignored. In this article, we concentrate on improving the performance of
existing image classification algorithms by adding color information. To
achieve this purpose, different kinds of colored SIFT descriptors are
introduced and implemented. Locality-constrained Linear Coding (LLC), a
state-of-the-art sparse coding technology, is employed to construct the image
classification system for the evaluation. The real experiments are carried out
on several benchmarks. With the enhancements of color SIFT, the proposed image
classification system obtains approximate 3% improvement of classification
accuracy on the Caltech-101 dataset and approximate 4% improvement of
classification accuracy on the Caltech-256 dataset.",1092,71
['cs.CV'],Mid-level Representation for Visual Recognition,"Visual Recognition is one of the fundamental challenges in AI, where the goal
is to understand the semantics of visual data. Employing mid-level
representation, in particular, shifted the paradigm in visual recognition. The
mid-level image/video representation involves discovering and training a set of
mid-level visual patterns (e.g., parts and attributes) and represent a given
image/video utilizing them. The mid-level patterns can be extracted from images
and videos using the motion and appearance information of visual phenomenas.
This thesis targets employing mid-level representations for different
high-level visual recognition tasks, namely (i)image understanding and
(ii)video understanding.
  In the case of image understanding, we focus on object detection/recognition
task. We investigate on discovering and learning a set of mid-level patches to
be used for representing the images of an object category. We specifically
employ the discriminative patches in a subcategory-aware webly-supervised
fashion. We, additionally, study the outcomes provided by employing the
subcategory-based models for undoing dataset bias.",1133,47
"['cs.CV', 'cs.LG', 'stat.ML']",Efficient Interpretation of Deep Learning Models Using Graph Structure and Cooperative Game Theory: Application to ASD Biomarker Discovery,"Discovering imaging biomarkers for autism spectrum disorder (ASD) is critical
to help explain ASD and predict or monitor treatment outcomes. Toward this end,
deep learning classifiers have recently been used for identifying ASD from
functional magnetic resonance imaging (fMRI) with higher accuracy than
traditional learning strategies. However, a key challenge with deep learning
models is understanding just what image features the network is using, which
can in turn be used to define the biomarkers. Current methods extract
biomarkers, i.e., important features, by looking at how the prediction changes
if ""ignoring"" one feature at a time. In this work, we go beyond looking at only
individual features by using Shapley value explanation (SVE) from cooperative
game theory. Cooperative game theory is advantageous here because it directly
considers the interaction between features and can be applied to any machine
learning method, making it a novel, more accurate way of determining
instance-wise biomarker importance from deep learning models. A barrier to
using SVE is its computational complexity: $2^N$ given $N$ features. We
explicitly reduce the complexity of SVE computation by two approaches based on
the underlying graph structure of the input data: 1) only consider the
centralized coalition of each feature; 2) a hierarchical pipeline which first
clusters features into small communities, then applies SVE in each community.
Monte Carlo approximation can be used for large permutation sets. We first
validate our methods on the MNIST dataset and compare to human perception.
Next, to insure plausibility of our biomarker results, we train a Random Forest
(RF) to classify ASD/control subjects from fMRI and compare SVE results to
standard RF-based feature importance. Finally, we show initial results on
ranked fMRI biomarkers using SVE on a deep learning classifier for the
ASD/control dataset.",1912,138
"['cs.CV', 'cs.LG']",Deep Transfer Learning for Identifications of Slope Surface Cracks,"Geohazards such as landslides have caused great losses to the safety of
people's lives and property, which is often accompanied with surface cracks. If
such surface cracks could be identified in time, it is of great significance
for the monitoring and early warning of geohazards. Currently, the most common
method for crack identification is manual detection, which is with low
efficiency and accuracy. In this paper, a deep transfer learning framework is
proposed to effectively and efficiently identify slope surface cracks for the
sake of fast monitoring and early warning of geohazards such as landslides. The
essential idea is to employ transfer learning by training (a) the large sample
dataset of concrete cracks and (b) the small sample dataset of soil and rock
masses cracks. In the proposed framework, (1) pretrained cracks identification
models are constructed based on the large sample dataset of concrete cracks;
(2) refined cracks identification models are further constructed based on the
small sample dataset of soil and rock masses cracks. The proposed framework
could be applied to conduct UAV surveys on high-steep slopes to realize the
monitoring and early warning of landslides to ensure the safety of people's
lives and property.",1252,66
['cs.CV'],Blockwise Based Detection of Local Defects,"Print quality is an important criterion for a printer's performance. The
detection, classification, and assessment of printing defects can reflect the
printer's working status and help to locate mechanical problems inside. To
handle all these questions, an efficient algorithm is needed to replace the
traditionally visual checking method. In this paper, we focus on pages with
local defects including gray spots and solid spots. We propose a coarse-to-fine
method to detect local defects in a block-wise manner, and aggregate the
blockwise attributes to generate the feature vector of the whole test page for
a further ranking task. In the detection part, we first select candidate
regions by thresholding a single feature. Then more detailed features of
candidate blocks are calculated and sent to a decision tree that is previously
trained on our training dataset. The final result is given by the decision tree
model to control the false alarm rate while maintaining the required miss rate.
Our algorithm is proved to be effective in detecting and classifying local
defects compared with previous methods.",1109,42
"['cs.CV', 'cs.LG']",Multi-view adaptive graph convolutions for graph classification,"In this paper, a novel multi-view methodology for graph-based neural networks
is proposed. A systematic and methodological adaptation of the key concepts of
classical deep learning methods such as convolution, pooling and multi-view
architectures is developed for the context of non-Euclidean manifolds. The aim
of the proposed work is to present a novel multi-view graph convolution layer,
as well as a new view pooling layer making use of: a) a new hybrid Laplacian
that is adjusted based on feature distance metric learning, b) multiple
trainable representations of a feature matrix of a graph, using trainable
distance matrices, adapting the notion of views to graphs and c) a multi-view
graph aggregation scheme called graph view pooling, in order to synthesise
information from the multiple generated views. The aforementioned layers are
used in an end-to-end graph neural network architecture for graph
classification and show competitive results to other state-of-the-art methods.",988,63
"['cs.LG', 'cs.DB', 'cs.DS', 'stat.ML']",Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond,"Computing approximate nearest neighbors in high dimensional spaces is a
central problem in large-scale data mining with a wide range of applications in
machine learning and data science. A popular and effective technique in
computing nearest neighbors approximately is the locality-sensitive hashing
(LSH) scheme. In this paper, we aim to develop LSH schemes for distance
functions that measure the distance between two probability distributions,
particularly for f-divergences as well as a generalization to capture mutual
information loss. First, we provide a general framework to design LHS schemes
for f-divergence distance functions and develop LSH schemes for the generalized
Jensen-Shannon divergence and triangular discrimination in this framework. We
show a two-sided approximation result for approximation of the generalized
Jensen-Shannon divergence by the Hellinger distance, which may be of
independent interest. Next, we show a general method of reducing the problem of
designing an LSH scheme for a Krein kernel (which can be expressed as the
difference of two positive definite kernels) to the problem of maximum inner
product search. We exemplify this method by applying it to the mutual
information loss, due to its several important applications such as model
compression.",1291,80
"['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",Information Bottleneck and its Applications in Deep Learning,"Information Theory (IT) has been used in Machine Learning (ML) from early
days of this field. In the last decade, advances in Deep Neural Networks (DNNs)
have led to surprising improvements in many applications of ML. The result has
been a paradigm shift in the community toward revisiting previous ideas and
applications in this new framework. Ideas from IT are no exception. One of the
ideas which is being revisited by many researchers in this new era, is
Information Bottleneck (IB); a formulation of information extraction based on
IT. The IB is promising in both analyzing and improving DNNs. The goal of this
survey is to review the IB concept and demonstrate its applications in deep
learning. The information theoretic nature of IB, makes it also a good
candidate in showing the more general concept of how IT can be used in ML. Two
important concepts are highlighted in this narrative on the subject, i) the
concise and universal view that IT provides on seemingly unrelated methods of
ML, demonstrated by explaining how IB relates to minimal sufficient statistics,
stochastic gradient descent, and variational auto-encoders, and ii) the common
technical mistakes and problems caused by applying ideas from IT, which is
discussed by a careful study of some recent methods suffering from them.",1302,60
"['cs.CV', 'cs.LG', 'eess.IV']",Optimizing the Dice Score and Jaccard Index for Medical Image Segmentation: Theory & Practice,"The Dice score and Jaccard index are commonly used metrics for the evaluation
of segmentation tasks in medical imaging. Convolutional neural networks trained
for image segmentation tasks are usually optimized for (weighted)
cross-entropy. This introduces an adverse discrepancy between the learning
optimization objective (the loss) and the end target metric. Recent works in
computer vision have proposed soft surrogates to alleviate this discrepancy and
directly optimize the desired metric, either through relaxations (soft-Dice,
soft-Jaccard) or submodular optimization (Lov\'asz-softmax). The aim of this
study is two-fold. First, we investigate the theoretical differences in a risk
minimization framework and question the existence of a weighted cross-entropy
loss with weights theoretically optimized to surrogate Dice or Jaccard. Second,
we empirically investigate the behavior of the aforementioned loss functions
w.r.t. evaluation with Dice score and Jaccard index on five medical
segmentation tasks. Through the application of relative approximation bounds,
we show that all surrogates are equivalent up to a multiplicative factor, and
that no optimal weighting of cross-entropy exists to approximate Dice or
Jaccard measures. We validate these findings empirically and show that, while
it is important to opt for one of the target metric surrogates rather than a
cross-entropy-based loss, the choice of the surrogate does not make a
statistical difference on a wide range of medical segmentation tasks.",1515,93
['cs.CV'],Adversarial Robustness Across Representation Spaces,"Adversarial robustness corresponds to the susceptibility of deep neural
networks to imperceptible perturbations made at test time. In the context of
image tasks, many algorithms have been proposed to make neural networks robust
to adversarial perturbations made to the input pixels. These perturbations are
typically measured in an $\ell_p$ norm. However, robustness often holds only
for the specific attack used for training. In this work we extend the above
setting to consider the problem of training of deep neural networks that can be
made simultaneously robust to perturbations applied in multiple natural
representation spaces. For the case of image data, examples include the
standard pixel representation as well as the representation in the discrete
cosine transform~(DCT) basis. We design a theoretically sound algorithm with
formal guarantees for the above problem. Furthermore, our guarantees also hold
when the goal is to require robustness with respect to multiple $\ell_p$ norm
based attacks. We then derive an efficient practical implementation and
demonstrate the effectiveness of our approach on standard datasets for image
classification.",1158,51
['cs.CV'],Duo-SegNet: Adversarial Dual-Views for Semi-Supervised Medical Image Segmentation,"Segmentation of images is a long-standing challenge in medical AI. This is
mainly due to the fact that training a neural network to perform image
segmentation requires a significant number of pixel-level annotated data, which
is often unavailable. To address this issue, we propose a semi-supervised image
segmentation technique based on the concept of multi-view learning. In contrast
to the previous art, we introduce an adversarial form of dual-view training and
employ a critic to formulate the learning problem in multi-view training as a
min-max problem. Thorough quantitative and qualitative evaluations on several
datasets indicate that our proposed method outperforms state-of-the-art medical
image segmentation algorithms consistently and comfortably. The code is
publicly available at https://github.com/himashi92/Duo-SegNet",835,81
"['stat.ML', 'cs.AI', 'cs.LG']",Understanding Black-box Predictions via Influence Functions,"How can we explain the predictions of a black-box model? In this paper, we
use influence functions -- a classic technique from robust statistics -- to
trace a model's prediction through the learning algorithm and back to its
training data, thereby identifying training points most responsible for a given
prediction. To scale up influence functions to modern machine learning
settings, we develop a simple, efficient implementation that requires only
oracle access to gradients and Hessian-vector products. We show that even on
non-convex and non-differentiable models where the theory breaks down,
approximations to influence functions can still provide valuable information.
On linear models and convolutional neural networks, we demonstrate that
influence functions are useful for multiple purposes: understanding model
behavior, debugging models, detecting dataset errors, and even creating
visually-indistinguishable training-set attacks.",943,59
['cs.CV'],GREEN: a Graph REsidual rE-ranking Network for Grading Diabetic Retinopathy,"The automatic grading of diabetic retinopathy (DR) facilitates medical
diagnosis for both patients and physicians. Existing researches formulate DR
grading as an image classification problem. As the stages/categories of DR
correlate with each other, the relationship between different classes cannot be
explicitly described via a one-hot label because it is empirically estimated by
different physicians with different outcomes. This class correlation limits
existing networks to achieve effective classification. In this paper, we
propose a Graph REsidual rE-ranking Network (GREEN) to introduce a class
dependency prior into the original image classification network. The class
dependency prior is represented by a graph convolutional network with an
adjacency matrix. This prior augments image classification pipeline by
re-ranking classification results in a residual aggregation manner. Experiments
on the standard benchmarks have shown that GREEN performs favorably against
state-of-the-art approaches.",1008,75
['cs.CV'],Attentive Generative Adversarial Network for Raindrop Removal from a Single Image,"Raindrops adhered to a glass window or camera lens can severely hamper the
visibility of a background scene and degrade an image considerably. In this
paper, we address the problem by visually removing raindrops, and thus
transforming a raindrop degraded image into a clean one. The problem is
intractable, since first the regions occluded by raindrops are not given.
Second, the information about the background scene of the occluded regions is
completely lost for most part. To resolve the problem, we apply an attentive
generative network using adversarial training. Our main idea is to inject
visual attention into both the generative and discriminative networks. During
the training, our visual attention learns about raindrop regions and their
surroundings. Hence, by injecting this information, the generative network will
pay more attention to the raindrop regions and the surrounding structures, and
the discriminative network will be able to assess the local consistency of the
restored regions. This injection of visual attention to both generative and
discriminative networks is the main contribution of this paper. Our experiments
show the effectiveness of our approach, which outperforms the state of the art
methods quantitatively and qualitatively.",1264,81
"['cs.CV', 'cs.CY', 'cs.LG', 'eess.IV']",An unsupervised approach to Geographical Knowledge Discovery using street level and street network images,"Recent researches have shown the increasing use of machine learn-ing methods
in geography and urban analytics, primarily to extract features and patterns
from spatial and temporal data using a supervised approach. Researches
integrating geographical processes in machine learning models and the use of
unsupervised approacheson geographical data for knowledge discovery had been
sparse. This research contributes to the ladder, where we show how latent
variables learned from unsupervised learning methods on urbanimages can be used
for geographic knowledge discovery. In particular, we propose a simple approach
called Convolutional-PCA(ConvPCA) which are applied on both street level and
street network images to find a set of uncorrelated and ordered visual
latentcomponents. The approach allows for meaningful explanations using a
combination of geographical and generative visualisations to explore the latent
space, and to show how the learned representation can be used to predict urban
characteristics such as streetquality and street network attributes. The
research also finds that the visual components from the ConvPCA model achieves
similaraccuracy when compared to less interpretable dimension reduction
techniques.",1229,105
"['cs.CV', 'cs.LG']",End-to-end Training of CNN-CRF via Differentiable Dual-Decomposition,"Modern computer vision (CV) is often based on convolutional neural networks
(CNNs) that excel at hierarchical feature extraction. The previous generation
of CV approaches was often based on conditional random fields (CRFs) that excel
at modeling flexible higher order interactions. As their benefits are
complementary they are often combined. However, these approaches generally use
mean-field approximations and thus, arguably, did not directly optimize the
real problem. Here we revisit dual-decomposition-based approaches to CRF
optimization, an alternative to the mean-field approximation. These algorithms
can efficiently and exactly solve sub-problems and directly optimize a convex
upper bound of the real problem, providing optimality certificates on the way.
Our approach uses a novel fixed-point iteration algorithm which enjoys
dual-monotonicity, dual-differentiability and high parallelism. The whole
system, CRF and CNN can thus be efficiently trained using back-propagation. We
demonstrate the effectiveness of our system on semantic image segmentation,
showing consistent improvement over baseline models.",1120,68
"['cs.LG', 'physics.bio-ph']",A Deep Generative Artificial Intelligence system to decipher species coexistence patterns,"1. Deciphering coexistence patterns is a current challenge to understanding
diversity maintenance, especially in rich communities where the complexity of
these patterns is magnified through indirect interactions that prevent their
approximation with classical experimental approaches. 2. We explore
cutting-edge Machine Learning techniques called Generative Artificial
Intelligence (GenAI) to decipher species coexistence patterns in vegetation
patches, training generative adversarial networks (GAN) and variational
AutoEncoders (VAE) that are then used to unravel some of the mechanisms behind
community assemblage. 3. The GAN accurately reproduces the species composition
of real patches as well as the affinity of plant species to different soil
types, and the VAE also reaches a high level of accuracy, above 99%. Using the
artificially generated patches, we found that high order interactions tend to
suppress the positive effects of low order interactions. Finally, by
reconstructing successional trajectories we could identify the pioneer species
with larger potential to generate a high diversity of distinct patches in terms
of species composition. 4. Understanding the complexity of species coexistence
patterns in diverse ecological communities requires new approaches beyond
heuristic rules. Generative Artificial Intelligence can be a powerful tool to
this end as it allows to overcome the inherent dimensionality of this
challenge.",1446,89
"['cs.LG', 'stat.ML']",Debiasing Concept-based Explanations with Causal Analysis,"Concept-based explanation approach is a popular model interpertability tool
because it expresses the reasons for a model's predictions in terms of concepts
that are meaningful for the domain experts. In this work, we study the problem
of the concepts being correlated with confounding information in the features.
We propose a new causal prior graph for modeling the impacts of unobserved
variables and a method to remove the impact of confounding information and
noise using a two-stage regression technique borrowed from the instrumental
variable literature. We also model the completeness of the concepts set and
show that our debiasing method works when the concepts are not complete. Our
synthetic and real-world experiments demonstrate the success of our method in
removing biases and improving the ranking of the concepts in terms of their
contribution to the explanation of the predictions.",898,57
['cs.CV'],SDTP: Semantic-aware Decoupled Transformer Pyramid for Dense Image Prediction,"Although transformer has achieved great progress on computer vision tasks,
the scale variation in dense image prediction is still the key challenge. Few
effective multi-scale techniques are applied in transformer and there are two
main limitations in the current methods. On one hand, self-attention module in
vanilla transformer fails to sufficiently exploit the diversity of semantic
information because of its rigid mechanism. On the other hand, it is hard to
build attention and interaction among different levels due to the heavy
computational burden. To alleviate this problem, we first revisit multi-scale
problem in dense prediction, verifying the significance of diverse semantic
representation and multi-scale interaction, and exploring the adaptation of
transformer to pyramidal structure. Inspired by these findings, we propose a
novel Semantic-aware Decoupled Transformer Pyramid (SDTP) for dense image
prediction, consisting of Intra-level Semantic Promotion (ISP), Cross-level
Decoupled Interaction (CDI) and Attention Refinement Function (ARF). ISP
explores the semantic diversity in different receptive space. CDI builds the
global attention and interaction among different levels in decoupled space
which also solves the problem of heavy computation. Besides, ARF is further
added to refine the attention in transformer. Experimental results demonstrate
the validity and generality of the proposed method, which outperforms the
state-of-the-art by a significant margin in dense image prediction tasks.
Furthermore, the proposed components are all plug-and-play, which can be
embedded in other methods.",1619,77
"['stat.ML', 'cs.AI', 'cs.LG']",A Weaker Faithfulness Assumption based on Triple Interactions,"One of the core assumptions in causal discovery is the faithfulness
assumption, i.e., assuming that independencies found in the data are due to
separations in the true causal graph. This assumption can, however, be violated
in many ways, including xor connections, deterministic functions or cancelling
paths. In this work, we propose a weaker assumption that we call $2$-adjacency
faithfulness. In contrast to adjacency faithfulness, which assumes that there
is no conditional independence between each pair of variables that are
connected in the causal graph, we only require no conditional independence
between a node and a subset of its Markov blanket that can contain up to two
nodes. Equivalently, we adapt orientation faithfulness to this setting. We
further propose a sound orientation rule for causal discovery that applies
under weaker assumptions. As a proof of concept, we derive a modified Grow and
Shrink algorithm that recovers the Markov blanket of a target node and prove
its correctness under strictly weaker assumptions than the standard
faithfulness assumption.",1081,61
"['stat.ML', 'cs.LG', 'physics.bio-ph', 'physics.flu-dyn', 'q-bio.QM']",Simultaneous Coherent Structure Coloring facilitates interpretable clustering of scientific data by amplifying dissimilarity,"The clustering of data into physically meaningful subsets often requires
assumptions regarding the number, size, or shape of the subgroups. Here, we
present a new method, simultaneous coherent structure coloring (sCSC), which
accomplishes the task of unsupervised clustering without a priori guidance
regarding the underlying structure of the data. sCSC performs a sequence of
binary splittings on the dataset such that the most dissimilar data points are
required to be in separate clusters. To achieve this, we obtain a set of
orthogonal coordinates along which dissimilarity in the dataset is maximized
from a generalized eigenvalue problem based on the pairwise dissimilarity
between the data points to be clustered. This sequence of bifurcations produces
a binary tree representation of the system, from which the number of clusters
in the data and their interrelationships naturally emerge. To illustrate the
effectiveness of the method in the absence of a priori assumptions, we apply it
to three exemplary problems in fluid dynamics. Then, we illustrate its capacity
for interpretability using a high-dimensional protein folding simulation
dataset. While we restrict our examples to dynamical physical systems in this
work, we anticipate straightforward translation to other fields where existing
analysis tools require ad hoc assumptions on the data structure, lack the
interpretability of the present method, or in which the underlying processes
are less accessible, such as genomics and neuroscience.",1511,124
['cs.CV'],Improving Heterogeneous Face Recognition with Conditional Adversarial Networks,"Heterogeneous face recognition between color image and depth image is a much
desired capacity for real world applications where shape information is looked
upon as merely involved in gallery. In this paper, we propose a cross-modal
deep learning method as an effective and efficient workaround for this
challenge. Specifically, we begin with learning two convolutional neural
networks (CNNs) to extract 2D and 2.5D face features individually. Once
trained, they can serve as pre-trained models for another two-way CNN which
explores the correlated part between color and depth for heterogeneous
matching. Compared with most conventional cross-modal approaches, our method
additionally conducts accurate depth image reconstruction from single color
image with Conditional Generative Adversarial Nets (cGAN), and further enhances
the recognition performance by fusing multi-modal matching results. Through
both qualitative and quantitative experiments on benchmark FRGC 2D/3D face
database, we demonstrate that the proposed pipeline outperforms
state-of-the-art performance on heterogeneous face recognition and ensures a
drastically efficient on-line stage.",1156,78
"['cs.LG', 'stat.ML']",Time Series Classification to Improve Poultry Welfare,"Poultry farms are an important contributor to the human food chain.
Worldwide, humankind keeps an enormous number of domesticated birds (e.g.
chickens) for their eggs and their meat, providing rich sources of low-fat
protein. However, around the world, there have been growing concerns about the
quality of life for the livestock in poultry farms; and increasingly vocal
demands for improved standards of animal welfare. Recent advances in sensing
technologies and machine learning allow the possibility of automatically
assessing the health of some individual birds, and employing the lessons
learned to improve the welfare for all birds. This task superficially appears
to be easy, given the dramatic progress in recent years in classifying human
behaviors, and given that human behaviors are presumably more complex. However,
as we shall demonstrate, classifying chicken behaviors poses several unique
challenges, chief among which is creating a generalizable dictionary of
behaviors from sparse and noisy data. In this work we introduce a novel time
series dictionary learning algorithm that can robustly learn from weakly
labeled data sources.",1148,53
"['cs.LG', 'stat.ML']",Sample Complexity of Multi-task Reinforcement Learning,"Transferring knowledge across a sequence of reinforcement-learning tasks is
challenging, and has a number of important applications. Though there is
encouraging empirical evidence that transfer can improve performance in
subsequent reinforcement-learning tasks, there has been very little theoretical
analysis. In this paper, we introduce a new multi-task algorithm for a sequence
of reinforcement-learning tasks when each task is sampled independently from
(an unknown) distribution over a finite set of Markov decision processes whose
parameters are initially unknown. For this setting, we prove under certain
assumptions that the per-task sample complexity of exploration is reduced
significantly due to transfer compared to standard single-task algorithms. Our
multi-task algorithm also has the desired characteristic that it is guaranteed
not to exhibit negative transfer: in the worst case its per-task sample
complexity is comparable to the corresponding single-task algorithm.",984,54
['cs.CV'],TFPose: Direct Human Pose Estimation with Transformers,"We propose a human pose estimation framework that solves the task in the
regression-based fashion. Unlike previous regression-based methods, which often
fall behind those state-of-the-art methods, we formulate the pose estimation
task into a sequence prediction problem that can effectively be solved by
transformers. Our framework is simple and direct, bypassing the drawbacks of
the heatmap-based pose estimation. Moreover, with the attention mechanism in
transformers, our proposed framework is able to adaptively attend to the
features most relevant to the target keypoints, which largely overcomes the
feature misalignment issue of previous regression-based methods and
considerably improves the performance. Importantly, our framework can
inherently take advantages of the structured relationship between keypoints.
Experiments on the MS-COCO and MPII datasets demonstrate that our method can
significantly improve the state-of-the-art of regression-based pose estimation
and perform comparably with the best heatmap-based pose estimation methods.",1053,54
"['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'math.OC', 'stat.ML']",A Geometric Analysis of Neural Collapse with Unconstrained Features,"We provide the first global optimization landscape analysis of
$Neural\;Collapse$ -- an intriguing empirical phenomenon that arises in the
last-layer classifiers and features of neural networks during the terminal
phase of training. As recently reported by Papyan et al., this phenomenon
implies that ($i$) the class means and the last-layer classifiers all collapse
to the vertices of a Simplex Equiangular Tight Frame (ETF) up to scaling, and
($ii$) cross-example within-class variability of last-layer activations
collapses to zero. We study the problem based on a simplified
$unconstrained\;feature\;model$, which isolates the topmost layers from the
classifier of the neural network. In this context, we show that the classical
cross-entropy loss with weight decay has a benign global landscape, in the
sense that the only global minimizers are the Simplex ETFs while all other
critical points are strict saddles whose Hessian exhibit negative curvature
directions. In contrast to existing landscape analysis for deep neural networks
which is often disconnected from practice, our analysis of the simplified model
not only does it explain what kind of features are learned in the last layer,
but it also shows why they can be efficiently optimized in the simplified
settings, matching the empirical observations in practical deep network
architectures. These findings could have profound implications for
optimization, generalization, and robustness of broad interests. For example,
our experiments demonstrate that one may set the feature dimension equal to the
number of classes and fix the last-layer classifier to be a Simplex ETF for
network training, which reduces memory cost by over $20\%$ on ResNet18 without
sacrificing the generalization performance.",1766,67
"['cs.CV', 'stat.ML']",Learning Texture Manifolds with the Periodic Spatial GAN,"This paper introduces a novel approach to texture synthesis based on
generative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the
structure of the input noise distribution by constructing tensors with
different types of dimensions. We call this technique Periodic Spatial GAN
(PSGAN). The PSGAN has several novel abilities which surpass the current state
of the art in texture synthesis. First, we can learn multiple textures from
datasets of one or more complex large images. Second, we show that the image
generation with PSGANs has properties of a texture manifold: we can smoothly
interpolate between samples in the structured noise space and generate novel
samples, which lie perceptually between the textures of the original dataset.
In addition, we can also accurately learn periodical textures. We make multiple
experiments which show that PSGANs can flexibly handle diverse texture and
image data sources. Our method is highly scalable and it can generate output
images of arbitrary large size.",1021,56
"['cs.CV', 'cs.CL']",Contrastive Attention for Automatic Chest X-ray Report Generation,"Recently, chest X-ray report generation, which aims to automatically generate
descriptions of given chest X-ray images, has received growing research
interests. The key challenge of chest X-ray report generation is to accurately
capture and describe the abnormal regions. In most cases, the normal regions
dominate the entire chest X-ray image, and the corresponding descriptions of
these normal regions dominate the final report. Due to such data bias,
learning-based models may fail to attend to abnormal regions. In this work, to
effectively capture and describe abnormal regions, we propose the Contrastive
Attention (CA) model. Instead of solely focusing on the current input image,
the CA model compares the current input image with normal images to distill the
contrastive information. The acquired contrastive information can better
represent the visual features of abnormal regions. According to the experiments
on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into
several existing models can boost their performance across most metrics. In
addition, according to the analysis, the CA model can help existing models
better attend to the abnormal regions and provide more accurate descriptions
which are crucial for an interpretable diagnosis. Specifically, we achieve the
state-of-the-art results on the two public datasets.",1353,65
['cs.CV'],Instance-aware Remote Sensing Image Captioning with Cross-hierarchy Attention,"The spatial attention is a straightforward approach to enhance the
performance for remote sensing image captioning. However, conventional spatial
attention approaches consider only the attention distribution on one fixed
coarse grid, resulting in the semantics of tiny objects can be easily ignored
or disturbed during the visual feature extraction. Worse still, the fixed
semantic level of conventional spatial attention limits the image understanding
in different levels and perspectives, which is critical for tackling the huge
diversity in remote sensing images. To address these issues, we propose a
remote sensing image caption generator with instance-awareness and
cross-hierarchy attention. 1) The instances awareness is achieved by
introducing a multi-level feature architecture that contains the visual
information of multi-level instance-possible regions and their surroundings. 2)
Moreover, based on this multi-level feature extraction, a cross-hierarchy
attention mechanism is proposed to prompt the decoder to dynamically focus on
different semantic hierarchies and instances at each time step. The
experimental results on public datasets demonstrate the superiority of proposed
approach over existing methods.",1224,77
"['cs.CV', 'cs.NE']",Generative Adversarial Residual Pairwise Networks for One Shot Learning,"Deep neural networks achieve unprecedented performance levels over many tasks
and scale well with large quantities of data, but performance in the low-data
regime and tasks like one shot learning still lags behind. While recent work
suggests many hypotheses from better optimization to more complicated network
structures, in this work we hypothesize that having a learnable and more
expressive similarity objective is an essential missing component. Towards
overcoming that, we propose a network design inspired by deep residual networks
that allows the efficient computation of this more expressive pairwise
similarity objective. Further, we argue that regularization is key in learning
with small amounts of data, and propose an additional generator network based
on the Generative Adversarial Networks where the discriminator is our residual
pairwise network. This provides a strong regularizer by leveraging the
generated data samples. The proposed model can generate plausible variations of
exemplars over unseen classes and outperforms strong discriminative baselines
for few shot classification tasks. Notably, our residual pairwise network
design outperforms previous state-of-theart on the challenging mini-Imagenet
dataset for one shot learning by getting over 55% accuracy for the 5-way
classification task over unseen classes.",1339,71
['cs.CV'],Band selection with Higher Order Multivariate Cumulants for small target detection in hyperspectral images,"In the small target detection problem a pattern to be located is on the order
of magnitude less numerous than other patterns present in the dataset. This
applies both to the case of supervised detection, where the known template is
expected to match in just a few areas and unsupervised anomaly detection, as
anomalies are rare by definition. This problem is frequently related to the
imaging applications, i.e. detection within the scene acquired by a camera. To
maximize available data about the scene, hyperspectral cameras are used; at
each pixel, they record spectral data in hundreds of narrow bands.
  The typical feature of hyperspectral imaging is that characteristic
properties of target materials are visible in the small number of bands, where
light of certain wavelength interacts with characteristic molecules. A
target-independent band selection method based on statistical principles is a
versatile tool for solving this problem in different practical applications.
  Combination of a regular background and a rare standing out anomaly will
produce a distortion in the joint distribution of hyperspectral pixels. Higher
Order Cumulants Tensors are a natural `window' into this distribution, allowing
to measure properties and suggest candidate bands for removal. While there have
been attempts at producing band selection algorithms based on the 3 rd
cumulant's tensor i.e. the joint skewness, the literature lacks a systematic
analysis of how the order of the cumulant tensor used affects effectiveness of
band selection in detection applications. In this paper we present an analysis
of a general algorithm for band selection based on higher order cumulants. We
discuss its usability related to the observed breaking points in performance,
depending both on method order and the desired number of bands. Finally we
perform experiments and evaluate these methods in a hyperspectral detection
scenario.",1918,106
['cs.CV'],3D Semantic Scene Completion from a Single Depth Image using Adversarial Training,"We address the task of 3D semantic scene completion, i.e. , given a single
depth image, we predict the semantic labels and occupancy of voxels in a 3D
grid representing the scene. In light of the recently introduced generative
adversarial networks (GAN), our goal is to explore the potential of this model
and the efficiency of various important design choices. Our results show that
using conditional GANs outperforms the vanilla GAN setup. We evaluate these
architecture designs on several datasets. Based on our experiments, we
demonstrate that GANs are able to outperform the performance of a baseline 3D
CNN in case of clean annotations, but they suffer from poorly aligned
annotations.",691,81
"['cs.CV', 'cs.LG', 'eess.IV']",Deep Diffusion Processes for Active Learning of Hyperspectral Images,"A method for active learning of hyperspectral images (HSI) is proposed, which
combines deep learning with diffusion processes on graphs. A deep variational
autoencoder extracts smoothed, denoised features from a high-dimensional HSI,
which are then used to make labeling queries based on graph diffusion
processes. The proposed method combines the robust representations of deep
learning with the mathematical tractability of diffusion geometry, and leads to
strong performance on real HSI.",490,68
"['cs.CV', 'cs.LG']",Segmentation of cell-level anomalies in electroluminescence images of photovoltaic modules,"In the operation & maintenance (O&M) of photovoltaic (PV) plants, the early
identification of failures has become crucial to maintain productivity and
prolong components' life. Of all defects, cell-level anomalies can lead to
serious failures and may affect surrounding PV modules in the long run. These
fine defects are usually captured with high spatial resolution
electroluminescence (EL) imaging. The difficulty of acquiring such images has
limited the availability of data. For this work, multiple data resources and
augmentation techniques have been used to surpass this limitation. Current
state-of-the-art detection methods extract barely low-level information from
individual PV cell images, and their performance is conditioned by the
available training data. In this article, we propose an end-to-end deep
learning pipeline that detects, locates and segments cell-level anomalies from
entire photovoltaic modules via EL images. The proposed modular pipeline
combines three deep learning techniques: 1. object detection (modified
Faster-RNN), 2. image classification (EfficientNet) and 3. weakly supervised
segmentation (autoencoder). The modular nature of the pipeline allows to
upgrade the deep learning models to the further improvements in the
state-of-the-art and also extend the pipeline towards new functionalities.",1332,90
['cs.CV'],HyperCon: Image-To-Video Model Transfer for Video-To-Video Translation Tasks,"Video-to-video translation is more difficult than image-to-image translation
due to the temporal consistency problem that, if unaddressed, leads to
distracting flickering effects. Although video models designed from scratch
produce temporally consistent results, training them to match the vast visual
knowledge captured by image models requires an intractable number of videos. To
combine the benefits of image and video models, we propose an image-to-video
model transfer method called Hyperconsistency (HyperCon) that transforms any
well-trained image model into a temporally consistent video model without
fine-tuning. HyperCon works by translating a temporally interpolated video
frame-wise and then aggregating over temporally localized windows on the
interpolated video. It handles both masked and unmasked inputs, enabling
support for even more video-to-video translation tasks than prior
image-to-video model transfer techniques. We demonstrate HyperCon on video
style transfer and inpainting, where it performs favorably compared to prior
state-of-the-art methods without training on a single stylized or incomplete
video. Our project website is available at
https://ryanszeto.com/projects/hypercon .",1210,76
['cs.CV'],Image-to-image translation for cross-domain disentanglement,"Deep image translation methods have recently shown excellent results,
outputting high-quality images covering multiple modes of the data
distribution. There has also been increased interest in disentangling the
internal representations learned by deep methods to further improve their
performance and achieve a finer control. In this paper, we bridge these two
objectives and introduce the concept of cross-domain disentanglement. We aim to
separate the internal representation into three parts. The shared part contains
information for both domains. The exclusive parts, on the other hand, contain
only factors of variation that are particular to each domain. We achieve this
through bidirectional image translation based on Generative Adversarial
Networks and cross-domain autoencoders, a novel network component. Our model
offers multiple advantages. We can output diverse samples covering multiple
modes of the distributions of both domains, perform domain-specific image
transfer and interpolation, and cross-domain retrieval without the need of
labeled data, only paired images. We compare our model to the state-of-the-art
in multi-modal image translation and achieve better results for translation on
challenging datasets as well as for cross-domain retrieval on realistic
datasets.",1290,59
"['cs.CV', 'cs.LG', 'stat.ML']",Improving Deep Image Clustering With Spatial Transformer Layers,"Image clustering is an important but challenging task in machine learning. As
in most image processing areas, the latest improvements came from models based
on the deep learning approach. However, classical deep learning methods have
problems to deal with spatial image transformations like scale and rotation. In
this paper, we propose the use of visual attention techniques to reduce this
problem in image clustering methods. We evaluate the combination of a deep
image clustering model called Deep Adaptive Clustering (DAC) with the Spatial
Transformer Networks (STN). The proposed model is evaluated in the datasets
MNIST and FashionMNIST and outperformed the baseline model.",679,63
['cs.CV'],Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts,"A few-shot font generation (FFG) method has to satisfy two objectives: the
generated images should preserve the underlying global structure of the target
character and present the diverse local reference style. Existing FFG methods
aim to disentangle content and style either by extracting a universal
representation style or extracting multiple component-wise style
representations. However, previous methods either fail to capture diverse local
styles or cannot be generalized to a character with unseen components, e.g.,
unseen language systems. To mitigate the issues, we propose a novel FFG method,
named Multiple Localized Experts Few-shot Font Generation Network (MX-Font).
MX-Font extracts multiple style features not explicitly conditioned on
component labels, but automatically by multiple experts to represent different
local concepts, e.g., left-side sub-glyph. Owing to the multiple experts,
MX-Font can capture diverse local concepts and show the generalizability to
unseen languages. During training, we utilize component labels as weak
supervision to guide each expert to be specialized for different local
concepts. We formulate the component assign problem to each expert as the graph
matching problem, and solve it by the Hungarian algorithm. We also employ the
independence loss and the content-style adversarial loss to impose the
content-style disentanglement. In our experiments, MX-Font outperforms previous
state-of-the-art FFG methods in the Chinese generation and cross-lingual, e.g.,
Chinese to Korean, generation. Source code is available at
https://github.com/clovaai/mxfont.",1605,92
"['cs.CV', 'cs.LG']",Finger-GAN: Generating Realistic Fingerprint Images Using Connectivity Imposed GAN,"Generating realistic biometric images has been an interesting and, at the
same time, challenging problem. Classical statistical models fail to generate
realistic-looking fingerprint images, as they are not powerful enough to
capture the complicated texture representation in fingerprint images. In this
work, we present a machine learning framework based on generative adversarial
networks (GAN), which is able to generate fingerprint images sampled from a
prior distribution (learned from a set of training images). We also add a
suitable regularization term to the loss function, to impose the connectivity
of generated fingerprint images. This is highly desirable for fingerprints, as
the lines in each finger are usually connected. We apply this framework to two
popular fingerprint databases, and generate images which look very realistic,
and similar to the samples in those databases. Through experimental results, we
show that the generated fingerprint images have a good diversity, and are able
to capture different parts of the prior distribution. We also evaluate the
Frechet Inception distance (FID) of our proposed model, and show that our model
is able to achieve good quantitative performance in terms of this score.",1231,82
"['cs.CV', 'eess.IV']",Progressive Perception-Oriented Network for Single Image Super-Resolution,"Recently, it has been demonstrated that deep neural networks can
significantly improve the performance of single image super-resolution (SISR).
Numerous studies have concentrated on raising the quantitative quality of
super-resolved (SR) images. However, these methods that target PSNR
maximization usually produce blurred images at large upscaling factor. The
introduction of generative adversarial networks (GANs) can mitigate this issue
and show impressive results with synthetic high-frequency textures.
Nevertheless, these GAN-based approaches always have a tendency to add fake
textures and even artifacts to make the SR image of visually higher-resolution.
In this paper, we propose a novel perceptual image super-resolution method that
progressively generates visually high-quality results by constructing a
stage-wise network. Specifically, the first phase concentrates on minimizing
pixel-wise error, and the second stage utilizes the features extracted by the
previous stage to pursue results with better structural retention. The final
stage employs fine structure features distilled by the second phase to produce
more realistic results. In this way, we can maintain the pixel, and structural
level information in the perceptual image as much as possible. It is useful to
note that the proposed method can build three types of images in a feed-forward
process. Also, we explore a new generator that adopts multi-scale hierarchical
features fusion. Extensive experiments on benchmark datasets show that our
approach is superior to the state-of-the-art methods. Code is available at
https://github.com/Zheng222/PPON.",1627,73
"['cs.CV', 'physics.ao-ph']",Application of Multi-channel 3D-cube Successive Convolution Network for Convective Storm Nowcasting,"Convective storm nowcasting has attracted substantial attention in various
fields. Existing methods under a deep learning framework rely primarily on
radar data. Although they perform nowcast storm advection well, it is still
challenging to nowcast storm initiation and growth, due to the limitations of
the radar observations. This paper describes the first attempt to nowcast storm
initiation, growth, and advection simultaneously under a deep learning
framework using multi-source meteorological data. To this end, we present a
multi-channel 3D-cube successive convolution network (3D-SCN). As real-time
re-analysis meteorological data can now provide valuable atmospheric boundary
layer thermal dynamic information, which is essential to predict storm
initiation and growth, both raw 3D radar and re-analysis data are used directly
without any handcraft feature engineering. These data are formulated as
multi-channel 3D cubes, to be fed into our network, which are convolved by
cross-channel 3D convolutions. By stacking successive convolutional layers
without pooling, we build an end-to-end trainable model for nowcasting.
Experimental results show that deep learning methods achieve better performance
than traditional extrapolation methods. The qualitative analyses of 3D-SCN show
encouraging results of nowcasting of storm initiation, growth, and advection.",1367,99
['cs.CV'],Joint and Progressive Subspace Analysis (JPSA) with Spatial-Spectral Manifold Alignment for Semi-Supervised Hyperspectral Dimensionality Reduction,"Conventional nonlinear subspace learning techniques (e.g., manifold learning)
usually introduce some drawbacks in explainability (explicit mapping) and
cost-effectiveness (linearization), generalization capability (out-of-sample),
and representability (spatial-spectral discrimination). To overcome these
shortcomings, a novel linearized subspace analysis technique with
spatial-spectral manifold alignment is developed for a semi-supervised
hyperspectral dimensionality reduction (HDR), called joint and progressive
subspace analysis (JPSA). The JPSA learns a high-level, semantically
meaningful, joint spatial-spectral feature representation from hyperspectral
data by 1) jointly learning latent subspaces and a linear classifier to find an
effective projection direction favorable for classification; 2) progressively
searching several intermediate states of subspaces to approach an optimal
mapping from the original space to a potential more discriminative subspace; 3)
spatially and spectrally aligning manifold structure in each learned latent
subspace in order to preserve the same or similar topological property between
the compressed data and the original data. A simple but effective classifier,
i.e., nearest neighbor (NN), is explored as a potential application for
validating the algorithm performance of different HDR approaches. Extensive
experiments are conducted to demonstrate the superiority and effectiveness of
the proposed JPSA on two widely-used hyperspectral datasets: Indian Pines
(92.98\%) and the University of Houston (86.09\%) in comparison with previous
state-of-the-art HDR methods. The demo of this basic work (i.e., ECCV2018) is
openly available at https://github.com/danfenghong/ECCV2018_J-Play.",1731,146
['cs.CV'],Frame Difference-Based Temporal Loss for Video Stylization,"Neural style transfer models have been used to stylize an ordinary video to
specific styles. To ensure temporal inconsistency between the frames of the
stylized video, a common approach is to estimate the optic flow of the pixels
in the original video and make the generated pixels match the estimated optical
flow. This is achieved by minimizing an optical flow-based (OFB) loss during
model training. However, optical flow estimation is itself a challenging task,
particularly in complex scenes. In addition, it incurs a high computational
cost. We propose a much simpler temporal loss called the frame difference-based
(FDB) loss to solve the temporal inconsistency problem. It is defined as the
distance between the difference between the stylized frames and the difference
between the original frames. The differences between the two frames are
measured in both the pixel space and the feature space specified by the
convolutional neural networks. A set of human behavior experiments involving 62
subjects with 25,600 votes showed that the performance of the proposed FDB loss
matched that of the OFB loss. The performance was measured by subjective
evaluation of stability and stylization quality of the generated videos on two
typical video stylization models. The results suggest that the proposed FDB
loss is a strong alternative to the commonly used OFB loss for video
stylization.",1391,58
"['cs.LG', 'stat.ML']",Comparison and Unification of Three Regularization Methods in Batch Reinforcement Learning,"In batch reinforcement learning, there can be poorly explored state-action
pairs resulting in poorly learned, inaccurate models and poorly performing
associated policies. Various regularization methods can mitigate the problem of
learning overly-complex models in Markov decision processes (MDPs), however
they operate in technically and intuitively distinct ways and lack a common
form in which to compare them. This paper unifies three regularization methods
in a common framework -- a weighted average transition matrix. Considering
regularization methods in this common form illuminates how the MDP structure
and the state-action pair distribution of the batch data set influence the
relative performance of regularization methods. We confirm intuitions generated
from the common framework by empirical evaluation across a range of MDPs and
data collection policies.",870,90
['cs.CV'],Fast Non-local Stereo Matching based on Hierarchical Disparity Prediction,"Stereo matching is the key step in estimating depth from two or more images.
Recently, some tree-based non-local stereo matching methods have been proposed,
which achieved state-of-the-art performance. The algorithms employed some tree
structures to aggregate cost and thus improved the performance and reduced the
coputation load of the stereo matching. However, the computational complexity
of these tree-based algorithms is still high because they search over the
entire disparity range. In addition, the extreme greediness of the minimum
spanning tree (MST) causes the poor performance in large areas with similar
colors but varying disparities. In this paper, we propose an efficient stereo
matching method using a hierarchical disparity prediction (HDP) framework to
dramatically reduce the disparity search range so as to speed up the tree-based
non-local stereo methods. Our disparity prediction scheme works on a graph
pyramid derived from an image whose disparity to be estimated. We utilize the
disparity of a upper graph to predict a small disparity range for the lower
graph. Some independent disparity trees (DT) are generated to form a disparity
prediction forest (HDPF) over which the cost aggregation is made. When combined
with the state-of-the-art tree-based methods, our scheme not only dramatically
speeds up the original methods but also improves their performance by
alleviating the second drawback of the tree-based methods. This is partially
because our DTs overcome the extreme greediness of the MST. Extensive
experimental results on some benchmark datasets demonstrate the effectiveness
and efficiency of our framework. For example, the segment-tree based stereo
matching becomes about 25.57 times faster and 2.2% more accurate over the
Middlebury 2006 full-size dataset.",1799,73
"['cs.LG', 'cs.AI', 'stat.ML']",An Empirical Study on Hyperparameters and their Interdependence for RL Generalization,"Recent results in Reinforcement Learning (RL) have shown that agents with
limited training environments are susceptible to a large amount of overfitting
across many domains. A key challenge for RL generalization is to quantitatively
explain the effects of changing parameters on testing performance. Such
parameters include architecture, regularization, and RL-dependent variables
such as discount factor and action stochasticity. We provide empirical results
that show complex and interdependent relationships between hyperparameters and
generalization. We further show that several empirical metrics such as gradient
cosine similarity and trajectory-dependent metrics serve to provide intuition
towards these results.",719,85
"['cs.LG', 'cs.DS', 'cs.IR', 'stat.ML']",The Boundary Forest Algorithm for Online Supervised and Unsupervised Learning,"We describe a new instance-based learning algorithm called the Boundary
Forest (BF) algorithm, that can be used for supervised and unsupervised
learning. The algorithm builds a forest of trees whose nodes store previously
seen examples. It can be shown data points one at a time and updates itself
incrementally, hence it is naturally online. Few instance-based algorithms have
this property while being simultaneously fast, which the BF is. This is crucial
for applications where one needs to respond to input data in real time. The
number of children of each node is not set beforehand but obtained from the
training procedure, which makes the algorithm very flexible with regards to
what data manifolds it can learn. We test its generalization performance and
speed on a range of benchmark datasets and detail in which settings it
outperforms the state of the art. Empirically we find that training time scales
as O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and N
the amount of data,",1011,77
"['cs.LG', 'cs.RO', 'cs.SY', 'stat.ML']",Decoupled Data Based Approach for Learning to Control Nonlinear Dynamical Systems,"This paper addresses the problem of learning the optimal control policy for a
nonlinear stochastic dynamical system with continuous state space, continuous
action space and unknown dynamics. This class of problems are typically
addressed in stochastic adaptive control and reinforcement learning literature
using model-based and model-free approaches respectively. Both methods rely on
solving a dynamic programming problem, either directly or indirectly, for
finding the optimal closed loop control policy. The inherent `curse of
dimensionality' associated with dynamic programming method makes these
approaches also computationally difficult.
  This paper proposes a novel decoupled data-based control (D2C) algorithm that
addresses this problem using a decoupled, `open loop - closed loop', approach.
First, an open-loop deterministic trajectory optimization problem is solved
using a black-box simulation model of the dynamical system. Then, a closed loop
control is developed around this open loop trajectory by linearization of the
dynamics about this nominal trajectory. By virtue of linearization, a linear
quadratic regulator based algorithm can be used for this closed loop control.
We show that the performance of D2C algorithm is approximately optimal.
Moreover, simulation performance suggests significant reduction in training
time compared to other state of the art algorithms.",1392,81
['cs.CV'],Instance Flow Based Online Multiple Object Tracking,"We present a method to perform online Multiple Object Tracking (MOT) of known
object categories in monocular video data. Current Tracking-by-Detection MOT
approaches build on top of 2D bounding box detections. In contrast, we exploit
state-of-the-art instance aware semantic segmentation techniques to compute 2D
shape representations of target objects in each frame. We predict position and
shape of segmented instances in subsequent frames by exploiting optical flow
cues. We define an affinity matrix between instances of subsequent frames which
reflects locality and visual similarity. The instance association is solved by
applying the Hungarian method. We evaluate different configurations of our
algorithm using the MOT 2D 2015 train dataset. The evaluation shows that our
tracking approach is able to track objects with high relative motions. In
addition, we provide results of our approach on the MOT 2D 2015 test set for
comparison with previous works. We achieve a MOTA score of 32.1.",995,51
['cs.CV'],YinYang-Net: Complementing Face and Body Information for Wild Gender Recognition,"Soft biometrics inference in surveillance scenarios is a topic of interest
for various applications, particularly in security-related areas. However, soft
biometric analysis is not extensively reported in wild conditions. In
particular, previous works on gender recognition report their results in face
datasets, with relatively good image quality and frontal poses. Given the
uncertainty of the availability of the facial region in wild conditions, we
consider that these methods are not adequate for surveillance settings. To
overcome these limitations, we: 1) present frontal and wild face versions of
three well-known surveillance datasets; and 2) propose YinYang-Net (YY-Net), a
model that effectively and dynamically complements facial and body information,
which makes it suitable for gender recognition in wild conditions. The frontal
and wild face datasets derive from widely used Pedestrian Attribute Recognition
(PAR) sets (PETA, PA-100K, and RAP), using a pose-based approach to filter the
frontal samples and facial regions. This approach retrieves the facial region
of images with varying image/subject conditions, where the state-of-the-art
face detectors often fail. YY-Net combines facial and body information through
a learnable fusion matrix and a channel-attention sub-network, focusing on the
most influential body parts according to the specific image/subject features.
We compare it with five PAR methods, consistently obtaining state-of-the-art
results on gender recognition, and reducing the prediction errors by up to 24%
in frontal samples. The announced PAR datasets versions and YY-Net serve as the
basis for wild soft biometrics classification and are available in
https://github.com/Tiago-Roxo.",1725,80
['cs.CV'],Adversarial Text-to-Image Synthesis: A Review,"With the advent of generative adversarial networks, synthesizing images from
textual descriptions has recently become an active research area. It is a
flexible and intuitive way for conditional image generation with significant
progress in the last years regarding visual realism, diversity, and semantic
alignment. However, the field still faces several challenges that require
further research efforts such as enabling the generation of high-resolution
images with multiple objects, and developing suitable and reliable evaluation
metrics that correlate with human judgement. In this review, we contextualize
the state of the art of adversarial text-to-image synthesis models, their
development since their inception five years ago, and propose a taxonomy based
on the level of supervision. We critically examine current strategies to
evaluate text-to-image synthesis models, highlight shortcomings, and identify
new areas of research, ranging from the development of better datasets and
evaluation metrics to possible improvements in architectural design and model
training. This review complements previous surveys on generative adversarial
networks with a focus on text-to-image synthesis which we believe will help
researchers to further advance the field.",1262,45
"['cs.LG', 'cs.SE']",HOMRS: High Order Metamorphic Relations Selector for Deep Neural Networks,"Deep Neural Networks (DNN) applications are increasingly becoming a part of
our everyday life, from medical applications to autonomous cars. Traditional
validation of DNN relies on accuracy measures, however, the existence of
adversarial examples has highlighted the limitations of these accuracy
measures, raising concerns especially when DNN are integrated into
safety-critical systems. In this paper, we present HOMRS, an approach to boost
metamorphic testing by automatically building a small optimized set of high
order metamorphic relations from an initial set of elementary metamorphic
relations. HOMRS' backbone is a multi-objective search; it exploits ideas drawn
from traditional systems testing such as code coverage, test case, and path
diversity. We applied HOMRS to LeNet5 DNN with MNIST dataset and we report
evidence that it builds a small but effective set of high order transformations
achieving a 95% kill ratio. Five raters manually labeled a pool of images
before and after high order transformation; Fleiss' Kappa and statistical tests
confirmed that they are metamorphic properties. HOMRS built-in relations are
also effective to confront adversarial or out-of-distribution examples; HOMRS
detected 92% of randomly sampled out-of-distribution images. HOMRS
transformations are also suitable for online real-time use.",1339,73
"['cs.LG', 'stat.ML']",Pruning Algorithms to Accelerate Convolutional Neural Networks for Edge Applications: A Survey,"With the general trend of increasing Convolutional Neural Network (CNN) model
sizes, model compression and acceleration techniques have become critical for
the deployment of these models on edge devices. In this paper, we provide a
comprehensive survey on Pruning, a major compression strategy that removes
non-critical or redundant neurons from a CNN model. The survey covers the
overarching motivation for pruning, different strategies and criteria, their
advantages and drawbacks, along with a compilation of major pruning techniques.
We conclude the survey with a discussion on alternatives to pruning and current
challenges for the model compression community.",665,94
"['cs.CV', 'eess.IV']",A Robust Method for Image Stitching,"We propose a novel method for large-scale image stitching that is robust
against repetitive patterns and featureless regions in the imagery. In such
cases, state-of-the-art image stitching methods easily produce image alignment
artifacts, since they may produce false pairwise image registrations that are
in conflict within the global connectivity graph. Our method augments the
current methods by collecting all the plausible pairwise image registration
candidates, among which globally consistent candidates are chosen. This enables
the stitching process to determine the correct pairwise registrations by
utilizing all the available information from the whole imagery, such as
unambiguous registrations outside the repeating pattern and featureless
regions. We formalize the method as a weighted multigraph whose nodes represent
the individual image transformations from the composite image, and whose sets
of multiple edges between two nodes represent all the plausible transformations
between the pixel coordinates of the two images. The edge weights represent the
plausibility of the transformations. The image transformations and the edge
weights are solved from a non-linear minimization problem with linear
constraints, for which a projection method is used. As an example, we apply the
method in a large-scale scanning application where the transformations are
primarily translations with only slight rotation and scaling component. Despite
these simplifications, the state-of-the-art methods do not produce adequate
results in such applications, since the image overlap is small, which can be
featureless or repetitive, and misalignment artifacts and their concealment are
unacceptable.",1698,35
"['cs.CV', 'q-fin.CP', 'q-fin.TR']",Trading via Image Classification,"The art of systematic financial trading evolved with an array of approaches,
ranging from simple strategies to complex algorithms all relying, primary, on
aspects of time-series analysis. Recently, after visiting the trading floor of
a leading financial institution, we noticed that traders always execute their
trade orders while observing images of financial time-series on their screens.
In this work, we built upon the success in image recognition and examine the
value in transforming the traditional time-series analysis to that of image
classification. We create a large sample of financial time-series images
encoded as candlestick (Box and Whisker) charts and label the samples following
three algebraically-defined binary trade strategies. Using the images, we train
over a dozen machine-learning classification models and find that the
algorithms are very efficient in recovering the complicated, multiscale
label-generating rules when the data is represented visually. We suggest that
the transformation of continuous numeric time-series classification problem to
a vision problem is useful for recovering signals typical of technical
analysis.",1156,32
"['cs.LG', 'cs.AI', 'stat.ML']",Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration,"In this paper, sample-aware policy entropy regularization is proposed to
enhance the conventional policy entropy regularization for better exploration.
Exploiting the sample distribution obtainable from the replay buffer, the
proposed sample-aware entropy regularization maximizes the entropy of the
weighted sum of the policy action distribution and the sample action
distribution from the replay buffer for sample-efficient exploration. A
practical algorithm named diversity actor-critic (DAC) is developed by applying
policy iteration to the objective function with the proposed sample-aware
entropy regularization. Numerical results show that DAC significantly
outperforms existing recent algorithms for reinforcement learning.",731,92
"['cs.LG', 'stat.ML']",Doubly-stochastic mining for heterogeneous retrieval,"Modern retrieval problems are characterised by training sets with potentially
billions of labels, and heterogeneous data distributions across subpopulations
(e.g., users of a retrieval system may be from different countries), each of
which poses a challenge. The first challenge concerns scalability: with a large
number of labels, standard losses are difficult to optimise even on a single
example. The second challenge concerns uniformity: one ideally wants good
performance on each subpopulation. While several solutions have been proposed
to address the first challenge, the second challenge has received relatively
less attention. In this paper, we propose doubly-stochastic mining (S2M ), a
stochastic optimization technique that addresses both challenges. In each
iteration of S2M, we compute a per-example loss based on a subset of hardest
labels, and then compute the minibatch loss based on the hardest examples. We
show theoretically and empirically that by focusing on the hardest examples,
S2M ensures that all data subpopulations are modelled well.",1062,52
"['cs.CV', 'eess.IV']",Online Multi-Object Tracking with delta-GLMB Filter based on Occlusion and Identity Switch Handling,"In this paper, we propose an online multi-object tracking (MOT) method in a
delta Generalized Labeled Multi-Bernoulli (delta-GLMB) filter framework to
address occlusion and miss-detection issues, reduce false alarms, and recover
identity switch (ID switch). To handle occlusion and miss-detection issues, we
propose a measurement-to-disappeared track association method based on one-step
delta-GLMB filter, so it is possible to manage these difficulties by jointly
processing occluded or miss-detected objects. This part of proposed method is
based on a proposed similarity metric which is responsible for defining the
weight of hypothesized reappeared tracks. We also extend the delta-GLMB filter
to efficiently recover switched IDs using the cardinality density, size and
color features of the hypothesized tracks. We also propose a novel birth model
to achieve more effective clutter removal performance. In both
occlusion/miss-detection handler and newly-birthed object detector sections of
the proposed method, unassigned measurements play a significant role, since
they are used as the candidates for reappeared or birth objects. In addition,
we perform an ablation study which confirms the effectiveness of our
contributions in comparison with the baseline method. We evaluate the proposed
method on well-known and publicly available MOT15 and MOT17 test datasets which
are focused on pedestrian tracking. Experimental results show that the proposed
tracker performs better or at least at the same level of the state-of-the-art
online and offline MOT methods. It effectively handles the occlusion and ID
switch issues and reduces false alarms as well.",1658,99
"['cs.LG', 'eess.SP', 'stat.ML']",Unsupervised Ensemble Classification with Sequential and Networked Data,"Ensemble learning, the machine learning paradigm where multiple algorithms
are combined, has exhibited promising perfomance in a variety of tasks. The
present work focuses on unsupervised ensemble classification. The term
unsupervised refers to the ensemble combiner who has no knowledge of the
ground-truth labels that each classifier has been trained on. While most prior
works on unsupervised ensemble classification are designed for independent and
identically distributed (i.i.d.) data, the present work introduces an
unsupervised scheme for learning from ensembles of classifiers in the presence
of data dependencies. Two types of data dependencies are considered: sequential
data and networked data whose dependencies are captured by a graph. Moment
matching and Expectation Maximization algorithms are developed for the
aforementioned cases, and their performance is evaluated on synthetic and real
datasets.",916,71
['cs.CV'],FCSR-GAN: Joint Face Completion and Super-resolution via Multi-task Learning,"Combined variations containing low-resolution and occlusion often present in
face images in the wild, e.g., under the scenario of video surveillance. While
most of the existing face image recovery approaches can handle only one type of
variation per model, in this work, we propose a deep generative adversarial
network (FCSR-GAN) for performing joint face completion and face
super-resolution via multi-task learning. The generator of FCSR-GAN aims to
recover a high-resolution face image without occlusion given an input
low-resolution face image with occlusion. The discriminator of FCSR-GAN uses a
set of carefully designed losses (an adversarial loss, a perceptual loss, a
pixel loss, a smooth loss, a style loss, and a face prior loss) to assure the
high quality of the recovered high-resolution face images without occlusion.
The whole network of FCSR-GAN can be trained end-to-end using our two-stage
training strategy. Experimental results on the public-domain CelebA and Helen
databases show that the proposed approach outperforms the state-of-the-art
methods in jointly performing face super-resolution (up to 8 $\times$) and face
completion, and shows good generalization ability in cross-database testing.
Our FCSR-GAN is also useful for improving face identification performance when
there are low-resolution and occlusion in face images.",1352,76
['cs.LG'],Reinforcement Learning Under Algorithmic Triage,"Methods to learn under algorithmic triage have predominantly focused on
supervised learning settings where each decision, or prediction, is independent
of each other. Under algorithmic triage, a supervised learning model predicts a
fraction of the instances and humans predict the remaining ones. In this work,
we take a first step towards developing reinforcement learning models that are
optimized to operate under algorithmic triage. To this end, we look at the
problem through the framework of options and develop a two-stage actor-critic
method to learn reinforcement learning models under triage. The first stage
performs offline, off-policy training using human data gathered in an
environment where the human has operated on their own. The second stage
performs on-policy training to account for the impact that switching may have
on the human policy, which may be difficult to anticipate from the above human
data. Extensive simulation experiments in a synthetic car driving task show
that the machine models and the triage policies trained using our two-stage
method effectively complement human policies and outperform those provided by
several competitive baselines.",1178,47
"['cs.CV', 'eess.IV']",Deep Optics for Monocular Depth Estimation and 3D Object Detection,"Depth estimation and 3D object detection are critical for scene understanding
but remain challenging to perform with a single image due to the loss of 3D
information during image capture. Recent models using deep neural networks have
improved monocular depth estimation performance, but there is still difficulty
in predicting absolute depth and generalizing outside a standard dataset. Here
we introduce the paradigm of deep optics, i.e. end-to-end design of optics and
image processing, to the monocular depth estimation problem, using coded
defocus blur as an additional depth cue to be decoded by a neural network. We
evaluate several optical coding strategies along with an end-to-end
optimization scheme for depth estimation on three datasets, including NYU Depth
v2 and KITTI. We find an optimized freeform lens design yields the best
results, but chromatic aberration from a singlet lens offers significantly
improved performance as well. We build a physical prototype and validate that
chromatic aberrations improve depth estimation on real-world results. In
addition, we train object detection networks on the KITTI dataset and show that
the lens optimized for depth estimation also results in improved 3D object
detection performance.",1245,66
['cs.LG'],Are all outliers alike? On Understanding the Diversity of Outliers for Detecting OODs,"Deep neural networks (DNNs) are known to produce incorrect predictions with
very high confidence on out-of-distribution (OOD) inputs. This limitation is
one of the key challenges in the adoption of deep learning models in
high-assurance systems such as autonomous driving, air traffic management, and
medical diagnosis. This challenge has received significant attention recently,
and several techniques have been developed to detect inputs where the model's
prediction cannot be trusted. These techniques use different statistical,
geometric, or topological signatures. This paper presents a taxonomy of OOD
outlier inputs based on their source and nature of uncertainty. We demonstrate
how different existing detection approaches fail to detect certain types of
outliers. We utilize these insights to develop a novel integrated detection
approach that uses multiple attributes corresponding to different types of
outliers. Our results include experiments on CIFAR10, SVHN and MNIST as
in-distribution data and Imagenet, LSUN, SVHN (for CIFAR10), CIFAR10 (for
SVHN), KMNIST, and F-MNIST as OOD data across different DNN architectures such
as ResNet34, WideResNet, DenseNet, and LeNet5.",1185,85
['cs.CV'],Learning to Compose Hypercolumns for Visual Correspondence,"Feature representation plays a crucial role in visual correspondence, and
recent methods for image matching resort to deeply stacked convolutional
layers. These models, however, are both monolithic and static in the sense that
they typically use a specific level of features, e.g., the output of the last
layer, and adhere to it regardless of the images to match. In this work, we
introduce a novel approach to visual correspondence that dynamically composes
effective features by leveraging relevant layers conditioned on the images to
match. Inspired by both multi-layer feature composition in object detection and
adaptive inference architectures in classification, the proposed method, dubbed
Dynamic Hyperpixel Flow, learns to compose hypercolumn features on the fly by
selecting a small number of relevant layers from a deep convolutional neural
network. We demonstrate the effectiveness on the task of semantic
correspondence, i.e., establishing correspondences between images depicting
different instances of the same object or scene category. Experiments on
standard benchmarks show that the proposed method greatly improves matching
performance over the state of the art in an adaptive and efficient manner.",1217,58
"['cs.CV', 'cs.LG']",CORE: Color Regression for Multiple Colors Fashion Garments,"Among all fashion attributes, color is challenging to detect due to its
subjective perception. Existing classification approaches can not go beyond the
predefined list of discrete color names. In this paper, we argue that color
detection is a regression problem. Thus, we propose a new architecture, based
on attention modules and in two-stages. The first stage corrects the image
illumination while detecting the main discrete color name. The second stage
combines a colorname-attention (dependent of the detected color) with an
object-attention (dependent of the clothing category) and finally weights a
spatial pooling over the image pixels' RGB values. We further expand our work
for multiple colors garments. We collect a dataset where each fashion item is
labeled with a continuous color palette: we empirically show the benefits of
our approach.",852,59
"['cs.CV', 'cs.GR', 'cs.LG']",Fast Patch-based Style Transfer of Arbitrary Style,"Artistic style transfer is an image synthesis problem where the content of an
image is reproduced with the style of another. Recent works show that a
visually appealing style transfer can be achieved by using the hidden
activations of a pretrained convolutional neural network. However, existing
methods either apply (i) an optimization procedure that works for any style
image but is very expensive, or (ii) an efficient feedforward network that only
allows a limited number of trained styles. In this work we propose a simpler
optimization objective based on local matching that combines the content
structure and style textures in a single layer of the pretrained network. We
show that our objective has desirable properties such as a simpler optimization
landscape, intuitive parameter tuning, and consistent frame-by-frame
performance on video. Furthermore, we use 80,000 natural images and 80,000
paintings to train an inverse network that approximates the result of the
optimization. This results in a procedure for artistic style transfer that is
efficient but also allows arbitrary content and style images.",1116,50
['cs.CV'],Deep Modular Co-Attention Networks for Visual Question Answering,"Visual Question Answering (VQA) requires a fine-grained and simultaneous
understanding of both the visual content of images and the textual content of
questions. Therefore, designing an effective `co-attention' model to associate
key words in questions with key objects in images is central to VQA
performance. So far, most successful attempts at co-attention learning have
been achieved by using shallow models, and deep co-attention models show little
improvement over their shallow counterparts. In this paper, we propose a deep
Modular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA)
layers cascaded in depth. Each MCA layer models the self-attention of questions
and images, as well as the guided-attention of images jointly using a modular
composition of two basic attention units. We quantitatively and qualitatively
evaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation
studies to explore the reasons behind MCAN's effectiveness. Experimental
results demonstrate that MCAN significantly outperforms the previous
state-of-the-art. Our best single model delivers 70.63$\%$ overall accuracy on
the test-dev set. Code is available at https://github.com/MILVLG/mcan-vqa.",1220,64
"['cs.CV', 'cs.LG']",What can computational models learn from human selective attention? A review from an audiovisual crossmodal perspective,"Selective attention plays an essential role in information acquisition and
utilization from the environment. In the past 50 years, research on selective
attention has been a central topic in cognitive science. Compared with unimodal
studies, crossmodal studies are more complex but necessary to solve real-world
challenges in both human experiments and computational modeling. Although an
increasing number of findings on crossmodal selective attention have shed light
on humans' behavioral patterns and neural underpinnings, a much better
understanding is still necessary to yield the same benefit for computational
intelligent agents. This article reviews studies of selective attention in
unimodal visual and auditory and crossmodal audiovisual setups from the
multidisciplinary perspectives of psychology and cognitive neuroscience, and
evaluates different ways to simulate analogous mechanisms in computational
models and robotics. We discuss the gaps between these fields in this
interdisciplinary review and provide insights about how to use psychological
findings and theories in artificial intelligence from different perspectives.",1140,119
"['cs.LG', 'cs.AI', 'cs.NE', 'cs.RO', 'stat.ML']",Towards robust and domain agnostic reinforcement learning competitions,"Reinforcement learning competitions have formed the basis for standard
research benchmarks, galvanized advances in the state-of-the-art, and shaped
the direction of the field. Despite this, a majority of challenges suffer from
the same fundamental problems: participant solutions to the posed challenge are
usually domain-specific, biased to maximally exploit compute resources, and not
guaranteed to be reproducible. In this paper, we present a new framework of
competition design that promotes the development of algorithms that overcome
these barriers. We propose four central mechanisms for achieving this end:
submission retraining, domain randomization, desemantization through domain
obfuscation, and the limitation of competition compute and environment-sample
budget. To demonstrate the efficacy of this design, we proposed, organized, and
ran the MineRL 2020 Competition on Sample-Efficient Reinforcement Learning. In
this work, we describe the organizational outcomes of the competition and show
that the resulting participant submissions are reproducible, non-specific to
the competition environment, and sample/resource efficient, despite the
difficult competition task.",1183,70
"['cs.CV', 'cs.AI']",Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution,"As deep reinforcement learning (RL) is applied to more tasks, there is a need
to visualize and understand the behavior of learned agents. Saliency maps
explain agent behavior by highlighting the features of the input state that are
most relevant for the agent in taking an action. Existing perturbation-based
approaches to compute saliency often highlight regions of the input that are
not relevant to the action taken by the agent. Our proposed approach, SARFA
(Specific and Relevant Feature Attribution), generates more focused saliency
maps by balancing two aspects (specificity and relevance) that capture
different desiderata of saliency. The first captures the impact of perturbation
on the relative expected reward of the action to be explained. The second
downweighs irrelevant features that alter the relative expected rewards of
actions other than the action to be explained. We compare SARFA with existing
approaches on agents trained to play board games (Chess and Go) and Atari games
(Breakout, Pong and Space Invaders). We show through illustrative examples
(Chess, Atari, Go), human studies (Chess), and automated evaluation methods
(Chess) that SARFA generates saliency maps that are more interpretable for
humans than existing approaches. For the code release and demo videos, see
https://nikaashpuri.github.io/sarfa-saliency/.",1344,94
['cs.CV'],Lane Boundary Geometry Extraction from Satellite Imagery,"Autonomous driving car is becoming more of a reality, as a key
component,high-definition(HD) maps shows its value in both market place and
industry. Even though HD maps generation from LiDAR or stereo/perspective
imagery has achieved impressive success, its inherent defects cannot be
ignored. In this paper, we proposal a novel method for Highway HD maps modeling
using pixel-wise segmentation on satellite imagery and formalized hypotheses
linking, which is cheaper and faster than current HD maps modeling approaches
from LiDAR point cloud and perspective view imagery, and let it becomes an
ideal complementary of state of the art. We also manual code/label an HD road
model dataset as ground truth, aligned with Bing tile image server, to train,
test and evaluate our methodology. This dataset will be publish at same time to
contribute research in HD maps modeling from aerial imagery.",891,56
['cs.CV'],Learning to Augment Expressions for Few-shot Fine-grained Facial Expression Recognition,"Affective computing and cognitive theory are widely used in modern
human-computer interaction scenarios. Human faces, as the most prominent and
easily accessible features, have attracted great attention from researchers.
Since humans have rich emotions and developed musculature, there exist a lot of
fine-grained expressions in real-world applications. However, it is extremely
time-consuming to collect and annotate a large number of facial images, of
which may even require psychologists to correctly categorize them. To the best
of our knowledge, the existing expression datasets are only limited to several
basic facial expressions, which are not sufficient to support our ambitions in
developing successful human-computer interaction systems. To this end, a novel
Fine-grained Facial Expression Database - F2ED is contributed in this paper,
and it includes more than 200k images with 54 facial expressions from 119
persons. Considering the phenomenon of uneven data distribution and lack of
samples is common in real-world scenarios, we further evaluate several tasks of
few-shot expression learning by virtue of our F2ED, which are to recognize the
facial expressions given only few training instances. These tasks mimic human
performance to learn robust and general representation from few examples. To
address such few-shot tasks, we propose a unified task-driven framework -
Compositional Generative Adversarial Network (Comp-GAN) learning to synthesize
facial images and thus augmenting the instances of few-shot expression classes.
Extensive experiments are conducted on F2ED and existing facial expression
datasets, i.e., JAFFE and FER2013, to validate the efficacy of our F2ED in
pre-training facial expression recognition network and the effectiveness of our
proposed approach Comp-GAN to improve the performance of few-shot recognition
tasks.",1858,87
"['cs.LG', 'cs.AI', 'I.2.6; I.2.8']",Reinforcement Learning with Linear Function Approximation and LQ control Converges,"Reinforcement learning is commonly used with function approximation. However,
very few positive results are known about the convergence of function
approximation based RL control algorithms. In this paper we show that TD(0) and
Sarsa(0) with linear function approximation is convergent for a simple class of
problems, where the system is linear and the costs are quadratic (the LQ
control problem). Furthermore, we show that for systems with Gaussian noise and
non-completely observable states (the LQG problem), the mentioned RL algorithms
are still convergent, if they are combined with Kalman filtering.",606,82
"['stat.ML', 'cs.LG']",Few-Shot Learning with Graph Neural Networks,"We propose to study the problem of few-shot learning with the prism of
inference on a partially observed graphical model, constructed from a
collection of input images whose label can be either observed or not. By
assimilating generic message-passing inference algorithms with their
neural-network counterparts, we define a graph neural network architecture that
generalizes several of the recently proposed few-shot learning models. Besides
providing improved numerical performance, our framework is easily extended to
variants of few-shot learning, such as semi-supervised or active learning,
demonstrating the ability of graph-based models to operate well on 'relational'
tasks.",681,44
"['cs.LG', 'cs.SI', 'stat.ML']",Self-Supervised Learning of Contextual Embeddings for Link Prediction in Heterogeneous Networks,"Representation learning methods for heterogeneous networks produce a
low-dimensional vector embedding for each node that is typically fixed for all
tasks involving the node. Many of the existing methods focus on obtaining a
static vector representation for a node in a way that is agnostic to the
downstream application where it is being used. In practice, however, downstream
tasks such as link prediction require specific contextual information that can
be extracted from the subgraphs related to the nodes provided as input to the
task. To tackle this challenge, we develop SLiCE, a framework bridging static
representation learning methods using global information from the entire graph
with localized attention driven mechanisms to learn contextual node
representations. We first pre-train our model in a self-supervised manner by
introducing higher-order semantic associations and masking nodes, and then
fine-tune our model for a specific link prediction task. Instead of training
node representations by aggregating information from all semantic neighbors
connected via metapaths, we automatically learn the composition of different
metapaths that characterize the context for a specific task without the need
for any pre-defined metapaths. SLiCE significantly outperforms both static and
contextual embedding learning methods on several publicly available benchmark
network datasets. We also interpret the semantic association matrix and provide
its utility and relevance in making successful link predictions between
heterogeneous nodes in the network.",1562,95
['cs.CV'],UFPR-Periocular: A Periocular Dataset Collected by Mobile Devices in Unconstrained Scenarios,"Recently, ocular biometrics in unconstrained environments using images
obtained at visible wavelength have gained the researchers' attention,
especially with images captured by mobile devices. Periocular recognition has
been demonstrated to be an alternative when the iris trait is not available due
to occlusions or low image resolution. However, the periocular trait does not
have the high uniqueness presented in the iris trait. Thus, the use of datasets
containing many subjects is essential to assess biometric systems' capacity to
extract discriminating information from the periocular region. Also, to address
the within-class variability caused by lighting and attributes in the
periocular region, it is of paramount importance to use datasets with images of
the same subject captured in distinct sessions. As the datasets available in
the literature do not present all these factors, in this work, we present a new
periocular dataset containing samples from 1,122 subjects, acquired in 3
sessions by 196 different mobile devices. The images were captured under
unconstrained environments with just a single instruction to the participants:
to place their eyes on a region of interest. We also performed an extensive
benchmark with several Convolutional Neural Network (CNN) architectures and
models that have been employed in state-of-the-art approaches based on
Multi-class Classification, Multitask Learning, Pairwise Filters Network, and
Siamese Network. The results achieved in the closed- and open-world protocol,
considering the identification and verification tasks, show that this area
still needs research and development.",1640,92
['cs.LG'],Tuning Mixed Input Hyperparameters on the Fly for Efficient Population Based AutoRL,"Despite a series of recent successes in reinforcement learning (RL), many RL
algorithms remain sensitive to hyperparameters. As such, there has recently
been interest in the field of AutoRL, which seeks to automate design decisions
to create more general algorithms. Recent work suggests that population based
approaches may be effective AutoRL algorithms, by learning hyperparameter
schedules on the fly. In particular, the PB2 algorithm is able to achieve
strong performance in RL tasks by formulating online hyperparameter
optimization as time varying GP-bandit problem, while also providing
theoretical guarantees. However, PB2 is only designed to work for continuous
hyperparameters, which severely limits its utility in practice. In this paper
we introduce a new (provably) efficient hierarchical approach for optimizing
both continuous and categorical variables, using a new time-varying bandit
algorithm specifically designed for the population based training regime. We
evaluate our approach on the challenging Procgen benchmark, where we show that
explicitly modelling dependence between data augmentation and other
hyperparameters improves generalization.",1166,83
"['stat.ML', 'cs.LG']",Probabilistic structure discovery in time series data,"Existing methods for structure discovery in time series data construct
interpretable, compositional kernels for Gaussian process regression models.
While the learned Gaussian process model provides posterior mean and variance
estimates, typically the structure is learned via a greedy optimization
procedure. This restricts the space of possible solutions and leads to
over-confident uncertainty estimates. We introduce a fully Bayesian approach,
inferring a full posterior over structures, which more reliably captures the
uncertainty of the model.",549,53
"['cs.LG', 'eess.IV', 'stat.ML']",PT-MMD: A Novel Statistical Framework for the Evaluation of Generative Systems,"Stochastic-sampling-based Generative Neural Networks, such as Restricted
Boltzmann Machines and Generative Adversarial Networks, are now used for
applications such as denoising, image occlusion removal, pattern completion,
and motion synthesis. In scenarios which involve performing such inference
tasks with these models, it is critical to determine metrics that allow for
model selection and/or maintenance of requisite generative performance under
pre-specified implementation constraints. In this paper, we propose a new
metric for evaluating generative model performance based on $p$-values derived
from the combined use of Maximum Mean Discrepancy (MMD) and permutation-based
(PT-based) resampling, which we refer to as PT-MMD. We demonstrate the
effectiveness of this metric for two cases: (1) Selection of bitwidth and
activation function complexity to achieve minimum power-at-performance for
Restricted Boltzmann Machines; (2) Quantitative comparison of images generated
by two types of Generative Adversarial Networks (PGAN and WGAN) to facilitate
model selection in order to maximize the fidelity of generated images. For
these applications, our results are shown using Euclidean and Haar-based
kernels for the PT-MMD two sample hypothesis test. This demonstrates the
critical role of distance functions in comparing generated images against their
corresponding ground truth counterparts as what would be perceived by human
users.",1442,78
['cs.CV'],MVT: Mask Vision Transformer for Facial Expression Recognition in the wild,"Facial Expression Recognition (FER) in the wild is an extremely challenging
task in computer vision due to variant backgrounds, low-quality facial images,
and the subjectiveness of annotators. These uncertainties make it difficult for
neural networks to learn robust features on limited-scale datasets. Moreover,
the networks can be easily distributed by the above factors and perform
incorrect decisions. Recently, vision transformer (ViT) and data-efficient
image transformers (DeiT) present their significant performance in traditional
classification tasks. The self-attention mechanism makes transformers obtain a
global receptive field in the first layer which dramatically enhances the
feature extraction capability. In this work, we first propose a novel pure
transformer-based mask vision transformer (MVT) for FER in the wild, which
consists of two modules: a transformer-based mask generation network (MGN) to
generate a mask that can filter out complex backgrounds and occlusion of face
images, and a dynamic relabeling module to rectify incorrect labels in FER
datasets in the wild. Extensive experimental results demonstrate that our MVT
outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with
89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable
result on AffectNet-8 with 61.40%.",1335,74
['cs.CV'],Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration,"Image restoration is a long-standing problem in low-level computer vision
with many interesting applications. We describe a flexible learning framework
based on the concept of nonlinear reaction diffusion models for various image
restoration problems. By embodying recent improvements in nonlinear diffusion
models, we propose a dynamic nonlinear reaction diffusion model with
time-dependent parameters (\ie, linear filters and influence functions). In
contrast to previous nonlinear diffusion models, all the parameters, including
the filters and the influence functions, are simultaneously learned from
training data through a loss based approach. We call this approach TNRD --
\textit{Trainable Nonlinear Reaction Diffusion}. The TNRD approach is
applicable for a variety of image restoration tasks by incorporating
appropriate reaction force. We demonstrate its capabilities with three
representative applications, Gaussian image denoising, single image super
resolution and JPEG deblocking. Experiments show that our trained nonlinear
diffusion models largely benefit from the training of the parameters and
finally lead to the best reported performance on common test datasets for the
tested applications. Our trained models preserve the structural simplicity of
diffusion models and take only a small number of diffusion steps, thus are
highly efficient. Moreover, they are also well-suited for parallel computation
on GPUs, which makes the inference procedure extremely fast.",1483,101
"['cs.LG', 'cs.CL', 'cs.IR', 'H.3.1; I.2.6; I.2.7']",Learning Analogies and Semantic Relations,"We present an algorithm for learning from unlabeled text, based on the Vector
Space Model (VSM) of information retrieval, that can solve verbal analogy
questions of the kind found in the Scholastic Aptitude Test (SAT). A verbal
analogy has the form A:B::C:D, meaning ""A is to B as C is to D""; for example,
mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B,
and the problem is to select the most analogous word pair, C:D, from a set of
five choices. The VSM algorithm correctly answers 47% of a collection of 374
college-level analogy questions (random guessing would yield 20% correct). We
motivate this research by relating it to work in cognitive science and
linguistics, and by applying it to a difficult problem in natural language
processing, determining semantic relations in noun-modifier pairs. The problem
is to classify a noun-modifier pair, such as ""laser printer"", according to the
semantic relation between the noun (printer) and the modifier (laser). We use a
supervised nearest-neighbour algorithm that assigns a class to a given
noun-modifier pair by finding the most analogous noun-modifier pair in the
training data. With 30 classes of semantic relations, on a collection of 600
labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5%
(random guessing: 3.3%). With 5 classes of semantic relations, the F value is
43.2% (random: 20%). The performance is state-of-the-art for these challenging
problems.",1469,41
['cs.CV'],Megapixel Photon-Counting Color Imaging using Quanta Image Sensor,"Quanta Image Sensor (QIS) is a single-photon detector designed for extremely
low light imaging conditions. Majority of the existing QIS prototypes are
monochrome based on single-photon avalanche diodes (SPAD). Passive color
imaging has not been demonstrated with single-photon detectors due to the
intrinsic difficulty of shrinking the pixel size and increasing the spatial
resolution while maintaining acceptable intra-pixel cross-talk. In this paper,
we present image reconstruction of the first color QIS with a resolution of
$1024 \times 1024$ pixels, supporting both single-bit and multi-bit photon
counting capability. Our color image reconstruction is enabled by a customized
joint demosaicing-denoising algorithm, leveraging truncated Poisson statistics
and variance stabilizing transforms. Experimental results of the new sensor and
algorithm demonstrate superior color imaging performance for very low-light
conditions with a mean exposure of as low as a few photons per pixel in both
real and simulated images.",1021,65
"['cs.CV', 'eess.IV']",Image Restoration by Deep Projected GSURE,"Ill-posed inverse problems appear in many image processing applications, such
as deblurring and super-resolution. In recent years, solutions that are based
on deep Convolutional Neural Networks (CNNs) have shown great promise. Yet,
most of these techniques, which train CNNs using external data, are restricted
to the observation models that have been used in the training phase. A recent
alternative that does not have this drawback relies on learning the target
image using internal learning. One such prominent example is the Deep Image
Prior (DIP) technique that trains a network directly on the input image with a
least-squares loss. In this paper, we propose a new image restoration framework
that is based on minimizing a loss function that includes a ""projected-version""
of the Generalized SteinUnbiased Risk Estimator (GSURE) and parameterization of
the latent image by a CNN. We demonstrate two ways to use our framework. In the
first one, where no explicit prior is used, we show that the proposed approach
outperforms other internal learning methods, such as DIP. In the second one, we
show that our GSURE-based loss leads to improved performance when used within a
plug-and-play priors scheme.",1206,41
['cs.CV'],Discrete Wavelet Transform and Gradient Difference based approach for text localization in videos,"The text detection and localization is important for video analysis and
understanding. The scene text in video contains semantic information and thus
can contribute significantly to video retrieval and understanding. However,
most of the approaches detect scene text in still images or single video frame.
Videos differ from images in temporal redundancy. This paper proposes a novel
hybrid method to robustly localize the texts in natural scene images and videos
based on fusion of discrete wavelet transform and gradient difference. A set of
rules and geometric properties have been devised to localize the actual text
regions. Then, morphological operation is performed to generate the text
regions and finally the connected component analysis is employed to localize
the text in a video frame. The experimental results obtained on publicly
available standard ICDAR 2003 and Hua dataset illustrate that the proposed
method can accurately detect and localize texts of various sizes, fonts and
colors. The experimentation on huge collection of video databases reveal the
suitability of the proposed method to video databases.",1126,97
"['cs.LG', 'cs.SI', 'stat.ML']",Pitfalls of Graph Neural Network Evaluation,"Semi-supervised node classification in graphs is a fundamental problem in
graph mining, and the recently proposed graph neural networks (GNNs) have
achieved unparalleled results on this task. Due to their massive success, GNNs
have attracted a lot of attention, and many novel architectures have been put
forward. In this paper we show that existing evaluation strategies for GNN
models have serious shortcomings. We show that using the same
train/validation/test splits of the same datasets, as well as making
significant changes to the training procedure (e.g. early stopping criteria)
precludes a fair comparison of different architectures. We perform a thorough
empirical evaluation of four prominent GNN models and show that considering
different splits of the data leads to dramatically different rankings of
models. Even more importantly, our findings suggest that simpler GNN
architectures are able to outperform the more sophisticated ones if the
hyperparameters and the training procedure are tuned fairly for all models.",1031,43
"['cs.LG', 'physics.chem-ph', 'stat.ML']",E(n) Equivariant Normalizing Flows,"This paper introduces a generative model equivariant to Euclidean symmetries:
E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the
discriminative E(n) graph neural networks and integrate them as a differential
equation to obtain an invertible equivariant function: a continuous-time
normalizing flow. We demonstrate that E-NFs considerably outperform baselines
and existing methods from the literature on particle systems such as DW4 and
LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our
knowledge, this is the first flow that jointly generates molecule features and
positions in 3D.",635,34
"['cs.CV', 'cs.AI', 'cs.LG', 'cs.MA']",Flip Learning: Erase to Segment,"Nodule segmentation from breast ultrasound images is challenging yet
essential for the diagnosis. Weakly-supervised segmentation (WSS) can help
reduce time-consuming and cumbersome manual annotation. Unlike existing
weakly-supervised approaches, in this study, we propose a novel and general WSS
framework called Flip Learning, which only needs the box annotation.
Specifically, the target in the label box will be erased gradually to flip the
classification tag, and the erased region will be considered as the
segmentation result finally. Our contribution is three-fold. First, our
proposed approach erases on superpixel level using a Multi-agent Reinforcement
Learning framework to exploit the prior boundary knowledge and accelerate the
learning process. Second, we design two rewards: classification score and
intensity distribution reward, to avoid under- and over-segmentation,
respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the
residual errors and improve the segmentation performance. Extensively validated
on a large dataset, our proposed approach achieves competitive performance and
shows great potential to narrow the gap between fully-supervised and
weakly-supervised learning.",1219,31
"['cs.LG', 'cond-mat.stat-mech', 'q-bio.NC', 'stat.ML']","Rethinking the limiting dynamics of SGD: modified loss, phase space oscillations, and anomalous diffusion","In this work we explore the limiting dynamics of deep neural networks trained
with stochastic gradient descent (SGD). We find empirically that long after
performance has converged, networks continue to move through parameter space by
a process of anomalous diffusion in which distance travelled grows as a power
law in the number of gradient updates with a nontrivial exponent. We reveal an
intricate interaction between the hyperparameters of optimization, the
structure in the gradient noise, and the Hessian matrix at the end of training
that explains this anomalous diffusion. To build this understanding, we first
derive a continuous-time model for SGD with finite learning rates and batch
sizes as an underdamped Langevin equation. We study this equation in the
setting of linear regression, where we can derive exact, analytic expressions
for the phase space dynamics of the parameters and their instantaneous
velocities from initialization to stationarity. Using the Fokker-Planck
equation, we show that the key ingredient driving these dynamics is not the
original training loss, but rather the combination of a modified loss, which
implicitly regularizes the velocity, and probability currents, which cause
oscillations in phase space. We identify qualitative and quantitative
predictions of this theory in the dynamics of a ResNet-18 model trained on
ImageNet. Through the lens of statistical physics, we uncover a mechanistic
origin for the anomalous limiting dynamics of deep neural networks trained with
SGD.",1522,105
['cs.CV'],Recursive Contour Saliency Blending Network for Accurate Salient Object Detection,"Contour information plays a vital role in salient object detection. However,
excessive false positives remain in predictions from existing contour-based
models due to insufficient contour-saliency fusion. In this work, we designed a
network for better edge quality in salient object detection. We proposed a
contour-saliency blending module to exchange information between contour and
saliency. We adopted recursive CNN to increase contour-saliency fusion while
keeping the total trainable parameters the same. Furthermore, we designed a
stage-wise feature extraction module to help the model pick up the most helpful
features from previous intermediate saliency predictions. Besides, we proposed
two new loss functions, namely Dual Confinement Loss and Confidence Loss, for
our model to generate better boundary predictions. Evaluation results on five
common benchmark datasets reveal that our model achieves competitive
state-of-the-art performance.",951,81
['cs.CV'],Image Segmentation by Using Threshold Techniques,"This paper attempts to undertake the study of segmentation image techniques
by using five threshold methods as Mean method, P-tile method, Histogram
Dependent Technique (HDT), Edge Maximization Technique (EMT) and visual
Technique and they are compared with one another so as to choose the best
technique for threshold segmentation techniques image. These techniques applied
on three satellite images to choose base guesses for threshold segmentation
image.",457,48
"['cs.LG', 'stat.ML']",Contrastive Variational Reinforcement Learning for Complex Observations,"Deep reinforcement learning (DRL) has achieved significant success in various
robot tasks: manipulation, navigation, etc. However, complex visual
observations in natural environments remains a major challenge. This paper
presents Contrastive Variational Reinforcement Learning (CVRL), a model-based
method that tackles complex visual observations in DRL. CVRL learns a
contrastive variational model by maximizing the mutual information between
latent states and observations discriminatively, through contrastive learning.
It avoids modeling the complex observation space unnecessarily, as the commonly
used generative observation model often does, and is significantly more robust.
CVRL achieves comparable performance with state-of-the-art model-based DRL
methods on standard Mujoco tasks. It significantly outperforms them on Natural
Mujoco tasks and a robot box-pushing task with complex observations, e.g.,
dynamic shadows. The CVRL code is available publicly at
https://github.com/Yusufma03/CVRL.",1002,71
['cs.CV'],MVOR: A Multi-view RGB-D Operating Room Dataset for 2D and 3D Human Pose Estimation,"Person detection and pose estimation is a key requirement to develop
intelligent context-aware assistance systems. To foster the development of
human pose estimation methods and their applications in the Operating Room
(OR), we release the Multi-View Operating Room (MVOR) dataset, the first public
dataset recorded during real clinical interventions. It consists of 732
synchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR.
It also includes the visual challenges present in such environments, such as
occlusions and clutter. We provide camera calibration parameters, color and
depth frames, human bounding boxes, and 2D/3D pose annotations. In this paper,
we present the dataset, its annotations, as well as baseline results from
several recent person detection and 2D/3D pose estimation methods. Since we
need to blur some parts of the images to hide identity and nudity in the
released dataset, we also present a comparative study of how the baselines have
been impacted by the blurring. Results show a large margin for improvement and
suggest that the MVOR dataset can be useful to compare the performance of the
different methods.",1160,83
"['cs.LG', 'stat.ML']",Learning Options from Demonstration using Skill Segmentation,"We present a method for learning options from segmented demonstration
trajectories. The trajectories are first segmented into skills using
nonparametric Bayesian clustering and a reward function for each segment is
then learned using inverse reinforcement learning. From this, a set of inferred
trajectories for the demonstration are generated. Option initiation sets and
termination conditions are learned from these trajectories using the one-class
support vector machine clustering algorithm. We demonstrate our method in the
four rooms domain, where an agent is able to autonomously discover usable
options from human demonstration. Our results show that these inferred options
can then be used to improve learning and planning.",732,60
"['cs.LG', 'cs.AI']",Reinforcement Learning for Learning of Dynamical Systems in Uncertain Environment: a Tutorial,"In this paper, a review of model-free reinforcement learning for learning of
dynamical systems in uncertain environments has discussed. For this purpose,
the Markov Decision Process (MDP) will be reviewed. Furthermore, some learning
algorithms such as Temporal Difference (TD) learning, Q-Learning, and
Approximate Q-learning as model-free algorithms which constitute the main part
of this article have been investigated, and benefits and drawbacks of each
algorithm will be discussed. The discussed concepts in each section are
explaining with details and examples.",566,93
['cs.CV'],Double Refinement Network for Efficient Indoor Monocular Depth Estimation,"Monocular depth estimation is the task of obtaining a measure of distance for
each pixel using a single image. It is an important problem in computer vision
and is usually solved using neural networks. Though recent works in this area
have shown significant improvement in accuracy, the state-of-the-art methods
tend to require massive amounts of memory and time to process an image. The
main purpose of this work is to improve the performance of the latest solutions
with no decrease in accuracy. To this end, we introduce the Double Refinement
Network architecture. The proposed method achieves state-of-the-art results on
the standard benchmark RGB-D dataset NYU Depth v2, while its frames per second
rate is significantly higher (up to 18 times speedup per image at batch size 1)
and the RAM usage per image is lower.",821,73
"['cs.LG', 'stat.ML']",Malaria Likelihood Prediction By Effectively Surveying Households Using Deep Reinforcement Learning,"We build a deep reinforcement learning (RL) agent that can predict the
likelihood of an individual testing positive for malaria by asking questions
about their household. The RL agent learns to determine which survey question
to ask next and when to stop to make a prediction about their likelihood of
malaria based on their responses hitherto. The agent incurs a small penalty for
each question asked, and a large reward/penalty for making the correct/wrong
prediction; it thus has to learn to balance the length of the survey with the
accuracy of its final predictions. Our RL agent is a Deep Q-network that learns
a policy directly from the responses to the questions, with an action defined
for each possible survey question and for each possible prediction class. We
focus on Kenya, where malaria is a massive health burden, and train the RL
agent on a dataset of 6481 households from the Kenya Malaria Indicator Survey
2015. To investigate the importance of having survey questions be adaptive to
responses, we compare our RL agent to a supervised learning (SL) baseline that
fixes its set of survey questions a priori. We evaluate on prediction accuracy
and on the number of survey questions asked on a holdout set and find that the
RL agent is able to predict with 80% accuracy, using only 2.5 questions on
average. In addition, the RL agent learns to survey adaptively to responses and
is able to match the SL baseline in prediction accuracy while significantly
reducing survey length.",1494,99
['cs.CV'],MAVNet: an Effective Semantic Segmentation Micro-Network for MAV-based Tasks,"Real-time semantic image segmentation on platforms subject to size, weight
and power (SWaP) constraints is a key area of interest for air surveillance and
inspection. In this work, we propose MAVNet: a small, light-weight, deep neural
network for real-time semantic segmentation on micro Aerial Vehicles (MAVs).
MAVNet, inspired by ERFNet, features 400 times fewer parameters and achieves
comparable performance with some reference models in empirical experiments. Our
model achieves a trade-off between speed and accuracy, achieving up to 48 FPS
on an NVIDIA 1080Ti and 9 FPS on the NVIDIA Jetson Xavier when processing high
resolution imagery. Additionally, we provide two novel datasets that represent
challenges in semantic segmentation for real-time MAV tracking and
infrastructure inspection tasks and verify MAVNet on these datasets. Our
algorithm and datasets are made publicly available.",896,76
"['cs.LG', 'cs.AI', 'cs.CV']",Traffic Agent Trajectory Prediction Using Social Convolution and Attention Mechanism,"The trajectory prediction is significant for the decision-making of
autonomous driving vehicles. In this paper, we propose a model to predict the
trajectories of target agents around an autonomous vehicle. The main idea of
our method is considering the history trajectories of the target agent and the
influence of surrounding agents on the target agent. To this end, we encode the
target agent history trajectories as an attention mask and construct a social
map to encode the interactive relationship between the target agent and its
surrounding agents. Given a trajectory sequence, the LSTM networks are firstly
utilized to extract the features for all agents, based on which the attention
mask and social map are formed. Then, the attention mask and social map are
fused to get the fusion feature map, which is processed by the social
convolution to obtain a fusion feature representation. Finally, this fusion
feature is taken as the input of a variable-length LSTM to predict the
trajectory of the target agent. We note that the variable-length LSTM enables
our model to handle the case that the number of agents in the sensing scope is
highly dynamic in traffic scenes. To verify the effectiveness of our method, we
widely compare with several methods on a public dataset, achieving a 20% error
decrease. In addition, the model satisfies the real-time requirement with the
32 fps.",1387,84
['cs.CV'],Meta-SR: A Magnification-Arbitrary Network for Super-Resolution,"Recent research on super-resolution has achieved great success due to the
development of deep convolutional neural networks (DCNNs). However,
super-resolution of arbitrary scale factor has been ignored for a long time.
Most previous researchers regard super-resolution of different scale factors as
independent tasks. They train a specific model for each scale factor which is
inefficient in computing, and prior work only take the super-resolution of
several integer scale factors into consideration. In this work, we propose a
novel method called Meta-SR to firstly solve super-resolution of arbitrary
scale factor (including non-integer scale factors) with a single model. In our
Meta-SR, the Meta-Upscale Module is proposed to replace the traditional upscale
module. For arbitrary scale factor, the Meta-Upscale Module dynamically
predicts the weights of the upscale filters by taking the scale factor as input
and use these weights to generate the HR image of arbitrary size. For any
low-resolution image, our Meta-SR can continuously zoom in it with arbitrary
scale factor by only using a single model. We evaluated the proposed method
through extensive experiments on widely used benchmark datasets on single image
super-resolution. The experimental results show the superiority of our
Meta-Upscale.",1306,63
"['cs.LG', 'cs.AI']",Discovering Novel Customer Features with Recurrent Neural Networks for Personality Based Financial Services,"The micro-segmentation of customers in the finance sector is a non-trivial
task and has been an atypical omission from recent scientific literature. Where
traditional segmentation classifies customers based on coarse features such as
demographics, micro-segmentation depicts more nuanced differences between
individuals, bringing forth several advantages including the potential for
improved personalization in financial services. AI and representation learning
offer a unique opportunity to solve the problem of micro-segmentation. Although
ubiquitous in many industries, the proliferation of AI in sensitive industries
such as finance has become contingent on the imperatives of responsible AI. We
had previously solved the micro-segmentation problem by extracting temporal
features from the state space of a recurrent neural network (RNN). However, due
to the inherent opacity of RNNs our solution lacked an explanation - one of the
imperatives of responsible AI. In this study, we address this issue by
extracting an explanation for and providing an interpretation of our temporal
features. We investigate the state space of our RNN and through a linear
regression model reconstruct the trajectories in the state space with high
fidelity. We show that our linear regression coefficients have not only learned
the rules used to create the RNN's output data but have also learned the
relationships that were not directly evident in the raw data.",1447,107
['cs.CV'],Adiabatic Quantum Graph Matching with Permutation Matrix Constraints,"Matching problems on 3D shapes and images are challenging as they are
frequently formulated as combinatorial quadratic assignment problems (QAPs)
with permutation matrix constraints, which are NP-hard. In this work, we
address such problems with emerging quantum computing technology and propose
several reformulations of QAPs as unconstrained problems suitable for efficient
execution on quantum hardware. We investigate several ways to inject
permutation matrix constraints in a quadratic unconstrained binary optimization
problem which can be mapped to quantum hardware. We focus on obtaining a
sufficient spectral gap, which further increases the probability to measure
optimal solutions and valid permutation matrices in a single run. We perform
our experiments on the quantum computer D-Wave 2000Q (2^11 qubits, adiabatic).
Despite the observed discrepancy between simulated adiabatic quantum computing
and execution on real quantum hardware, our reformulation of permutation matrix
constraints increases the robustness of the numerical computations over other
penalty approaches in our experiments. The proposed algorithm has the potential
to scale to higher dimensions on future quantum computing architectures, which
opens up multiple new directions for solving matching problems in 3D computer
vision and graphics.",1324,68
"['cs.LG', 'cs.AI', 'stat.ML']",IPF for Discrete Chain Factor Graphs,"Iterative Proportional Fitting (IPF), combined with EM, is commonly used as
an algorithm for likelihood maximization in undirected graphical models. In
this paper, we present two iterative algorithms that generalize upon IPF. The
first one is for likelihood maximization in discrete chain factor graphs, which
we define as a wide class of discrete variable models including undirected
graphical models and Bayesian networks, but also chain graphs and sigmoid
belief networks. The second one is for conditional likelihood maximization in
standard undirected models and Bayesian networks. In both algorithms, the
iteration steps are expressed in closed form. Numerical simulations show that
the algorithms are competitive with state of the art methods.",750,36
"['cs.LG', 'cs.AI']",Self-Supervised Graph Learning with Hyperbolic Embedding for Temporal Health Event Prediction,"Electronic Health Records (EHR) have been heavily used in modern healthcare
systems for recording patients' admission information to hospitals. Many
data-driven approaches employ temporal features in EHR for predicting specific
diseases, readmission times, or diagnoses of patients. However, most existing
predictive models cannot fully utilize EHR data, due to an inherent lack of
labels in supervised training for some temporal events. Moreover, it is hard
for existing works to simultaneously provide generic and personalized
interpretability. To address these challenges, we first propose a hyperbolic
embedding method with information flow to pre-train medical code
representations in a hierarchical structure. We incorporate these pre-trained
representations into a graph neural network to detect disease complications,
and design a multi-level attention method to compute the contributions of
particular diseases and admissions, thus enhancing personalized
interpretability. We present a new hierarchy-enhanced historical prediction
proxy task in our self-supervised learning framework to fully utilize EHR data
and exploit medical domain knowledge. We conduct a comprehensive set of
experiments and case studies on widely used publicly available EHR datasets to
verify the effectiveness of our model. The results demonstrate our model's
strengths in both predictive tasks and interpretable abilities.",1408,93
['cs.CV'],Focal Loss in 3D Object Detection,"3D object detection is still an open problem in autonomous driving scenes.
When recognizing and localizing key objects from sparse 3D inputs, autonomous
vehicles suffer from a larger continuous searching space and higher
fore-background imbalance compared to image-based object detection. In this
paper, we aim to solve this fore-background imbalance in 3D object detection.
Inspired by the recent use of focal loss in image-based object detection, we
extend this hard-mining improvement of binary cross entropy to
point-cloud-based object detection and conduct experiments to show its
performance based on two different 3D detectors: 3D-FCN and VoxelNet. The
evaluation results show up to 11.2AP gains through the focal loss in a wide
range of hyperparameters for 3D object detection.",785,33
['cs.CV'],Robust Physical-World Attacks on Face Recognition,"Face recognition has been greatly facilitated by the development of deep
neural networks (DNNs) and has been widely applied to many safety-critical
applications. However, recent studies have shown that DNNs are very vulnerable
to adversarial examples, raising serious concerns on the security of real-world
face recognition. In this work, we study sticker-based physical attacks on face
recognition for better understanding its adversarial robustness. To this end,
we first analyze in-depth the complicated physical-world conditions confronted
by attacking face recognition, including the different variations of stickers,
faces, and environmental conditions. Then, we propose a novel robust physical
attack framework, dubbed PadvFace, to model these challenging variations
specifically. Furthermore, considering the difference in attack complexity, we
propose an efficient Curriculum Adversarial Attack (CAA) algorithm that
gradually adapts adversarial stickers to environmental variations from easy to
complex. Finally, we construct a standardized testing protocol to facilitate
the fair evaluation of physical attacks on face recognition, and extensive
experiments on both dodging and impersonation attacks demonstrate the superior
performance of the proposed method.",1270,49
"['cs.LG', 'eess.SP', 'stat.ML']",Linear Multiple Low-Rank Kernel Based Stationary Gaussian Processes Regression for Time Series,"Gaussian processes (GP) for machine learning have been studied systematically
over the past two decades and they are by now widely used in a number of
diverse applications. However, GP kernel design and the associated
hyper-parameter optimization are still hard and to a large extend open
problems. In this paper, we consider the task of GP regression for time series
modeling and analysis. The underlying stationary kernel can be approximated
arbitrarily close by a new proposed grid spectral mixture (GSM) kernel, which
turns out to be a linear combination of low-rank sub-kernels. In the case where
a large number of the sub-kernels are used, either the Nystr\""{o}m or the
random Fourier feature approximations can be adopted to deal efficiently with
the computational demands. The unknown GP hyper-parameters consist of the
non-negative weights of all sub-kernels as well as the noise variance; their
estimation is performed via the maximum-likelihood (ML) estimation framework.
Two efficient numerical optimization methods for solving the unknown
hyper-parameters are derived, including a sequential majorization-minimization
(MM) method and a non-linearly constrained alternating direction of multiplier
method (ADMM). The MM matches perfectly with the proven low-rank property of
the proposed GSM sub-kernels and turns out to be a part of efficiency, stable,
and efficient solver, while the ADMM has the potential to generate better local
minimum in terms of the test MSE. Experimental results, based on various
classic time series data sets, corroborate that the proposed GSM kernel-based
GP regression model outperforms several salient competitors of similar kind in
terms of prediction mean-squared-error and numerical stability.",1739,94
"['cs.LG', 'stat.ML']",Retraining-Based Iterative Weight Quantization for Deep Neural Networks,"Model compression has gained a lot of attention due to its ability to reduce
hardware resource requirements significantly while maintaining accuracy of
DNNs. Model compression is especially useful for memory-intensive recurrent
neural networks because smaller memory footprint is crucial not only for
reducing storage requirement but also for fast inference operations.
Quantization is known to be an effective model compression method and
researchers are interested in minimizing the number of bits to represent
parameters. In this work, we introduce an iterative technique to apply
quantization, presenting high compression ratio without any modifications to
the training algorithm. In the proposed technique, weight quantization is
followed by retraining the model with full precision weights. We show that
iterative retraining generates new sets of weights which can be quantized with
decreasing quantization loss at each iteration. We also show that quantization
is efficiently able to leverage pruning, another effective model compression
method. Implementation issues on combining the two methods are also addressed.
Our experimental results demonstrate that an LSTM model using 1-bit quantized
weights is sufficient for PTB dataset without any accuracy degradation while
previous methods demand at least 2-4 bits for quantized weights.",1343,71
['cs.CV'],Learnable Visual Markers,"We propose a new approach to designing visual markers (analogous to QR-codes,
markers for augmented reality, and robotic fiducial tags) based on the advances
in deep generative networks. In our approach, the markers are obtained as color
images synthesized by a deep network from input bit strings, whereas another
deep network is trained to recover the bit strings back from the photos of
these markers. The two networks are trained simultaneously in a joint
backpropagation process that takes characteristic photometric and geometric
distortions associated with marker fabrication and marker scanning into
account. Additionally, a stylization loss based on statistics of activations in
a pretrained classification network can be inserted into the learning in order
to shift the marker appearance towards some texture prototype. In the
experiments, we demonstrate that the markers obtained using our approach are
capable of retaining bit strings that are long enough to be practical. The
ability to automatically adapt markers according to the usage scenario and the
desired capacity as well as the ability to combine information encoding with
artistic stylization are the unique properties of our approach. As a byproduct,
our approach provides an insight on the structure of patterns that are most
suitable for recognition by ConvNets and on their ability to distinguish
composite patterns.",1393,24
['cs.CV'],Tracking system of Mine Patrol Robot for Low Illumination Environment,"Computer vision has received a significant attention in recent years, which
is one of the important parts for robots to apperceive external environment.
Discriminative Correlation Filter (DCF) based trackers gained more popularity
due to their efficiency, however, tracking in low-illumination environments is
a challenging problem, not yet successfully addressed in the literature. In
this work, we tackle the problems by introducing Low-Illumination Long-term
Correlation Tracker (LLCT). First, fused features only including HOG and Color
Names are employed to boost the tracking efficiency. Second, we used the
standard PCA to reduction scheme in the translation and scale estimation phase
for accelerating. Third, we learned a long-term correlation filter to keep the
long-term memory ability. Finally, update memory templates with interval
updates, then re-match existing and initial templates every few frames to
maintain template accuracy. The extensive experiments on popular Object
Tracking Benchmark OTB-50 datasets have demonstrated that the proposed tracker
outperforms the state-of-the-art trackers significantly achieves a high
real-time (33FPS) performance. In addition, the proposed approach can be
integrated easily in robot system and the running speed performed well. The
experimental results show that the novel tracker performance in
low-illumination environment is better than that of general trackers.",1424,69
"['cs.CV', 'cs.AI']",Attention for Image Registration (AiR): an unsupervised Transformer approach,"Image registration as an important basis in signal processing task often
encounter the problem of stability and efficiency. Non-learning registration
approaches rely on the optimization of the similarity metrics between the fix
and moving images. Yet, those approaches are usually costly in both time and
space complexity. The problem can be worse when the size of the image is large
or the deformations between the images are severe. Recently, deep learning, or
precisely saying, the convolutional neural network (CNN) based image
registration methods have been widely investigated in the research community
and show promising effectiveness to overcome the weakness of non-learning based
methods. To explore the advanced learning approaches in image registration
problem for solving practical issues, we present in this paper a method of
introducing attention mechanism in deformable image registration problem. The
proposed approach is based on learning the deformation field with a Transformer
framework (AiR) that does not rely on the CNN but can be efficiently trained on
GPGPU devices also. In a more vivid interpretation: we treat the image
registration problem as the same as a language translation task and introducing
a Transformer to tackle the problem. Our method learns an unsupervised
generated deformation map and is tested on two benchmark datasets. The source
code of the AiR will be released at Gitlab.",1420,76
"['stat.ML', 'cs.CV', 'cs.LG', 'physics.comp-ph']",GAN-based Priors for Quantifying Uncertainty,"Bayesian inference is used extensively to quantify the uncertainty in an
inferred field given the measurement of a related field when the two are linked
by a mathematical model. Despite its many applications, Bayesian inference
faces challenges when inferring fields that have discrete representations of
large dimension, and/or have prior distributions that are difficult to
characterize mathematically. In this work we demonstrate how the approximate
distribution learned by a deep generative adversarial network (GAN) may be used
as a prior in a Bayesian update to address both these challenges. We
demonstrate the efficacy of this approach on two distinct, and remarkably
broad, classes of problems. The first class leads to supervised learning
algorithms for image classification with superior out of distribution detection
and accuracy, and for image inpainting with built-in variance estimation. The
second class leads to unsupervised learning algorithms for image denoising and
for solving physics-driven inverse problems.",1030,44
"['cs.CV', 'eess.IV', 'I.4.2']",Distributed Learning and Inference with Compressed Images,"Modern computer vision requires processing large amounts of data, both while
training the model and/or during inference, once the model is deployed.
Scenarios where images are captured and processed in physically separated
locations are increasingly common (e.g. autonomous vehicles, cloud computing).
In addition, many devices suffer from limited resources to store or transmit
data (e.g. storage space, channel capacity). In these scenarios, lossy image
compression plays a crucial role to effectively increase the number of images
collected under such constraints. However, lossy compression entails some
undesired degradation of the data that may harm the performance of the
downstream analysis task at hand, since important semantic information may be
lost in the process. Moreover, we may only have compressed images at training
time but are able to use original images at inference time, or vice versa, and
in such a case, the downstream model suffers from covariate shift. In this
paper, we analyze this phenomenon, with a special focus on vision-based
perception for autonomous driving as a paradigmatic scenario. We see that loss
of semantic information and covariate shift do indeed exist, resulting in a
drop in performance that depends on the compression rate. In order to address
the problem, we propose dataset restoration, based on image restoration with
generative adversarial networks (GANs). Our method is agnostic to both the
particular image compression method and the downstream task; and has the
advantage of not adding additional cost to the deployed models, which is
particularly important in resource-limited devices. The presented experiments
focus on semantic segmentation as a challenging use case, cover a broad range
of compression rates and diverse datasets, and show how our method is able to
significantly alleviate the negative effects of compression on the downstream
visual task.",1916,57
['cs.CV'],Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks,"Despite the state-of-the-art performance for medical image segmentation, deep
convolutional neural networks (CNNs) have rarely provided uncertainty
estimations regarding their segmentation outputs, e.g., model (epistemic) and
image-based (aleatoric) uncertainties. In this work, we analyze these different
types of uncertainties for CNN-based 2D and 3D medical image segmentation
tasks. We additionally propose a test-time augmentation-based aleatoric
uncertainty to analyze the effect of different transformations of the input
image on the segmentation output. Test-time augmentation has been previously
used to improve segmentation accuracy, yet not been formulated in a consistent
mathematical framework. Hence, we also propose a theoretical formulation of
test-time augmentation, where a distribution of the prediction is estimated by
Monte Carlo simulation with prior distributions of parameters in an image
acquisition model that involves image transformations and noise. We compare and
combine our proposed aleatoric uncertainty with model uncertainty. Experiments
with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic
Resonance Images (MRI) showed that 1) the test-time augmentation-based
aleatoric uncertainty provides a better uncertainty estimation than calculating
the test-time dropout-based model uncertainty alone and helps to reduce
overconfident incorrect predictions, and 2) our test-time augmentation
outperforms a single-prediction baseline and dropout-based multiple
predictions.",1521,126
['cs.CV'],Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation,"In this work, we explore the issue of the inter-annotator agreement for
training and evaluating automated segmentation of skin lesions. We explore what
different degrees of agreement represent, and how they affect different use
cases for segmentation. We also evaluate how conditioning the ground truths
using different (but very simple) algorithms may help to enhance agreement and
may be appropriate for some use cases. The segmentation of skin lesions is a
cornerstone task for automated skin lesion analysis, useful both as an
end-result to locate/detect the lesions and as an ancillary task for lesion
classification. Lesion segmentation, however, is a very challenging task, due
not only to the challenge of image segmentation itself but also to the
difficulty in obtaining properly annotated data. Detecting accurately the
borders of lesions is challenging even for trained humans, since, for many
lesions, those borders are fuzzy and ill-defined. Using lesions and annotations
from the ISIC Archive, we estimate inter-annotator agreement for skin-lesion
segmentation and propose several simple procedures that may help to improve
inter-annotator agreement if used to condition the ground truths.",1203,73
"['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']",A Distributional View on Multi-Objective Policy Optimization,"Many real-world problems require trading off multiple competing objectives.
However, these objectives are often in different units and/or scales, which can
make it challenging for practitioners to express numerical preferences over
objectives in their native units. In this paper we propose a novel algorithm
for multi-objective reinforcement learning that enables setting desired
preferences for objectives in a scale-invariant way. We propose to learn an
action distribution for each objective, and we use supervised learning to fit a
parametric policy to a combination of these distributions. We demonstrate the
effectiveness of our approach on challenging high-dimensional real and
simulated robotics tasks, and show that setting different preferences in our
framework allows us to trace out the space of nondominated solutions.",832,60
"['cs.CV', 'cs.AI']",A Novel Application of Image-to-Image Translation: Chromosome Straightening Framework by Learning from a Single Image,"In medical imaging, chromosome straightening plays a significant role in the
pathological study of chromosomes and in the development of cytogenetic maps.
Whereas different approaches exist for the straightening task, they are mostly
geometric algorithms whose outputs are characterized by jagged edges or
fragments with discontinued banding patterns. To address the flaws in the
geometric algorithms, we propose a novel framework based on image-to-image
translation to learn a pertinent mapping dependence for synthesizing
straightened chromosomes with uninterrupted banding patterns and preserved
details. In addition, to avoid the pitfall of deficient input chromosomes, we
construct an augmented dataset using only one single curved chromosome image
for training models. Based on this framework, we apply two popular
image-to-image translation architectures, U-shape networks and conditional
generative adversarial networks, to assess its efficacy. Experiments on a
dataset comprising of 642 real-world chromosomes demonstrate the superiority of
our framework as compared to the geometric method in straightening performance
by rendering realistic and continued chromosome details. Furthermore, our
straightened results improve the chromosome classification, achieving
0.98%-1.39% in mean accuracy.",1302,117
"['cs.LG', 'cs.PF', 'stat.ML']",Batch Size Influence on Performance of Graphic and Tensor Processing Units during Training and Inference Phases,"The impact of the maximally possible batch size (for the better runtime) on
performance of graphic processing units (GPU) and tensor processing units (TPU)
during training and inference phases is investigated. The numerous runs of the
selected deep neural network (DNN) were performed on the standard MNIST and
Fashion-MNIST datasets. The significant speedup was obtained even for extremely
low-scale usage of Google TPUv2 units (8 cores only) in comparison to the quite
powerful GPU NVIDIA Tesla K80 card with the speedup up to 10x for training
stage (without taking into account the overheads) and speedup up to 2x for
prediction stage (with and without taking into account overheads). The precise
speedup values depend on the utilization level of TPUv2 units and increase with
the increase of the data volume under processing, but for the datasets used in
this work (MNIST and Fashion-MNIST with images of sizes 28x28) the speedup was
observed for batch sizes >512 images for training phase and >40 000 images for
prediction phase. It should be noted that these results were obtained without
detriment to the prediction accuracy and loss that were equal for both GPU and
TPU runs up to the 3rd significant digit for MNIST dataset, and up to the 2nd
significant digit for Fashion-MNIST dataset.",1296,111
['cs.CV'],Misfire Detection in IC Engine using Kstar Algorithm,"Misfire in an IC Engine continues to be a problem leading to reduced fuel
efficiency, increased power loss and emissions containing heavy concentration
of hydrocarbons. Misfiring creates a unique vibration pattern attributed to a
particular cylinder. Useful features can be extracted from these patterns and
can be analyzed to detect misfire. Statistical features from these vibration
signals were extracted. Out of these, useful features were identified using the
J48 decision tree algorithm and selected features were used for classification
using the Kstar algorithm. In this paper performance analysis of Kstar
algorithm is presented.",638,52
"['cs.LG', 'stat.ML']",Born-Again Tree Ensembles,"The use of machine learning algorithms in finance, medicine, and criminal
justice can deeply impact human lives. As a consequence, research into
interpretable machine learning has rapidly grown in an attempt to better
control and fix possible sources of mistakes and biases. Tree ensembles offer a
good prediction quality in various domains, but the concurrent use of multiple
trees reduces the interpretability of the ensemble. Against this background, we
study born-again tree ensembles, i.e., the process of constructing a single
decision tree of minimum size that reproduces the exact same behavior as a
given tree ensemble in its entire feature space. To find such a tree, we
develop a dynamic-programming based algorithm that exploits sophisticated
pruning and bounding rules to reduce the number of recursive calls. This
algorithm generates optimal born-again trees for many datasets of practical
interest, leading to classifiers which are typically simpler and more
interpretable without any other form of compromise.",1025,25
"['cs.LG', 'stat.ML']",Tabular Benchmarks for Joint Architecture and Hyperparameter Optimization,"Due to the high computational demands executing a rigorous comparison between
hyperparameter optimization (HPO) methods is often cumbersome. The goal of this
paper is to facilitate a better empirical evaluation of HPO methods by
providing benchmarks that are cheap to evaluate, but still represent realistic
use cases. We believe these benchmarks provide an easy and efficient way to
conduct reproducible experiments for neural hyperparameter search. Our
benchmarks consist of a large grid of configurations of a feed forward neural
network on four different regression datasets including architectural
hyperparameters and hyperparameters concerning the training pipeline. Based on
this data, we performed an in-depth analysis to gain a better understanding of
the properties of the optimization problem, as well as of the importance of
different types of hyperparameters. Second, we exhaustively compared various
different state-of-the-art methods from the hyperparameter optimization
literature on these benchmarks in terms of performance and robustness.",1056,73
"['cs.LG', 'stat.ML', 'I.2.6']",Enhancing Trajectory Prediction using Sparse Outputs: Application to Team Sports,"Sophisticated trajectory prediction models that effectively mimic team
dynamics have many potential uses for sports coaches, broadcasters and
spectators. However, through experiments on soccer data we found that it can be
surprisingly challenging to train a deep learning model for player trajectory
prediction which outperforms linear extrapolation on average distance between
predicted and true future trajectories. We propose and test a novel method for
improving training by predicting a sparse trajectory and interpolating using
constant acceleration, which improves performance for several models. This
interpolation can also be used on models that aren't trained with sparse
outputs, and we find that this consistently improves performance for all tested
models. Additionally, we find that the accuracy of predicted trajectories for a
subset of players can be improved by conditioning on the full trajectories of
the other players, and that this is further improved when combined with sparse
predictions. We also propose a novel architecture using graph networks and
multi-head attention (GraN-MA) which achieves better performance than other
tested state-of-the-art models on our dataset and is trivially adapted for both
sparse trajectories and full-trajectory conditioned trajectory prediction.",1304,80
['cs.CV'],Unsupervised Domain Adaptation for Object Detection via Cross-Domain Semi-Supervised Learning,"Current state-of-the-art object detectors can have significant performance
drop when deployed in the wild due to domain gaps with training data.
Unsupervised Domain Adaptation (UDA) is a promising approach to adapt models
for new domains/environments without any expensive label cost. However, without
ground truth labels, most prior works on UDA for object detection tasks can
only perform coarse image-level and/or feature-level adaptation by using
adversarial learning methods. In this work, we show that such adversarial-based
methods can only reduce the domain style gap, but cannot address the domain
content distribution gap that is shown to be important for object detectors. To
overcome this limitation, we propose the Cross-Domain Semi-Supervised Learning
(CDSSL) framework by leveraging high-quality pseudo labels to learn better
representations from the target domain directly. To enable SSL for cross-domain
object detection, we propose fine-grained domain transfer,
progressive-confidence-based label sharpening and imbalanced sampling strategy
to address two challenges: (i) non-identical distribution between source and
target domain data, (ii) error amplification/accumulation due to noisy pseudo
labeling on the target domain. Experiment results show that our proposed
approach consistently achieves new state-of-the-art performance (2.2% - 9.5%
better than prior best work on mAP) under various domain gap scenarios. The
code will be released.",1462,93
['cs.CV'],"Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight","Vision Transformer (ViT) attains state-of-the-art performance in visual
recognition, and the variant, Local Vision Transformer, makes further
improvements. The major component in Local Vision Transformer, local attention,
performs the attention separately over small local windows. We rephrase local
attention as a channel-wise locally-connected layer and analyze it from two
network regularization manners, sparse connectivity and weight sharing, as well
as weight computation. Sparse connectivity: there is no connection across
channels, and each position is connected to the positions within a small local
window. Weight sharing: the connection weights for one position are shared
across channels or within each group of channels. Dynamic weight: the
connection weights are dynamically predicted according to each image instance.
We point out that local attention resembles depth-wise convolution and its
dynamic version in sparse connectivity. The main difference lies in weight
sharing - depth-wise convolution shares connection weights (kernel weights)
across spatial positions. We empirically observe that the models based on
depth-wise convolution and the dynamic variant with lower computation
complexity perform on-par with or sometimes slightly better than Swin
Transformer, an instance of Local Vision Transformer, for ImageNet
classification, COCO object detection and ADE semantic segmentation. These
observations suggest that Local Vision Transformer takes advantage of two
regularization forms and dynamic weight to increase the network capacity.",1562,94
"['cs.LG', 'stat.ML']",ICPRAI 2018 SI: On dynamic ensemble selection and data preprocessing for multi-class imbalance learning,"Class-imbalance refers to classification problems in which many more
instances are available for certain classes than for others. Such imbalanced
datasets require special attention because traditional classifiers generally
favor the majority class which has a large number of instances. Ensemble of
classifiers have been reported to yield promising results. However, the
majority of ensemble methods applied to imbalanced learning are static ones.
Moreover, they only deal with binary imbalanced problems. Hence, this paper
presents an empirical analysis of dynamic selection techniques and data
preprocessing methods for dealing with multi-class imbalanced problems. We
considered five variations of preprocessing methods and fourteen dynamic
selection schemes. Our experiments conducted on 26 multi-class imbalanced
problems show that the dynamic ensemble improves the AUC and the G-mean as
compared to the static ensemble. Moreover, data preprocessing plays an
important role in such cases.",993,103
"['stat.ML', 'cs.LG', 'stat.AP']",Kaggle forecasting competitions: An overlooked learning opportunity,"Competitions play an invaluable role in the field of forecasting, as
exemplified through the recent M4 competition. The competition received
attention from both academics and practitioners and sparked discussions around
the representativeness of the data for business forecasting. Several
competitions featuring real-life business forecasting tasks on the Kaggle
platform has, however, been largely ignored by the academic community. We
believe the learnings from these competitions have much to offer to the
forecasting community and provide a review of the results from six Kaggle
competitions. We find that most of the Kaggle datasets are characterized by
higher intermittence and entropy than the M-competitions and that global
ensemble models tend to outperform local single models. Furthermore, we find
the strong performance of gradient boosted decision trees, increasing success
of neural networks for forecasting, and a variety of techniques for adapting
machine learning models to the forecasting task.",1012,67
['cs.CV'],1st Place Solution to Google Landmark Retrieval 2020,"This paper presents the 1st place solution to the Google Landmark Retrieval
2020 Competition on Kaggle. The solution is based on metric learning to
classify numerous landmark classes, and uses transfer learning with two train
datasets, fine-tuning on bigger images, adjusting loss weight for cleaner
samples, and esemble to enhance the model's performance further. Finally, it
scored 0.38677 mAP@100 on the private leaderboard.",427,52
['cs.CV'],A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering,"While deep convolutional neural networks frequently approach or exceed
human-level performance at benchmark tasks involving static images, extending
this success to moving images is not straightforward. Having models which can
learn to understand video is of interest for many applications, including
content recommendation, prediction, summarization, event/object detection and
understanding human visual perception, but many domains lack sufficient data to
explore and perfect video models. In order to address the need for a simple,
quantitative benchmark for developing and understanding video, we present
MovieFIB, a fill-in-the-blank question-answering dataset with over 300,000
examples, based on descriptive video annotations for the visually impaired. In
addition to presenting statistics and a description of the dataset, we perform
a detailed analysis of 5 different models' predictions, and compare these with
human performance. We investigate the relative importance of language, static
(2D) visual features, and moving (3D) visual features; the effects of
increasing dataset size, the number of frames sampled; and of vocabulary size.
We illustrate that: this task is not solvable by a language model alone; our
model combining 2D and 3D visual information indeed provides the best result;
all models perform significantly worse than human-level. We provide human
evaluations for responses given by different models and find that accuracy on
the MovieFIB evaluation corresponds well with human judgement. We suggest
avenues for improving video models, and hope that the proposed dataset can be
useful for measuring and encouraging progress in this very interesting field.",1685,109
"['cs.CV', 'cs.LG', 'eess.IV']",BDANet: Multiscale Convolutional Neural Network with Cross-directional Attention for Building Damage Assessment from Satellite Images,"Fast and effective responses are required when a natural disaster (e.g.,
earthquake, hurricane, etc.) strikes. Building damage assessment from satellite
imagery is critical before relief effort is deployed. With a pair of pre- and
post-disaster satellite images, building damage assessment aims at predicting
the extent of damage to buildings. With the powerful ability of feature
representation, deep neural networks have been successfully applied to building
damage assessment. Most existing works simply concatenate pre- and
post-disaster images as input of a deep neural network without considering
their correlations. In this paper, we propose a novel two-stage convolutional
neural network for Building Damage Assessment, called BDANet. In the first
stage, a U-Net is used to extract the locations of buildings. Then the network
weights from the first stage are shared in the second stage for building damage
assessment. In the second stage, a two-branch multi-scale U-Net is employed as
backbone, where pre- and post-disaster images are fed into the network
separately. A cross-directional attention module is proposed to explore the
correlations between pre- and post-disaster images. Moreover, CutMix data
augmentation is exploited to tackle the challenge of difficult classes. The
proposed method achieves state-of-the-art performance on a large-scale dataset
-- xBD. The code is available at
https://github.com/ShaneShen/BDANet-Building-Damage-Assessment.",1466,133
"['cs.LG', 'cs.NE', 'stat.ML']",Generalising Recursive Neural Models by Tensor Decomposition,"Most machine learning models for structured data encode the structural
knowledge of a node by leveraging simple aggregation functions (in neural
models, typically a weighted sum) of the information in the node's
neighbourhood. Nevertheless, the choice of simple context aggregation
functions, such as the sum, can be widely sub-optimal. In this work we
introduce a general approach to model aggregation of structural context
leveraging a tensor-based formulation. We show how the exponential growth in
the size of the parameter space can be controlled through an approximation
based on the Tucker tensor decomposition. This approximation allows limiting
the parameters space size, decoupling it from its strict relation with the size
of the hidden encoding space. By this means, we can effectively regulate the
trade-off between expressivity of the encoding, controlled by the hidden size,
computational complexity and model generalisation, influenced by
parameterisation. Finally, we introduce a new Tensorial Tree-LSTM derived as an
instance of our framework and we use it to experimentally assess our working
hypotheses on tree classification scenarios.",1156,60
"['stat.ML', 'cs.LG']",GP-VAE: Deep Probabilistic Time Series Imputation,"Multivariate time series with missing values are common in areas such as
healthcare and finance, and have grown in number and complexity over the years.
This raises the question whether deep learning methodologies can outperform
classical data imputation methods in this domain. However, naive applications
of deep learning fall short in giving reliable confidence estimates and lack
interpretability. We propose a new deep sequential latent variable model for
dimensionality reduction and data imputation. Our modeling assumption is simple
and interpretable: the high dimensional time series has a lower-dimensional
representation which evolves smoothly in time according to a Gaussian process.
The non-linear dimensionality reduction in the presence of missing data is
achieved using a VAE approach with a novel structured variational
approximation. We demonstrate that our approach outperforms several classical
and deep learning-based data imputation methods on high-dimensional data from
the domains of computer vision and healthcare, while additionally improving the
smoothness of the imputations and providing interpretable uncertainty
estimates.",1153,49
"['cs.CV', 'cs.CL', 'cs.LG']",Disentangled Non-Local Neural Networks,"The non-local block is a popular module for strengthening the context
modeling ability of a regular convolutional neural network. This paper first
studies the non-local block in depth, where we find that its attention
computation can be split into two terms, a whitened pairwise term accounting
for the relationship between two pixels and a unary term representing the
saliency of every pixel. We also observe that the two terms trained alone tend
to model different visual clues, e.g. the whitened pairwise term learns
within-region relationships while the unary term learns salient boundaries.
However, the two terms are tightly coupled in the non-local block, which
hinders the learning of each. Based on these findings, we present the
disentangled non-local block, where the two terms are decoupled to facilitate
learning for both terms. We demonstrate the effectiveness of the decoupled
design on various tasks, such as semantic segmentation on Cityscapes, ADE20K
and PASCAL Context, object detection on COCO, and action recognition on
Kinetics.",1050,38
['cs.CV'],Bi-GANs-ST for Perceptual Image Super-resolution,"Image quality measurement is a critical problem for image super-resolution
(SR) algorithms. Usually, they are evaluated by some well-known objective
metrics, e.g., PSNR and SSIM, but these indices cannot provide suitable results
in accordance with the perception of human being. Recently, a more reasonable
perception measurement has been proposed in [1], which is also adopted by the
PIRM-SR 2018 challenge. In this paper, motivated by [1], we aim to generate a
high-quality SR result which balances between the two indices, i.e., the
perception index and root-mean-square error (RMSE). To do so, we design a new
deep SR framework, dubbed Bi-GANs-ST, by integrating two complementary
generative adversarial networks (GAN) branches. One is memory residual SRGAN
(MR-SRGAN), which emphasizes on improving the objective performance, such as
reducing the RMSE. The other is weight perception SRGAN (WP-SRGAN), which
obtains the result that favors better subjective perception via a two-stage
adversarial training mechanism. Then, to produce final result with excellent
perception scores and RMSE, we use soft-thresholding method to merge the
results generated by the two GANs. Our method performs well on the perceptual
image super-resolution task of the PIRM 2018 challenge. Experimental results on
five benchmarks show that our proposal achieves highly competent performance
compared with other state-of-the-art methods.",1419,48
['cs.LG'],Convolutional Dynamic Alignment Networks for Interpretable Classifications,"We introduce a new family of neural network models called Convolutional
Dynamic Alignment Networks (CoDA-Nets), which are performant classifiers with a
high degree of inherent interpretability. Their core building blocks are
Dynamic Alignment Units (DAUs), which linearly transform their input with
weight vectors that dynamically align with task-relevant patterns. As a result,
CoDA-Nets model the classification prediction through a series of
input-dependent linear transformations, allowing for linear decomposition of
the output into individual input contributions. Given the alignment of the
DAUs, the resulting contribution maps align with discriminative input patterns.
These model-inherent decompositions are of high visual quality and outperform
existing attribution methods under quantitative metrics. Further, CoDA-Nets
constitute performant classifiers, achieving on par results to ResNet and VGG
models on e.g. CIFAR-10 and TinyImagenet.",950,74
['cs.CV'],Investigating and Exploiting Image Resolution for Transfer Learning-based Skin Lesion Classification,"Skin cancer is among the most common cancer types. Dermoscopic image analysis
improves the diagnostic accuracy for detection of malignant melanoma and other
pigmented skin lesions when compared to unaided visual inspection. Hence,
computer-based methods to support medical experts in the diagnostic procedure
are of great interest. Fine-tuning pre-trained convolutional neural networks
(CNNs) has been shown to work well for skin lesion classification. Pre-trained
CNNs are usually trained with natural images of a fixed image size which is
typically significantly smaller than captured skin lesion images and
consequently dermoscopic images are downsampled for fine-tuning. However,
useful medical information may be lost during this transformation. In this
paper, we explore the effect of input image size on skin lesion classification
performance of fine-tuned CNNs. For this, we resize dermoscopic images to
different resolutions, ranging from 64x64 to 768x768 pixels and investigate the
resulting classification performance of three well-established CNNs, namely
DenseNet-121, ResNet-18, and ResNet-50. Our results show that using very small
images (of size 64x64 pixels) degrades the classification performance, while
images of size 128x128 pixels and above support good performance with larger
image sizes leading to slightly improved classification. We further propose a
novel fusion approach based on a three-level ensemble strategy that exploits
multiple fine-tuned networks trained with dermoscopic images at various sizes.
When applied on the ISIC 2017 skin lesion classification challenge, our fusion
approach yields an area under the receiver operating characteristic curve of
89.2% and 96.6% for melanoma classification and seborrheic keratosis
classification, respectively, outperforming state-of-the-art algorithms.",1832,100
"['cs.CV', 'cs.LG']",Auxiliary Tasks in Multi-task Learning,"Multi-task convolutional neural networks (CNNs) have shown impressive results
for certain combinations of tasks, such as single-image depth estimation (SIDE)
and semantic segmentation. This is achieved by pushing the network towards
learning a robust representation that generalizes well to different atomic
tasks. We extend this concept by adding auxiliary tasks, which are of minor
relevance for the application, to the set of learned tasks. As a kind of
additional regularization, they are expected to boost the performance of the
ultimately desired main tasks. To study the proposed approach, we picked
vision-based road scene understanding (RSU) as an exemplary application. Since
multi-task learning requires specialized datasets, particularly when using
extensive sets of tasks, we provide a multi-modal dataset for multi-task RSU,
called synMT. More than 2.5 $\cdot$ 10^5 synthetic images, annotated with 21
different labels, were acquired from the video game Grand Theft Auto V (GTA V).
Our proposed deep multi-task CNN architecture was trained on various
combination of tasks using synMT. The experiments confirmed that auxiliary
tasks can indeed boost network performance, both in terms of final results and
training time.",1233,38
"['cs.CV', 'cs.IR']",Extracting Visual Knowledge from the Internet: Making Sense of Image Data,"Recent successes in visual recognition can be primarily attributed to feature
representation, learning algorithms, and the ever-increasing size of labeled
training data. Extensive research has been devoted to the first two, but much
less attention has been paid to the third. Due to the high cost of manual
labeling, the size of recent efforts such as ImageNet is still relatively small
in respect to daily applications. In this work, we mainly focus on how to
automatically generate identifying image data for a given visual concept on a
vast scale. With the generated image data, we can train a robust recognition
model for the given concept. We evaluate the proposed webly supervised approach
on the benchmark Pascal VOC 2007 dataset and the results demonstrates the
superiority of our proposed approach in image data collection.",832,73
"['cs.LG', 'cs.AI']",Neural Relational Inference with Efficient Message Passing Mechanisms,"Many complex processes can be viewed as dynamical systems of interacting
agents. In many cases, only the state sequences of individual agents are
observed, while the interacting relations and the dynamical rules are unknown.
The neural relational inference (NRI) model adopts graph neural networks that
pass messages over a latent graph to jointly learn the relations and the
dynamics based on the observed data. However, NRI infers the relations
independently and suffers from error accumulation in multi-step prediction at
dynamics learning procedure. Besides, relation reconstruction without prior
knowledge becomes more difficult in more complex systems. This paper introduces
efficient message passing mechanisms to the graph neural networks with
structural prior knowledge to address these problems. A relation interaction
mechanism is proposed to capture the coexistence of all relations, and a
spatio-temporal message passing mechanism is proposed to use historical
information to alleviate error accumulation. Additionally, the structural prior
knowledge, symmetry as a special case, is introduced for better relation
prediction in more complex systems. The experimental results on simulated
physics systems show that the proposed method outperforms existing
state-of-the-art methods.",1293,69
"['cs.LG', 'cs.AI']",Proximal Policy Optimization with Mixed Distributed Training,"Instability and slowness are two main problems in deep reinforcement
learning. Even if proximal policy optimization (PPO) is the state of the art,
it still suffers from these two problems. We introduce an improved algorithm
based on proximal policy optimization, mixed distributed proximal policy
optimization (MDPPO), and show that it can accelerate and stabilize the
training process. In our algorithm, multiple different policies train
simultaneously and each of them controls several identical agents that interact
with environments. Actions are sampled by each policy separately as usual, but
the trajectories for the training process are collected from all agents,
instead of only one policy. We find that if we choose some auxiliary
trajectories elaborately to train policies, the algorithm will be more stable
and quicker to converge especially in the environments with sparse rewards.",893,60
"['cs.CV', 'cs.LG']",Towards Efficient Point Cloud Graph Neural Networks Through Architectural Simplification,"In recent years graph neural network (GNN)-based approaches have become a
popular strategy for processing point cloud data, regularly achieving
state-of-the-art performance on a variety of tasks. To date, the research
community has primarily focused on improving model expressiveness, with
secondary thought given to how to design models that can run efficiently on
resource constrained mobile devices including smartphones or mixed reality
headsets. In this work we make a step towards improving the efficiency of these
models by making the observation that these GNN models are heavily limited by
the representational power of their first, feature extracting, layer. We find
that it is possible to radically simplify these models so long as the feature
extraction layer is retained with minimal degradation to model performance;
further, we discover that it is possible to improve performance overall on
ModelNet40 and S3DIS by improving the design of the feature extractor. Our
approach reduces memory consumption by 20$\times$ and latency by up to
9.9$\times$ for graph layers in models such as DGCNN; overall, we achieve
speed-ups of up to 4.5$\times$ and peak memory reductions of 72.5%.",1193,88
"['cs.LG', 'stat.ML']",Towards a General Model of Knowledge for Facial Analysis by Multi-Source Transfer Learning,"This paper proposes a step toward obtaining general models of knowledge for
facial analysis, by addressing the question of multi-source transfer learning.
More precisely, the proposed approach consists in two successive training
steps: the first one consists in applying a combination operator to define a
common embedding for the multiple sources materialized by different existing
trained models. The proposed operator relies on an auto-encoder, trained on a
large dataset, efficient both in terms of compression ratio and transfer
learning performance. In a second step we exploit a distillation approach to
obtain a lightweight student model mimicking the collection of the fused
existing models. This model outperforms its teacher on novel tasks, achieving
results on par with state-of-the-art methods on 15 facial analysis tasks (and
domains), at an affordable training cost. Moreover, this student has 75 times
less parameters than the original teacher and can be applied to a variety of
novel face-related tasks.",1020,90
"['cs.CV', 'cs.LG']",A general approach to bridge the reality-gap,"Employing machine learning models in the real world requires collecting large
amounts of data, which is both time consuming and costly to collect. A common
approach to circumvent this is to leverage existing, similar data-sets with
large amounts of labelled data. However, models trained on these canonical
distributions do not readily transfer to real-world ones. Domain adaptation and
transfer learning are often used to breach this ""reality gap"", though both
require a substantial amount of real-world data. In this paper we discuss a
more general approach: we propose learning a general transformation to bring
arbitrary images towards a canonical distribution where we can naively apply
the trained machine learning models. This transformation is trained in an
unsupervised regime, leveraging data augmentation to generate off-canonical
examples of images and training a Deep Learning model to recover their original
counterpart. We quantify the performance of this transformation using
pre-trained ImageNet classifiers, demonstrating that this procedure can recover
half of the loss in performance on the distorted data-set. We then validate the
effectiveness of this approach on a series of pre-trained ImageNet models on a
real world data set collected by printing and photographing images in different
lighting conditions.",1331,44
['cs.CV'],Utilizing Large Scale Vision and Text Datasets for Image Segmentation from Referring Expressions,"Image segmentation from referring expressions is a joint vision and language
modeling task, where the input is an image and a textual expression describing
a particular region in the image; and the goal is to localize and segment the
specific image region based on the given expression. One major difficulty to
train such language-based image segmentation systems is the lack of datasets
with joint vision and text annotations. Although existing vision datasets such
as MS COCO provide image captions, there are few datasets with region-level
textual annotations for images, and these are often smaller in scale. In this
paper, we explore how existing large scale vision-only and text-only datasets
can be utilized to train models for image segmentation from referring
expressions. We propose a method to address this problem, and show in
experiments that our method can help this joint vision and language modeling
task with vision-only and text-only data and outperforms previous results.",990,96
"['cs.LG', 'cs.AI']",Forgetful Experience Replay in Hierarchical Reinforcement Learning from Demonstrations,"Currently, deep reinforcement learning (RL) shows impressive results in
complex gaming and robotic environments. Often these results are achieved at
the expense of huge computational costs and require an incredible number of
episodes of interaction between the agent and the environment. There are two
main approaches to improving the sample efficiency of reinforcement learning
methods - using hierarchical methods and expert demonstrations. In this paper,
we propose a combination of these approaches that allow the agent to use
low-quality demonstrations in complex vision-based environments with multiple
related goals. Our forgetful experience replay (ForgER) algorithm effectively
handles errors in expert data and reduces quality losses when adapting the
action space and states representation to the agent's capabilities. Our
proposed goal-oriented structuring of replay buffer allows the agent to
automatically highlight sub-goals for solving complex hierarchical tasks in
demonstrations. Our method is universal and can be integrated into various
off-policy methods. It surpasses all known existing state-of-the-art RL methods
using expert demonstrations on various model environments. The solution based
on our algorithm beats all the solutions for the famous MineRL competition and
allows the agent to mine a diamond in the Minecraft environment.",1358,86
"['cs.LG', 'cs.AI']",CoBERL: Contrastive BERT for Reinforcement Learning,"Many reinforcement learning (RL) agents require a large amount of experience
to solve tasks. We propose Contrastive BERT for RL (CoBERL), an agent that
combines a new contrastive loss and a hybrid LSTM-transformer architecture to
tackle the challenge of improving data efficiency. CoBERL enables efficient,
robust learning from pixels across a wide range of domains. We use
bidirectional masked prediction in combination with a generalization of recent
contrastive methods to learn better representations for transformers in RL,
without the need of hand engineered data augmentations. We find that CoBERL
consistently improves performance across the full Atari suite, a set of control
tasks and a challenging 3D environment.",724,51
"['cs.LG', 'stat.ML']",Differentiable Causal Backdoor Discovery,"Discovering the causal effect of a decision is critical to nearly all forms
of decision-making. In particular, it is a key quantity in drug development, in
crafting government policy, and when implementing a real-world machine learning
system. Given only observational data, confounders often obscure the true
causal effect. Luckily, in some cases, it is possible to recover the causal
effect by using certain observed variables to adjust for the effects of
confounders. However, without access to the true causal model, finding this
adjustment requires brute-force search. In this work, we present an algorithm
that exploits auxiliary variables, similar to instruments, in order to find an
appropriate adjustment by a gradient-based optimization method. We demonstrate
that it outperforms practical alternatives in estimating the true causal
effect, without knowledge of the full causal graph.",894,40
"['cs.LG', 'cs.AI']",Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement,"The ability to transfer skills across tasks has the potential to scale up
reinforcement learning (RL) agents to environments currently out of reach.
Recently, a framework based on two ideas, successor features (SFs) and
generalised policy improvement (GPI), has been introduced as a principled way
of transferring skills. In this paper we extend the SFs & GPI framework in two
ways. One of the basic assumptions underlying the original formulation of SFs &
GPI is that rewards for all tasks of interest can be computed as linear
combinations of a fixed set of features. We relax this constraint and show that
the theoretical guarantees supporting the framework can be extended to any set
of tasks that only differ in the reward function. Our second contribution is to
show that one can use the reward functions themselves as features for future
tasks, without any loss of expressiveness, thus removing the need to specify a
set of features beforehand. This makes it possible to combine SFs & GPI with
deep learning in a more stable way. We empirically verify this claim on a
complex 3D environment where observations are images from a first-person
perspective. We show that the transfer promoted by SFs & GPI leads to very good
policies on unseen tasks almost instantaneously. We also describe how to learn
policies specialised to the new tasks in a way that allows them to be added to
the agent's set of skills, and thus be reused in the future.",1446,99
"['cs.LG', 'cs.SI', 'stat.ML']",CAGNN: Cluster-Aware Graph Neural Networks for Unsupervised Graph Representation Learning,"Unsupervised graph representation learning aims to learn low-dimensional node
embeddings without supervision while preserving graph topological structures
and node attributive features. Previous graph neural networks (GNN) require a
large number of labeled nodes, which may not be accessible in real-world graph
data. In this paper, we present a novel cluster-aware graph neural network
(CAGNN) model for unsupervised graph representation learning using
self-supervised techniques. In CAGNN, we perform clustering on the node
embeddings and update the model parameters by predicting the cluster
assignments. Moreover, we observe that graphs often contain inter-class edges,
which mislead the GNN model to aggregate noisy information from neighborhood
nodes. We further refine the graph topology by strengthening intra-class edges
and reducing node connections between different classes based on cluster
labels, which better preserves cluster structures in the embedding space. We
conduct comprehensive experiments on two benchmark tasks using real-world
datasets. The results demonstrate the superior performance of the proposed
model over existing baseline methods. Notably, our model gains over 7%
improvements in terms of accuracy on node clustering over state-of-the-arts.",1276,89
"['cs.CV', 'cs.AI']",Siamese Network Training Using Artificial Triplets By Sampling and Image Transformation,"The device used in this work detects the objects over the surface of the
water using two thermal cameras which aid the users to detect and avoid the
objects in scenarios where the human eyes cannot (night, fog, etc.). To avoid
the obstacle collision autonomously, it is required to track the objects in
real-time and assign a specific identity to each object to determine its
dynamics (trajectory, velocity, etc.) for making estimated collision
predictions. In the following work, a Machine Learning (ML) approach for
Computer Vision (CV) called Convolutional Neural Network (CNN) was used using
TensorFlow as the high-level programming environment in Python. To validate the
algorithm a test set was generated using an annotation tool that was created
during the work for proper evaluation. Once validated, the algorithm was
deployed on the platform and tested with the sequence generated by the test
boat.",907,87
['cs.CV'],RIT-Eyes: Rendering of near-eye images for eye-tracking applications,"Deep neural networks for video-based eye tracking have demonstrated
resilience to noisy environments, stray reflections, and low resolution.
However, to train these networks, a large number of manually annotated images
are required. To alleviate the cumbersome process of manual labeling, computer
graphics rendering is employed to automatically generate a large corpus of
annotated eye images under various conditions. In this work, we introduce a
synthetic eye image generation platform that improves upon previous work by
adding features such as an active deformable iris, an aspherical cornea,
retinal retro-reflection, gaze-coordinated eye-lid deformations, and blinks. To
demonstrate the utility of our platform, we render images reflecting the
represented gaze distributions inherent in two publicly available datasets,
NVGaze and OpenEDS. We also report on the performance of two semantic
segmentation architectures (SegNet and RITnet) trained on rendered images and
tested on the original datasets.",1007,68
"['cs.CV', 'cs.AI']",Skin-color based videos categorization,"On dedicated websites, people can upload videos and share it with the rest of
the world. Currently these videos are cat- egorized manually by the help of the
user community. In this paper, we propose a combination of color spaces with
the Bayesian network approach for robust detection of skin color followed by an
automated video categorization. Exper- imental results show that our method can
achieve satisfactory performance for categorizing videos based on skin color.",472,38
['cs.LG'],Improving Adversarial Robustness via Channel-wise Activation Suppressing,"The study of adversarial examples and their activation has attracted
significant attention for secure and robust learning with deep neural networks
(DNNs). Different from existing works, in this paper, we highlight two new
characteristics of adversarial examples from the channel-wise activation
perspective: 1) the activation magnitudes of adversarial examples are higher
than that of natural examples; and 2) the channels are activated more uniformly
by adversarial examples than natural examples. We find that the
state-of-the-art defense adversarial training has addressed the first issue of
high activation magnitudes via training on adversarial examples, while the
second issue of uniform activation remains. This motivates us to suppress
redundant activation from being activated by adversarial perturbations via a
Channel-wise Activation Suppressing (CAS) strategy. We show that CAS can train
a model that inherently suppresses adversarial activation, and can be easily
applied to existing defense methods to further improve their robustness. Our
work provides a simple but generic training strategy for robustifying the
intermediate layer activation of DNNs.",1167,72
"['cs.LG', 'cs.AI', 'cs.SY', 'stat.ML']",Worst-Case Regret Bounds for Exploration via Randomized Value Functions,"This paper studies a recent proposal to use randomized value functions to
drive exploration in reinforcement learning. These randomized value functions
are generated by injecting random noise into the training data, making the
approach compatible with many popular methods for estimating parameterized
value functions. By providing a worst-case regret bound for tabular
finite-horizon Markov decision processes, we show that planning with respect to
these randomized value functions can induce provably efficient exploration.",525,71
"['cs.CV', 'cs.LG', 'eess.IV']",3D dynamic hand gestures recognition using the Leap Motion sensor and convolutional neural networks,"Defining methods for the automatic understanding of gestures is of paramount
importance in many application contexts and in Virtual Reality applications for
creating more natural and easy-to-use human-computer interaction methods. In
this paper, we present a method for the recognition of a set of non-static
gestures acquired through the Leap Motion sensor. The acquired gesture
information is converted in color images, where the variation of hand joint
positions during the gesture are projected on a plane and temporal information
is represented with color intensity of the projected points. The classification
of the gestures is performed using a deep Convolutional Neural Network (CNN). A
modified version of the popular ResNet-50 architecture is adopted, obtained by
removing the last fully connected layer and adding a new layer with as many
neurons as the considered gesture classes. The method has been successfully
applied to the existing reference dataset and preliminary tests have already
been performed for the real-time recognition of dynamic gestures performed by
users.",1087,99
['cs.CV'],Generative adversarial networks and adversarial methods in biomedical image analysis,"Generative adversarial networks (GANs) and other adversarial methods are
based on a game-theoretical perspective on joint optimization of two neural
networks as players in a game. Adversarial techniques have been extensively
used to synthesize and analyze biomedical images. We provide an introduction to
GANs and adversarial methods, with an overview of biomedical image analysis
tasks that have benefited from such methods. We conclude with a discussion of
strengths and limitations of adversarial methods in biomedical image analysis,
and propose potential future research directions.",587,84
"['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML']",Learning Non-Parametric Invariances from Data with Permanent Random Connectomes,"One of the fundamental problems in supervised classification and in machine
learning in general, is the modelling of non-parametric invariances that exist
in data. Most prior art has focused on enforcing priors in the form of
invariances to parametric nuisance transformations that are expected to be
present in data. Learning non-parametric invariances directly from data remains
an important open problem. In this paper, we introduce a new architectural
layer for convolutional networks which is capable of learning general
invariances from data itself. This layer can learn invariance to non-parametric
transformations and interestingly, motivates and incorporates permanent random
connectomes, thereby being called Permanent Random Connectome Non-Parametric
Transformation Networks (PRC-NPTN). PRC-NPTN networks are initialized with
random connections (not just weights) which are a small subset of the
connections in a fully connected convolution layer. Importantly, these
connections in PRC-NPTNs once initialized remain permanent throughout training
and testing. Permanent random connectomes make these architectures loosely more
biologically plausible than many other mainstream network architectures which
require highly ordered structures. We motivate randomly initialized connections
as a simple method to learn invariance from data itself while invoking
invariance towards multiple nuisance transformations simultaneously. We find
that these randomly initialized permanent connections have positive effects on
generalization, outperform much larger ConvNet baselines and the recently
proposed Non-Parametric Transformation Network (NPTN) on benchmarks that
enforce learning invariances from the data itself.",1719,79
"['cs.LG', 'stat.ML']",STRODE: Stochastic Boundary Ordinary Differential Equation,"Perception of time from sequentially acquired sensory inputs is rooted in
everyday behaviors of individual organisms. Yet, most algorithms for
time-series modeling fail to learn dynamics of random event timings directly
from visual or audio inputs, requiring timing annotations during training that
are usually unavailable for real-world applications. For instance, neuroscience
perspectives on postdiction imply that there exist variable temporal ranges
within which the incoming sensory inputs can affect the earlier perception, but
such temporal ranges are mostly unannotated for real applications such as
automatic speech recognition (ASR). In this paper, we present a probabilistic
ordinary differential equation (ODE), called STochastic boundaRy ODE (STRODE),
that learns both the timings and the dynamics of time series data without
requiring any timing annotations during training. STRODE allows the usage of
differential equations to sample from the posterior point processes,
efficiently and analytically. We further provide theoretical guarantees on the
learning of STRODE. Our empirical results show that our approach successfully
infers event timings of time series data. Our method achieves competitive or
superior performances compared to existing state-of-the-art methods for both
synthetic and real-world datasets.",1331,58
"['cs.LG', 'cs.AI', 'cs.ET', '68T05', 'I.2.6; B.m']",Hybrid computer approach to train a machine learning system,"This book chapter describes a novel approach to training machine learning
systems by means of a hybrid computer setup i.e. a digital computer tightly
coupled with an analog computer. As an example a reinforcement learning system
is trained to balance an inverted pendulum which is simulated on an analog
computer, thus demonstrating a solution to the major challenge of adequately
simulating the environment for reinforcement learning.",435,59
"['cs.LG', 'math.OC']","SOLO: Search Online, Learn Offline for Combinatorial Optimization Problems","We study combinatorial problems with real world applications such as machine
scheduling, routing, and assignment. We propose a method that combines
Reinforcement Learning (RL) and planning. This method can equally be applied to
both the offline, as well as online, variants of the combinatorial problem, in
which the problem components (e.g., jobs in scheduling problems) are not known
in advance, but rather arrive during the decision-making process. Our solution
is quite generic, scalable, and leverages distributional knowledge of the
problem parameters. We frame the solution process as an MDP, and take a Deep
Q-Learning approach wherein states are represented as graphs, thereby allowing
our trained policies to deal with arbitrary changes in a principled manner.
Though learned policies work well in expectation, small deviations can have
substantial negative effects in combinatorial settings. We mitigate these
drawbacks by employing our graph-convolutional policies as non-optimal
heuristics in a compatible search algorithm, Monte Carlo Tree Search, to
significantly improve overall performance. We demonstrate our method on two
problems: Machine Scheduling and Capacitated Vehicle Routing. We show that our
method outperforms custom-tailored mathematical solvers, state of the art
learning-based algorithms, and common heuristics, both in computation time and
performance.",1385,74
"['cs.LG', 'cs.AI', 'stat.ML']",TimeAutoML: Autonomous Representation Learning for Multivariate Irregularly Sampled Time Series,"Multivariate time series (MTS) data are becoming increasingly ubiquitous in
diverse domains, e.g., IoT systems, health informatics, and 5G networks. To
obtain an effective representation of MTS data, it is not only essential to
consider unpredictable dynamics and highly variable lengths of these data but
also important to address the irregularities in the sampling rates of MTS.
Existing parametric approaches rely on manual hyperparameter tuning and may
cost a huge amount of labor effort. Therefore, it is desirable to learn the
representation automatically and efficiently. To this end, we propose an
autonomous representation learning approach for multivariate time series
(TimeAutoML) with irregular sampling rates and variable lengths. As opposed to
previous works, we first present a representation learning pipeline in which
the configuration and hyperparameter optimization are fully automatic and can
be tailored for various tasks, e.g., anomaly detection, clustering, etc. Next,
a negative sample generation approach and an auxiliary classification task are
developed and integrated within TimeAutoML to enhance its representation
capability. Extensive empirical studies on real-world datasets demonstrate that
the proposed TimeAutoML outperforms competing approaches on various tasks by a
large margin. In fact, it achieves the best anomaly detection performance among
all comparison algorithms on 78 out of all 85 UCR datasets, acquiring up to 20%
performance improvement in terms of AUC score.",1509,95
"['cs.LG', 'stat.ML']",Visualizing the Loss Landscape of Actor Critic Methods with Applications in Inventory Optimization,"Continuous control is a widely applicable area of reinforcement learning. The
main players of this area are actor-critic methods that utilize policy
gradients of neural approximators as a common practice. The focus of our study
is to show the characteristics of the actor loss function which is the
essential part of the optimization. We exploit low dimensional visualizations
of the loss function and provide comparisons for loss landscapes of various
algorithms. Furthermore, we apply our approach to multi-store dynamic inventory
control, a notoriously difficult problem in supply chain operations, and
explore the shape of the loss function associated with the optimal policy. We
modelled and solved the problem using reinforcement learning while having a
loss landscape in favor of optimality.",798,98
['cs.CV'],The 1st Tiny Object Detection Challenge:Methods and Results,"The 1st Tiny Object Detection (TOD) Challenge aims to encourage research in
developing novel and accurate methods for tiny object detection in images which
have wide views, with a current focus on tiny person detection. The TinyPerson
dataset was used for the TOD Challenge and is publicly released. It has 1610
images and 72651 box-levelannotations. Around 36 participating teams from the
globe competed inthe 1st TOD Challenge. In this paper, we provide a brief
summary of the1st TOD Challenge including brief introductions to the top three
methods.The submission leaderboard will be reopened for researchers that
areinterested in the TOD challenge. The benchmark dataset and other information
can be found at: https://github.com/ucas-vg/TinyBenchmark.",754,59
"['cs.LG', 'stat.ML']",Sinkhorn Barycenter via Functional Gradient Descent,"In this paper, we consider the problem of computing the barycenter of a set
of probability distributions under the Sinkhorn divergence.
  This problem has recently found applications across various domains,
including graphics, learning, and vision, as it provides a meaningful mechanism
to aggregate knowledge.
  Unlike previous approaches which directly operate in the space of probability
measures, we recast the Sinkhorn barycenter problem as an instance of
unconstrained functional optimization and develop a novel functional gradient
descent method named Sinkhorn Descent (SD).
  We prove that SD converges to a stationary point at a sublinear rate, and
under reasonable assumptions, we further show that it asymptotically finds a
global minimizer of the Sinkhorn barycenter problem. Moreover, by providing a
mean-field analysis, we show that SD preserves the weak convergence of
empirical measures.
  Importantly, the computational complexity of SD scales linearly in the
dimension $d$ and we demonstrate its scalability by solving a $100$-dimensional
Sinkhorn barycenter problem.",1086,51
"['cs.LG', 'cs.CL']",Learning Slice-Aware Representations with Mixture of Attentions,"Real-world machine learning systems are achieving remarkable performance in
terms of coarse-grained metrics like overall accuracy and F-1 score. However,
model improvement and development often require fine-grained modeling on
individual data subsets or slices, for instance, the data slices where the
models have unsatisfactory results. In practice, it gives tangible values for
developing such models that can pay extra attention to critical or interested
slices while retaining the original overall performance. This work extends the
recent slice-based learning (SBL)~\cite{chen2019slice} with a mixture of
attentions (MoA) to learn slice-aware dual attentive representations. We
empirically show that the MoA approach outperforms the baseline method as well
as the original SBL approach on monitored slices with two natural language
understanding (NLU) tasks.",863,63
"['cs.CV', 'cs.LG', 'cs.RO']",Scene Transformer: A unified multi-task model for behavior prediction and planning,"Predicting the future motion of multiple agents is necessary for planning in
dynamic environments. This task is challenging for autonomous driving since
agents (e.g., vehicles and pedestrians) and their associated behaviors may be
diverse and influence each other. Most prior work has focused on first
predicting independent futures for each agent based on all past motion, and
then planning against these independent predictions. However, planning against
fixed predictions can suffer from the inability to represent the future
interaction possibilities between different agents, leading to sub-optimal
planning. In this work, we formulate a model for predicting the behavior of all
agents jointly in real-world driving environments in a unified manner. Inspired
by recent language modeling approaches, we use a masking strategy as the query
to our model, enabling one to invoke a single model to predict agent behavior
in many ways, such as potentially conditioned on the goal or full future
trajectory of the autonomous vehicle or the behavior of other agents in the
environment. Our model architecture fuses heterogeneous world state in a
unified Transformer architecture by employing attention across road elements,
agent interactions and time steps. We evaluate our approach on autonomous
driving datasets for behavior prediction, and achieve state-of-the-art
performance. Our work demonstrates that formulating the problem of behavior
prediction in a unified architecture with a masking strategy may allow us to
have a single model that can perform multiple motion prediction and planning
related tasks effectively.",1622,82
['cs.CV'],Cascade Convolutional Neural Network for Image Super-Resolution,"With the development of the super-resolution convolutional neural network
(SRCNN), deep learning technique has been widely applied in the field of image
super-resolution. Previous works mainly focus on optimizing the structure of
SRCNN, which have been achieved well performance in speed and restoration
quality for image super-resolution. However, most of these approaches only
consider a specific scale image during the training process, while ignoring the
relationship between different scales of images. Motivated by this concern, in
this paper, we propose a cascaded convolution neural network for image
super-resolution (CSRCNN), which includes three cascaded Fast SRCNNs and each
Fast SRCNN can process a specific scale image. Images of different scales can
be trained simultaneously and the learned network can make full use of the
information resided in different scales of images. Extensive experiments show
that our network can achieve well performance for image SR.",977,63
['cs.CV'],Detection and Localization of Robotic Tools in Robot-Assisted Surgery Videos Using Deep Neural Networks for Region Proposal and Detection,"Video understanding of robot-assisted surgery (RAS) videos is an active
research area. Modeling the gestures and skill level of surgeons presents an
interesting problem. The insights drawn may be applied in effective skill
acquisition, objective skill assessment, real-time feedback, and human-robot
collaborative surgeries. We propose a solution to the tool detection and
localization open problem in RAS video understanding, using a strictly computer
vision approach and the recent advances of deep learning. We propose an
architecture using multimodal convolutional neural networks for fast detection
and localization of tools in RAS videos. To our knowledge, this approach will
be the first to incorporate deep neural networks for tool detection and
localization in RAS videos. Our architecture applies a Region Proposal Network
(RPN), and a multi-modal two stream convolutional network for object detection,
to jointly predict objectness and localization on a fusion of image and
temporal motion cues. Our results with an Average Precision (AP) of 91% and a
mean computation time of 0.1 seconds per test frame detection indicate that our
study is superior to conventionally used methods for medical imaging while also
emphasizing the benefits of using RPN for precision and efficiency. We also
introduce a new dataset, ATLAS Dione, for RAS video understanding. Our dataset
provides video data of ten surgeons from Roswell Park Cancer Institute (RPCI)
(Buffalo, NY) performing six different surgical tasks on the daVinci Surgical
System (dVSS R ) with annotations of robotic tools per frame.",1595,137
"['cs.CV', 'cs.LG']","Towards Explainable, Privacy-Preserved Human-Motion Affect Recognition","Human motion characteristics are used to monitor the progression of
neurological diseases and mood disorders. Since perceptions of emotions are
also interleaved with body posture and movements, emotion recognition from
human gait can be used to quantitatively monitor mood changes. Many existing
solutions often use shallow machine learning models with raw positional data or
manually extracted features to achieve this. However, gait is composed of many
highly expressive characteristics that can be used to identify human subjects,
and most solutions fail to address this, disregarding the subject's privacy.
This work introduces a novel deep neural network architecture to disentangle
human emotions and biometrics. In particular, we propose a cross-subject
transfer learning technique for training a multi-encoder autoencoder deep
neural network to learn disentangled latent representations of human motion
features. By disentangling subject biometrics from the gait data, we show that
the subject's privacy is preserved while the affect recognition performance
outperforms traditional methods. Furthermore, we exploit Guided Grad-CAM to
provide global explanations of the model's decision across gait cycles. We
evaluate the effectiveness of our method to existing methods at recognizing
emotions using both 3D temporal joint signals and manually extracted features.
We also show that this data can easily be exploited to expose a subject's
identity. Our method shows up to 7% improvement and highlights the joints with
the most significant influence across the average gait cycle.",1586,70
"['cs.LG', 'cs.AI', 'stat.ML']",Quantity vs. Quality: On Hyperparameter Optimization for Deep Reinforcement Learning,"Reinforcement learning algorithms can show strong variation in performance
between training runs with different random seeds. In this paper we explore how
this affects hyperparameter optimization when the goal is to find
hyperparameter settings that perform well across random seeds. In particular,
we benchmark whether it is better to explore a large quantity of hyperparameter
settings via pruning of bad performers, or if it is better to aim for quality
of collected results by using repetitions. For this we consider the Successive
Halving, Random Search, and Bayesian Optimization algorithms, the latter two
with and without repetitions. We apply these to tuning the PPO2 algorithm on
the Cartpole balancing task and the Inverted Pendulum Swing-up task. We
demonstrate that pruning may negatively affect the optimization and that
repeated sampling does not help in finding hyperparameter settings that perform
better across random seeds. From our experiments we conclude that Bayesian
optimization with a noise robust acquisition function is the best choice for
hyperparameter optimization in reinforcement learning tasks.",1127,84
"['cs.LG', 'stat.ML']",A Kernel-Based Approach to Non-Stationary Reinforcement Learning in Metric Spaces,"In this work, we propose KeRNS: an algorithm for episodic reinforcement
learning in non-stationary Markov Decision Processes (MDPs) whose state-action
set is endowed with a metric. Using a non-parametric model of the MDP built
with time-dependent kernels, we prove a regret bound that scales with the
covering dimension of the state-action space and the total variation of the MDP
with time, which quantifies its level of non-stationarity. Our method
generalizes previous approaches based on sliding windows and exponential
discounting used to handle changing environments. We further propose a
practical implementation of KeRNS, we analyze its regret and validate it
experimentally.",683,81
['cs.CV'],Hierarchical Attention Network for Action Segmentation,"The temporal segmentation of events is an essential task and a precursor for
the automatic recognition of human actions in the video. Several attempts have
been made to capture frame-level salient aspects through attention but they
lack the capacity to effectively map the temporal relationships in between the
frames as they only capture a limited span of temporal dependencies. To this
end we propose a complete end-to-end supervised learning approach that can
better learn relationships between actions over time, thus improving the
overall segmentation performance. The proposed hierarchical recurrent attention
framework analyses the input video at multiple temporal scales, to form
embeddings at frame level and segment level, and perform fine-grained action
segmentation. This generates a simple, lightweight, yet extremely effective
architecture for segmenting continuous video streams and has multiple
application domains. We evaluate our system on multiple challenging public
benchmark datasets, including MERL Shopping, 50 salads, and Georgia Tech
Egocentric datasets, and achieves state-of-the-art performance. The evaluated
datasets encompass numerous video capture settings which are inclusive of
static overhead camera views and dynamic, ego-centric head-mounted camera
views, demonstrating the direct applicability of the proposed framework in a
variety of settings.",1382,54
"['cs.LG', 'stat.ML']",Improved Information Gain Estimates for Decision Tree Induction,"Ensembles of classification and regression trees remain popular machine
learning methods because they define flexible non-parametric models that
predict well and are computationally efficient both during training and
testing. During induction of decision trees one aims to find predicates that
are maximally informative about the prediction target. To select good
predicates most approaches estimate an information-theoretic scoring function,
the information gain, both for classification and regression problems. We point
out that the common estimation procedures are biased and show that by replacing
them with improved estimators of the discrete and the differential entropy we
can obtain better decision trees. In effect our modifications yield improved
predictive performance and are simple to implement in any decision tree code.",835,63
"['cs.LG', 'cs.AI', 'stat.ML']",Stabilizing Deep Reinforcement Learning with Conservative Updates,"In recent years, advances in deep learning have enabled the application of
reinforcement learning algorithms in complex domains. However, they lack the
theoretical guarantees which are present in the tabular setting and suffer from
many stability and reproducibility problems \citep{henderson2018deep}. In this
work, we suggest a simple approach for improving stability and providing
probabilistic performance improvement in off-policy actor-critic deep
reinforcement learning regimes. Experiments on continuous action spaces, in the
MuJoCo control suite, show that our proposed method reduces the variance of the
process and improves the overall performance.",659,65
"['cs.LG', 'q-bio.QM']",Knowledge-aware Contrastive Molecular Graph Learning,"Leveraging domain knowledge including fingerprints and functional groups in
molecular representation learning is crucial for chemical property prediction
and drug discovery. When modeling the relation between graph structure and
molecular properties implicitly, existing works can hardly capture structural
or property changes and complex structure, with much smaller atom vocabulary
and highly frequent atoms. In this paper, we propose the Contrastive
Knowledge-aware GNN (CKGNN) for self-supervised molecular representation
learning to fuse domain knowledge into molecular graph representation. We
explicitly encode domain knowledge via knowledge-aware molecular encoder under
the contrastive learning framework, ensuring that the generated molecular
embeddings equipped with chemical domain knowledge to distinguish molecules
with similar chemical formula but dissimilar functions. Extensive experiments
on 8 public datasets demonstrate the effectiveness of our model with a 6\%
absolute improvement on average against strong competitors. Ablation study and
further investigation also verify the best of both worlds: incorporation of
chemical domain knowledge into self-supervised learning.",1193,52
"['cs.LG', 'cs.CV']",Convolutional Clustering for Unsupervised Learning,"The task of labeling data for training deep neural networks is daunting and
tedious, requiring millions of labels to achieve the current state-of-the-art
results. Such reliance on large amounts of labeled data can be relaxed by
exploiting hierarchical features via unsupervised learning techniques. In this
work, we propose to train a deep convolutional network based on an enhanced
version of the k-means clustering algorithm, which reduces the number of
correlated parameters in the form of similar filters, and thus increases test
categorization accuracy. We call our algorithm convolutional k-means
clustering. We further show that learning the connection between the layers of
a deep convolutional neural network improves its ability to be trained on a
smaller amount of labeled data. Our experiments show that the proposed
algorithm outperforms other techniques that learn filters unsupervised.
Specifically, we obtained a test accuracy of 74.1% on STL-10 and a test error
of 0.5% on MNIST.",996,50
['cs.CV'],Adversarial Image Alignment and Interpolation,"Volumetric (3d) images are acquired for many scientific and biomedical
purposes using imaging methods such as serial section microscopy, CT scans, and
MRI. A frequent step in the analysis and reconstruction of such data is the
alignment and registration of images that were acquired in succession along a
spatial or temporal dimension. For example, in serial section electron
microscopy, individual 2d sections are imaged via electron microscopy and then
must be aligned to one another in order to produce a coherent 3d volume. State
of the art approaches find image correspondences derived from patch matching
and invariant feature detectors, and then solve optimization problems that
rigidly or elastically deform series of images into an aligned volume. Here we
show how fully convolutional neural networks trained with an adversarial loss
function can be used for two tasks: (1) synthesis of missing or damaged image
data from adjacent sections, and (2) fine-scale alignment of block-face
electron microscopy data. Finally, we show how these two capabilities can be
combined in order to produce artificial isotropic volumes from anisotropic
image volumes using a super-resolution adversarial alignment and interpolation
approach.",1233,45
"['cs.LG', 'cs.AI', 'stat.ML']",Investigating Generalisation in Continuous Deep Reinforcement Learning,"Deep Reinforcement Learning has shown great success in a variety of control
tasks. However, it is unclear how close we are to the vision of putting Deep RL
into practice to solve real world problems. In particular, common practice in
the field is to train policies on largely deterministic simulators and to
evaluate algorithms through training performance alone, without a train/test
distinction to ensure models generalise and are not overfitted. Moreover, it is
not standard practice to check for generalisation under domain shift, although
robustness to such system change between training and testing would be
necessary for real-world Deep RL control, for example, in robotics. In this
paper we study these issues by first characterising the sources of uncertainty
that provide generalisation challenges in Deep RL. We then provide a new
benchmark and thorough empirical evaluation of generalisation challenges for
state of the art Deep RL methods. In particular, we show that, if
generalisation is the goal, then common practice of evaluating algorithms based
on their training performance leads to the wrong conclusions about algorithm
choice. Finally, we evaluate several techniques for improving generalisation
and draw conclusions about the most robust techniques to date.",1282,70
"['cs.CV', 'cs.CR', 'cs.LG']",Learning Discriminators as Energy Networks in Adversarial Learning,"We propose a novel framework for structured prediction via adversarial
learning. Existing adversarial learning methods involve two separate networks,
i.e., the structured prediction models and the discriminative models, in the
training. The information captured by discriminative models complements that in
the structured prediction models, but few existing researches have studied on
utilizing such information to improve structured prediction models at the
inference stage. In this work, we propose to refine the predictions of
structured prediction models by effectively integrating discriminative models
into the prediction. Discriminative models are treated as energy-based models.
Similar to the adversarial learning, discriminative models are trained to
estimate scores which measure the quality of predicted outputs, while
structured prediction models are trained to predict contrastive outputs with
maximal energy scores. In this way, the gradient vanishing problem is
ameliorated, and thus we are able to perform inference by following the ascent
gradient directions of discriminative models to refine structured prediction
models. The proposed method is able to handle a range of tasks, e.g.,
multi-label classification and image segmentation. Empirical results on these
two tasks validate the effectiveness of our learning method.",1342,66
['cs.CV'],Factors of Transferability for a Generic ConvNet Representation,"Evidence is mounting that Convolutional Networks (ConvNets) are the most
effective representation learning method for visual recognition tasks. In the
common scenario, a ConvNet is trained on a large labeled dataset (source) and
the feed-forward units activation of the trained network, at a certain layer of
the network, is used as a generic representation of an input image for a task
with relatively smaller training set (target). Recent studies have shown this
form of representation transfer to be suitable for a wide range of target
visual recognition tasks. This paper introduces and investigates several
factors affecting the transferability of such representations. It includes
parameters for training of the source ConvNet such as its architecture,
distribution of the training data, etc. and also the parameters of feature
extraction such as layer of the trained ConvNet, dimensionality reduction, etc.
Then, by optimizing these factors, we show that significant improvements can be
achieved on various (17) visual recognition tasks. We further show that these
visual recognition tasks can be categorically ordered based on their distance
from the source task such that a correlation between the performance of tasks
and their distance from the source task w.r.t. the proposed factors is
observed.",1308,63
['cs.LG'],Identifying Decision Points for Safe and Interpretable Reinforcement Learning in Hypotension Treatment,"Many batch RL health applications first discretize time into fixed intervals.
However, this discretization both loses resolution and forces a policy
computation at each (potentially fine) interval. In this work, we develop a
novel framework to compress continuous trajectories into a few, interpretable
decision points --places where the batch data support multiple alternatives. We
apply our approach to create recommendations from a cohort of hypotensive
patients dataset. Our reduced state space results in faster planning and allows
easy inspection by a clinical expert.",574,102
"['cs.LG', 'cs.CV']",Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity,"Neural network pruning is a fruitful area of research with surging interest
in high sparsity regimes. Benchmarking in this domain heavily relies on
faithful representation of the sparsity of subnetworks, which has been
traditionally computed as the fraction of removed connections (direct
sparsity). This definition, however, fails to recognize unpruned parameters
that detached from input or output layers of underlying subnetworks,
potentially underestimating actual effective sparsity: the fraction of
inactivated connections. While this effect might be negligible for moderately
pruned networks (up to 10-100 compression rates), we find that it plays an
increasing role for thinner subnetworks, greatly distorting comparison between
different pruning algorithms. For example, we show that effective compression
of a randomly pruned LeNet-300-100 can be orders of magnitude larger than its
direct counterpart, while no discrepancy is ever observed when using SynFlow
for pruning [Tanaka et al., 2020]. In this work, we adopt the lens of effective
sparsity to reevaluate several recent pruning algorithms on common benchmark
architectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and discover that their
absolute and relative performance changes dramatically in this new and more
appropriate framework. To aim for effective, rather than direct, sparsity, we
develop a low-cost extension to most pruning algorithms. Further, equipped with
effective sparsity as a reference frame, we partially reconfirm that random
pruning with appropriate sparsity allocation across layers performs as well or
better than more sophisticated algorithms for pruning at initialization [Su et
al., 2020]. In response to this observation, using a simple analogy of pressure
distribution in coupled cylinders from physics, we design novel layerwise
sparsity quotas that outperform all existing baselines in the context of random
pruning.",1916,83
['cs.CV'],AutoTrajectory: Label-free Trajectory Extraction and Prediction from Videos using Dynamic Points,"Current methods for trajectory prediction operate in supervised manners, and
therefore require vast quantities of corresponding ground truth data for
training. In this paper, we present a novel, label-free algorithm,
AutoTrajectory, for trajectory extraction and prediction to use raw videos
directly. To better capture the moving objects in videos, we introduce dynamic
points. We use them to model dynamic motions by using a forward-backward
extractor to keep temporal consistency and using image reconstruction to keep
spatial consistency in an unsupervised manner. Then we aggregate dynamic points
to instance points, which stand for moving objects such as pedestrians in
videos. Finally, we extract trajectories by matching instance points for
prediction training. To the best of our knowledge, our method is the first to
achieve unsupervised learning of trajectory extraction and prediction. We
evaluate the performance on well-known trajectory datasets and show that our
method is effective for real-world videos and can use raw videos to further
improve the performance of existing models.",1097,96
"['cs.LG', 'stat.ML']",Anderson Acceleration for Reinforcement Learning,"Anderson acceleration is an old and simple method for accelerating the
computation of a fixed point. However, as far as we know and quite
surprisingly, it has never been applied to dynamic programming or reinforcement
learning. In this paper, we explain briefly what Anderson acceleration is and
how it can be applied to value iteration, this being supported by preliminary
experiments showing a significant speed up of convergence, that we critically
discuss. We also discuss how this idea could be applied more generally to
(deep) reinforcement learning.",556,48
"['stat.ML', 'cs.LG']",The Mathematics Behind Spectral Clustering And The Equivalence To PCA,"Spectral clustering is a popular algorithm that clusters points using the
eigenvalues and eigenvectors of Laplacian matrices derived from the data. For
years, spectral clustering has been working mysteriously. This paper explains
spectral clustering by dividing it into two categories based on whether the
graph Laplacian is fully connected or not. For a fully connected graph, this
paper demonstrates the dimension reduction part by offering an objective
function: the covariance between the original data points' similarities and the
mapped data points' similarities. For a multi-connected graph, this paper
proves that with a proper $k$, the first $k$ eigenvectors are the indicators of
the connected components. This paper also proves there is an equivalence
between spectral embedding and PCA.",798,69
['cs.CV'],CBAM: Convolutional Block Attention Module,"We propose Convolutional Block Attention Module (CBAM), a simple yet
effective attention module for feed-forward convolutional neural networks.
Given an intermediate feature map, our module sequentially infers attention
maps along two separate dimensions, channel and spatial, then the attention
maps are multiplied to the input feature map for adaptive feature refinement.
Because CBAM is a lightweight and general module, it can be integrated into any
CNN architectures seamlessly with negligible overheads and is end-to-end
trainable along with base CNNs. We validate our CBAM through extensive
experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets.
Our experiments show consistent improvements in classification and detection
performances with various models, demonstrating the wide applicability of CBAM.
The code and models will be publicly available.",882,42
['cs.CV'],R-CNN minus R,"Deep convolutional neural networks (CNNs) have had a major impact in most
areas of image understanding, including object category detection. In object
detection, methods such as R-CNN have obtained excellent results by integrating
CNNs with region proposal generation algorithms such as selective search. In
this paper, we investigate the role of proposal generation in CNN-based
detectors in order to determine whether it is a necessary modelling component,
carrying essential geometric information not contained in the CNN, or whether
it is merely a way of accelerating detection. We do so by designing and
evaluating a detector that uses a trivial region generation scheme, constant
for each image. Combined with SPP, this results in an excellent and fast
detector that does not require to process an image with algorithms other than
the CNN itself. We also streamline and simplify the training of CNN-based
detectors by integrating several learning steps in a single algorithm, as well
as by proposing a number of improvements that accelerate detection.",1057,13
"['cs.CV', 'cs.LG']",Large-scale detection and categorization of oil spills from SAR images with deep learning,"We propose a deep learning framework to detect and categorize oil spills in
synthetic aperture radar (SAR) images at a large scale. By means of a carefully
designed neural network model for image segmentation trained on an extensive
dataset, we are able to obtain state-of-the-art performance in oil spill
detection, achieving results that are comparable to results produced by human
operators. We also introduce a classification task, which is novel in the
context of oil spill detection in SAR. Specifically, after being detected, each
oil spill is also classified according to different categories pertaining to
its shape and texture characteristics. The classification results provide
valuable insights for improving the design of oil spill services by
world-leading providers. As the last contribution, we present our operational
pipeline and a visualization tool for large-scale data, which allows to detect
and analyze the historical presence of oil spills worldwide.",974,89
['cs.CV'],Depth Estimation on Underwater Omni-directional Images Using a Deep Neural Network,"In this work, we exploit a depth estimation Fully Convolutional Residual
Neural Network (FCRN) for in-air perspective images to estimate the depth of
underwater perspective and omni-directional images. We train one conventional
and one spherical FCRN for underwater perspective and omni-directional images,
respectively. The spherical FCRN is derived from the perspective FCRN via a
spherical longitude-latitude mapping. For that, the omni-directional camera is
modeled as a sphere, while images captured by it are displayed in the
longitude-latitude form. Due to the lack of underwater datasets, we synthesize
images in both data-driven and theoretical ways, which are used in training and
testing. Finally, experiments are conducted on these synthetic images and
results are displayed in both qualitative and quantitative way. The comparison
between ground truth and the estimated depth map indicates the effectiveness of
our method.",935,82
"['cs.CV', 'cs.LG', 'eess.IV']",Variational Topic Inference for Chest X-Ray Report Generation,"Automating report generation for medical imaging promises to reduce workload
and assist diagnosis in clinical practice. Recent work has shown that deep
learning models can successfully caption natural images. However, learning from
medical data is challenging due to the diversity and uncertainty inherent in
the reports written by different radiologists with discrepant expertise and
experience. To tackle these challenges, we propose variational topic inference
for automatic report generation. Specifically, we introduce a set of topics as
latent variables to guide sentence generation by aligning image and language
modalities in a latent space. The topics are inferred in a conditional
variational inference framework, with each topic governing the generation of a
sentence in the report. Further, we adopt a visual attention module that
enables the model to attend to different locations in the image and generate
more informative descriptions. We conduct extensive experiments on two
benchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results
demonstrate that our proposed variational topic inference method can generate
novel reports rather than mere copies of reports used in training, while still
achieving comparable performance to state-of-the-art methods in terms of
standard language generation criteria.",1329,61
['cs.CV'],Image-Based Alignment of 3D Scans,"Full 3D scanning can efficiently be obtained using structured light scanning
combined with a rotation stage. In this setting it is, however, necessary to
reposition the object and scan it in different poses in order to cover the
entire object. In this case, correspondence between the scans is lost, since
the object was moved. In this paper, we propose a fully automatic method for
aligning the scans of an object in two different poses. This is done by
matching 2D features between images from two poses and utilizing correspondence
between the images and the scanned point clouds. To demonstrate the approach,
we present the results of scanning three dissimilar objects.",673,33
"['cs.LG', 'cs.RO']",Learning to Locomote: Understanding How Environment Design Matters for Deep Reinforcement Learning,"Learning to locomote is one of the most common tasks in physics-based
animation and deep reinforcement learning (RL). A learned policy is the product
of the problem to be solved, as embodied by the RL environment, and the RL
algorithm. While enormous attention has been devoted to RL algorithms, much
less is known about the impact of design choices for the RL environment. In
this paper, we show that environment design matters in significant ways and
document how it can contribute to the brittle nature of many RL results.
Specifically, we examine choices related to state representations, initial
state distributions, reward structure, control frequency, episode termination
procedures, curriculum usage, the action space, and the torque limits. We aim
to stimulate discussion around such choices, which in practice strongly impact
the success of RL when applied to continuous-action control problems of
interest to animation, such as learning to locomote.",960,98
"['cs.LG', 'cs.CL', 'cs.CR']",SIENA: Stochastic Multi-Expert Neural Patcher,"Neural network (NN) models that are solely trained to maximize the likelihood
of an observed dataset are often vulnerable to adversarial attacks. Even though
several methods have been proposed to enhance NN models' adversarial
robustness, they often require re-training from scratch. This leads to
redundant computation, especially in the NLP domain where current
state-of-the-art models, such as BERT and ROBERTA, require great time and space
resources. By borrowing ideas from Software Engineering, we, therefore, first
introduce the Neural Patching mechanism to improve adversarial robustness by
""patching"" only parts of a NN model. Then, we propose a novel neural patching
algorithm, SIENA, that transforms a textual NN model into a stochastic ensemble
of multi-expert predictors by upgrading and re-training its last layer only.
SIENA forces adversaries to attack not only one but multiple models that are
specialized in diverse sub-sets of features, labels, and instances so that the
ensemble model becomes more robust to adversarial attacks. By conducting
comprehensive experiments, we demonstrate that all of CNN, RNN, BERT, and
ROBERTA-based textual models, once patched by SIENA, witness an absolute
increase of as much as 20% in accuracy on average under 5 different white and
black-box attacks, outperforming 6 defensive baselines across 4 public NLP
datasets.",1372,45
['cs.LG'],Reinforcement Learning via AIXI Approximation,"This paper introduces a principled approach for the design of a scalable
general reinforcement learning agent. This approach is based on a direct
approximation of AIXI, a Bayesian optimality notion for general reinforcement
learning agents. Previously, it has been unclear whether the theory of AIXI
could motivate the design of practical algorithms. We answer this hitherto open
question in the affirmative, by providing the first computationally feasible
approximation to the AIXI agent. To develop our approximation, we introduce a
Monte Carlo Tree Search algorithm along with an agent-specific extension of the
Context Tree Weighting algorithm. Empirically, we present a set of encouraging
results on a number of stochastic, unknown, and partially observable domains.",771,45
"['stat.ML', 'cs.LG']",Predicting Patient State-of-Health using Sliding Window and Recurrent Classifiers,"Bedside monitors in Intensive Care Units (ICUs) frequently sound incorrectly,
slowing response times and desensitising nurses to alarms (Chambrin, 2001),
causing true alarms to be missed (Hug et al., 2011). We compare sliding window
predictors with recurrent predictors to classify patient state-of-health from
ICU multivariate time series; we report slightly improved performance for the
RNN for three out of four targets.",423,81
['cs.CV'],Crop mapping from image time series: deep learning with multi-scale label hierarchies,"The aim of this paper is to map agricultural crops by classifying satellite
image time series. Domain experts in agriculture work with crop type labels
that are organised in a hierarchical tree structure, where coarse classes (like
orchards) are subdivided into finer ones (like apples, pears, vines, etc.). We
develop a crop classification method that exploits this expert knowledge and
significantly improves the mapping of rare crop types. The three-level label
hierarchy is encoded in a convolutional, recurrent neural network (convRNN),
such that for each pixel the model predicts three labels at different level of
granularity. This end-to-end trainable, hierarchical network architecture
allows the model to learn joint feature representations of rare classes (e.g.,
apples, pears) at a coarser level (e.g., orchard), thereby boosting
classification performance at the fine-grained level. Additionally, labelling
at different granularity also makes it possible to adjust the output according
to the classification scores; as coarser labels with high confidence are
sometimes more useful for agricultural practice than fine-grained but very
uncertain labels. We validate the proposed method on a new, large dataset that
we make public. ZueriCrop covers an area of 50 km x 48 km in the Swiss cantons
of Zurich and Thurgau with a total of 116'000 individual fields spanning 48
crop classes, and 28,000 (multi-temporal) image patches from Sentinel-2. We
compare our proposed hierarchical convRNN model with several baselines,
including methods designed for imbalanced class distributions. The hierarchical
approach performs superior by at least 9.9 percentage points in F1-score.",1682,85
['cs.CV'],Object Detection Using Deep CNNs Trained on Synthetic Images,"The need for large annotated image datasets for training Convolutional Neural
Networks (CNNs) has been a significant impediment for their adoption in
computer vision applications. We show that with transfer learning an effective
object detector can be trained almost entirely on synthetically rendered
datasets. We apply this strategy for detecting pack- aged food products
clustered in refrigerator scenes. Our CNN trained only with 4000 synthetic
images achieves mean average precision (mAP) of 24 on a test set with 55
distinct products as objects of interest and 17 distractor objects. A further
increase of 12% in the mAP is obtained by adding only 400 real images to these
4000 synthetic images in the training set. A high degree of photorealism in the
synthetic images was not essential in achieving this performance. We analyze
factors like training data set size and 3D model dictionary size for their
influence on detection performance. Additionally, training strategies like
fine-tuning with selected layers and early stopping which affect transfer
learning from synthetic scenes to real scenes are explored. Training CNNs with
synthetic datasets is a novel application of high-performance computing and a
promising approach for object detection applications in domains where there is
a dearth of large annotated image data.",1335,60
"['cs.CV', 'cs.LG', 'cs.RO']",Active Perception and Representation for Robotic Manipulation,"The vast majority of visual animals actively control their eyes, heads,
and/or bodies to direct their gaze toward different parts of their environment.
In contrast, recent applications of reinforcement learning in robotic
manipulation employ cameras as passive sensors. These are carefully placed to
view a scene from a fixed pose. Active perception allows animals to gather the
most relevant information about the world and focus their computational
resources where needed. It also enables them to view objects from different
distances and viewpoints, providing a rich visual experience from which to
learn abstract representations of the environment. Inspired by the primate
visual-motor system, we present a framework that leverages the benefits of
active perception to accomplish manipulation tasks. Our agent uses viewpoint
changes to localize objects, to learn state representations in a
self-supervised manner, and to perform goal-directed actions. We apply our
model to a simulated grasping task with a 6-DoF action space. Compared to its
passive, fixed-camera counterpart, the active model achieves 8% better
performance in targeted grasping. Compared to vanilla deep Q-learning
algorithms, our model is at least four times more sample-efficient,
highlighting the benefits of both active perception and representation
learning.",1336,61
['cs.CV'],BreakingNews: Article Annotation by Image and Text Processing,"Building upon recent Deep Neural Network architectures, current approaches
lying in the intersection of computer vision and natural language processing
have achieved unprecedented breakthroughs in tasks like automatic captioning or
image retrieval. Most of these learning methods, though, rely on large training
sets of images associated with human annotations that specifically describe the
visual content. In this paper we propose to go a step further and explore the
more complex cases where textual descriptions are loosely related to the
images. We focus on the particular domain of News articles in which the textual
content often expresses connotative and ambiguous relations that are only
suggested but not directly inferred from images. We introduce new deep learning
methods that address source detection, popularity prediction, article
illustration and geolocation of articles. An adaptive CNN architecture is
proposed, that shares most of the structure for all the tasks, and is suitable
for multitask and transfer learning. Deep Canonical Correlation Analysis is
deployed for article illustration, and a new loss function based on Great
Circle Distance is proposed for geolocation. Furthermore, we present
BreakingNews, a novel dataset with approximately 100K news articles including
images, text and captions, and enriched with heterogeneous meta-data (such as
GPS coordinates and popularity metrics). We show this dataset to be appropriate
to explore all aforementioned problems, for which we provide a baseline
performance using various Deep Learning architectures, and different
representations of the textual and visual features. We report very promising
results and bring to light several limitations of current state-of-the-art in
this kind of domain, which we hope will help spur progress in the field.",1823,61
"['stat.ML', 'cs.CV', 'cs.LG']",Uncertainty Propagation in Deep Neural Network Using Active Subspace,"The inputs of deep neural network (DNN) from real-world data usually come
with uncertainties. Yet, it is challenging to propagate the uncertainty in the
input features to the DNN predictions at a low computational cost. This work
employs a gradient-based subspace method and response surface technique to
accelerate the uncertainty propagation in DNN. Specifically, the active
subspace method is employed to identify the most important subspace in the
input features using the gradient of the DNN output to the inputs. Then the
response surface within that low-dimensional subspace can be efficiently built,
and the uncertainty of the prediction can be acquired by evaluating the
computationally cheap response surface instead of the DNN models. In addition,
the subspace can help explain the adversarial examples. The approach is
demonstrated in MNIST datasets with a convolutional neural network. Code is
available at: https://github.com/jiweiqi/nnsubspace.",959,68
"['cs.LG', 'cs.AI', 'cs.CR']",Better sampling in explanation methods can prevent dieselgate-like deception,"Machine learning models are used in many sensitive areas where besides
predictive accuracy their comprehensibility is also important. Interpretability
of prediction models is necessary to determine their biases and causes of
errors, and is a necessary prerequisite for users' confidence. For complex
state-of-the-art black-box models post-hoc model-independent explanation
techniques are an established solution. Popular and effective techniques, such
as IME, LIME, and SHAP, use perturbation of instance features to explain
individual predictions. Recently, Slack et al. (2020) put their robustness into
question by showing that their outcomes can be manipulated due to poor
perturbation sampling employed. This weakness would allow dieselgate type
cheating of owners of sensitive models who could deceive inspection and hide
potentially unethical or illegal biases existing in their predictive models.
This could undermine public trust in machine learning models and give rise to
legal restrictions on their use.
  We show that better sampling in these explanation methods prevents malicious
manipulations. The proposed sampling uses data generators that learn the
training set distribution and generate new perturbation instances much more
similar to the training set. We show that the improved sampling increases the
robustness of the LIME and SHAP, while previously untested method IME is
already the most robust of all.",1425,76
['cs.CV'],MAMNet: Multi-path Adaptive Modulation Network for Image Super-Resolution,"In recent years, single image super-resolution (SR) methods based on deep
convolutional neural networks (CNNs) have made significant progress. However,
due to the non-adaptive nature of the convolution operation, they cannot adapt
to various characteristics of images, which limits their representational
capability and, consequently, results in unnecessarily large model sizes. To
address this issue, we propose a novel multi-path adaptive modulation network
(MAMNet). Specifically, we propose a multi-path adaptive modulation block
(MAMB), which is a lightweight yet effective residual block that adaptively
modulates residual feature responses by fully exploiting their information via
three paths. The three paths model three types of information suitable for SR:
1) channel-specific information (CSI) using global variance pooling, 2)
inter-channel dependencies (ICD) based on the CSI, 3) and channel-specific
spatial dependencies (CSD) via depth-wise convolution. We demonstrate that the
proposed MAMB is effective and parameter-efficient for image SR than other
feature modulation methods. In addition, experimental results show that our
MAMNet outperforms most of the state-of-the-art methods with a relatively small
number of parameters.",1246,73
['cs.LG'],Style Transfer Generative Adversarial Networks: Learning to Play Chess Differently,"The idea of style transfer has largely only been explored in image-based
tasks, which we attribute in part to the specific nature of loss functions used
for style transfer. We propose a general formulation of style transfer as an
extension of generative adversarial networks, by using a discriminator to
regularize a generator with an otherwise separate loss function. We apply our
approach to the task of learning to play chess in the style of a specific
player, and present empirical evidence for the viability of our approach.",529,82
['cs.CV'],Rethinking Performance Estimation in Neural Architecture Search,"Neural architecture search (NAS) remains a challenging problem, which is
attributed to the indispensable and time-consuming component of performance
estimation (PE). In this paper, we provide a novel yet systematic rethinking of
PE in a resource constrained regime, termed budgeted PE (BPE), which precisely
and effectively estimates the performance of an architecture sampled from an
architecture space. Since searching an optimal BPE is extremely time-consuming
as it requires to train a large number of networks for evaluation, we propose a
Minimum Importance Pruning (MIP) approach. Given a dataset and a BPE search
space, MIP estimates the importance of hyper-parameters using random forest and
subsequently prunes the minimum one from the next iteration. In this way, MIP
effectively prunes less important hyper-parameters to allocate more
computational resource on more important ones, thus achieving an effective
exploration. By combining BPE with various search algorithms including
reinforcement learning, evolution algorithm, random search, and differentiable
architecture search, we achieve 1, 000x of NAS speed up with a negligible
performance drop comparing to the SOTA",1183,63
"['cs.CV', 'cs.AI']",A Deeper Insight into the UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation,"This paper presents an unsupervised deep learning framework called UnDEMoN
for estimating dense depth map and 6-DoF camera pose information directly from
monocular images. The proposed network is trained using unlabeled monocular
stereo image pairs and is shown to provide superior performance in depth and
ego-motion estimation compared to the existing state-of-the-art. These
improvements are achieved by introducing a new objective function that aims to
minimize spatial as well as temporal reconstruction losses simultaneously.
These losses are defined using bi-linear sampling kernel and penalized using
the Charbonnier penalty function. The objective function, thus created,
provides robustness to image gradient noises thereby improving the overall
estimation accuracy without resorting to any coarse to fine strategies which
are currently prevalent in the literature. Another novelty lies in the fact
that we combine a disparity-based depth estimation network with a pose
estimation network to obtain absolute scale-aware 6 DOF Camera pose and
superior depth map. The effectiveness of the proposed approach is demonstrated
through performance comparison with the existing supervised and unsupervised
methods on the KITTI driving dataset.",1245,96
['cs.CV'],TANet++: Triple Attention Network with Filtered Pointcloud on 3D Detection,"TANet is one of state-of-the-art 3D object detection method on KITTI and JRDB
benchmark, the network contains a Triple Attention module and Coarse-to-Fine
Regression module to improve the robustness and accuracy of 3D Detection.
However, since the original input data (point clouds) contains a lot of noise
during collecting the data, which will further affect the training of the
model. For example, the object is far from the robot, the sensor is difficult
to obtain enough pointcloud. If the objects only contains few point clouds, and
the samples are fed into model with the normal samples together during
training, the detector will be difficult to distinguish the individual with few
pointcloud belong to object or background. In this paper, we propose TANet++ to
improve the performance on 3D Detection, which adopt a novel training strategy
on training the TANet. In order to reduce the negative impact by the weak
samples, the training strategy previously filtered the training data, and then
the TANet++ is trained by the rest of data. The experimental results shows that
AP score of TANet++ is 8.98 higher than TANet on JRDB benchmark.",1146,74
"['cs.CV', 'eess.IV']",Robust Unsupervised Small Area Change Detection from SAR Imagery Using Deep Learning,"Small area change detection from synthetic aperture radar (SAR) is a highly
challenging task. In this paper, a robust unsupervised approach is proposed for
small area change detection from multi-temporal SAR images using deep learning.
First, a multi-scale superpixel reconstruction method is developed to generate
a difference image (DI), which can suppress the speckle noise effectively and
enhance edges by exploiting local, spatially homogeneous information. Second, a
two-stage centre-constrained fuzzy c-means clustering algorithm is proposed to
divide the pixels of the DI into changed, unchanged and intermediate classes
with a parallel clustering strategy. Image patches belonging to the first two
classes are then constructed as pseudo-label training samples, and image
patches of the intermediate class are treated as testing samples. Finally, a
convolutional wavelet neural network (CWNN) is designed and trained to classify
testing samples into changed or unchanged classes, coupled with a deep
convolutional generative adversarial network (DCGAN) to increase the number of
changed class within the pseudo-label training samples. Numerical experiments
on four real SAR datasets demonstrate the validity and robustness of the
proposed approach, achieving up to 99.61% accuracy for small area change
detection.",1321,84
['cs.LG'],Towards Efficient Full 8-bit Integer DNN Online Training on Resource-limited Devices without Batch Normalization,"Huge computational costs brought by convolution and batch normalization (BN)
have caused great challenges for the online training and corresponding
applications of deep neural networks (DNNs), especially in resource-limited
devices. Existing works only focus on the convolution or BN acceleration and no
solution can alleviate both problems with satisfactory performance. Online
training has gradually become a trend in resource-limited devices like mobile
phones while there is still no complete technical scheme with acceptable model
performance, processing speed, and computational cost. In this research, an
efficient online-training quantization framework termed EOQ is proposed by
combining Fixup initialization and a novel quantization scheme for DNN model
compression and acceleration. Based on the proposed framework, we have
successfully realized full 8-bit integer network training and removed BN in
large-scale DNNs. Especially, weight updates are quantized to 8-bit integers
for the first time. Theoretical analyses of EOQ utilizing Fixup initialization
for removing BN have been further given using a novel Block Dynamical Isometry
theory with weaker assumptions. Benefiting from rational quantization
strategies and the absence of BN, the full 8-bit networks based on EOQ can
achieve state-of-the-art accuracy and immense advantages in computational cost
and processing speed. What is more, the design of deep learning chips can be
profoundly simplified for the absence of unfriendly square root operations in
BN. Beyond this, EOQ has been evidenced to be more advantageous in small-batch
online training with fewer batch samples. In summary, the EOQ framework is
specially designed for reducing the high cost of convolution and BN in network
training, demonstrating a broad application prospect of online training in
resource-limited devices.",1858,112
"['cs.CV', 'I.4.0; I.4.8; I.4.9; I.2.10']",Holopix50k: A Large-Scale In-the-wild Stereo Image Dataset,"With the mass-market adoption of dual-camera mobile phones, leveraging stereo
information in computer vision has become increasingly important. Current
state-of-the-art methods utilize learning-based algorithms, where the amount
and quality of training samples heavily influence results. Existing stereo
image datasets are limited either in size or subject variety. Hence, algorithms
trained on such datasets do not generalize well to scenarios encountered in
mobile photography. We present Holopix50k, a novel in-the-wild stereo image
dataset, comprising 49,368 image pairs contributed by users of the Holopix
mobile social platform. In this work, we describe our data collection process
and statistically compare our dataset to other popular stereo datasets. We
experimentally show that using our dataset significantly improves results for
tasks such as stereo super-resolution and self-supervised monocular depth
estimation. Finally, we showcase practical applications of our dataset to
motivate novel works and use cases. The Holopix50k dataset is available at
http://github.com/leiainc/holopix50k",1101,58
['cs.CV'],Diversity Helps: Unsupervised Few-shot Learning via Distribution Shift-based Data Augmentation,"Few-shot learning aims to learn a new concept when only a few training
examples are available, which has been extensively explored in recent years.
However, most of the current works heavily rely on a large-scale labeled
auxiliary set to train their models in an episodic-training paradigm. Such a
kind of supervised setting basically limits the widespread use of few-shot
learning algorithms. Instead, in this paper, we develop a novel framework
called Unsupervised Few-shot Learning via Distribution Shift-based Data
Augmentation (ULDA), which pays attention to the distribution diversity inside
each constructed pretext few-shot task when using data augmentation.
Importantly, we highlight the value and importance of the distribution
diversity in the augmentation-based pretext few-shot tasks, which can
effectively alleviate the overfitting problem and make the few-shot model learn
more robust feature representations. In ULDA, we systemically investigate the
effects of different augmentation techniques and propose to strengthen the
distribution diversity (or difference) between the query set and support set in
each few-shot task, by augmenting these two sets diversely (i.e., distribution
shifting). In this way, even incorporated with simple augmentation techniques
(e.g., random crop, color jittering, or rotation), our ULDA can produce a
significant improvement. In the experiments, few-shot models learned by ULDA
can achieve superior generalization performance and obtain state-of-the-art
results in a variety of established few-shot learning tasks on Omniglot and
miniImageNet. The source code is available in
https://github.com/WonderSeven/ULDA.",1663,94
"['cs.LG', 'stat.ML', '68T07']",Partially Conditioned Generative Adversarial Networks,"Generative models are undoubtedly a hot topic in Artificial Intelligence,
among which the most common type is Generative Adversarial Networks (GANs).
These architectures let one synthesise artificial datasets by implicitly
modelling the underlying probability distribution of a real-world training
dataset. With the introduction of Conditional GANs and their variants, these
methods were extended to generating samples conditioned on ancillary
information available for each sample within the dataset. From a practical
standpoint, however, one might desire to generate data conditioned on partial
information. That is, only a subset of the ancillary conditioning variables
might be of interest when synthesising data. In this work, we argue that
standard Conditional GANs are not suitable for such a task and propose a new
Adversarial Network architecture and training strategy to deal with the ensuing
problems. Experiments illustrating the value of the proposed approach in digit
and face image synthesis under partial conditioning information are presented,
showing that the proposed method can effectively outperform the standard
approach under these circumstances.",1169,53
['cs.CV'],Unsupervised Domain Adaptation using Generative Adversarial Networks for Semantic Segmentation of Aerial Images,"Segmenting aerial images is being of great potential in surveillance and
scene understanding of urban areas. It provides a mean for automatic reporting
of the different events that happen in inhabited areas. This remarkably
promotes public safety and traffic management applications. After the wide
adoption of convolutional neural networks methods, the accuracy of semantic
segmentation algorithms could easily surpass 80% if a robust dataset is
provided. Despite this success, the deployment of a pre-trained segmentation
model to survey a new city that is not included in the training set
significantly decreases the accuracy. This is due to the domain shift between
the source dataset on which the model is trained and the new target domain of
the new city images. In this paper, we address this issue and consider the
challenge of domain adaptation in semantic segmentation of aerial images. We
design an algorithm that reduces the domain shift impact using Generative
Adversarial Networks (GANs). In the experiments, we test the proposed
methodology on the International Society for Photogrammetry and Remote Sensing
(ISPRS) semantic segmentation dataset and found that our method improves the
overall accuracy from 35% to 52% when passing from Potsdam domain (considered
as source domain) to Vaihingen domain (considered as target domain). In
addition, the method allows recovering efficiently the inverted classes due to
sensor variation. In particular, it improves the average segmentation accuracy
of the inverted classes due to sensor variation from 14% to 61%.",1572,111
['cs.CV'],The MECCANO Dataset: Understanding Human-Object Interactions from Egocentric Videos in an Industrial-like Domain,"Wearable cameras allow to collect images and videos of humans interacting
with the world. While human-object interactions have been thoroughly
investigated in third person vision, the problem has been understudied in
egocentric settings and in industrial scenarios. To fill this gap, we introduce
MECCANO, the first dataset of egocentric videos to study human-object
interactions in industrial-like settings. MECCANO has been acquired by 20
participants who were asked to build a motorbike model, for which they had to
interact with tiny objects and tools. The dataset has been explicitly labeled
for the task of recognizing human-object interactions from an egocentric
perspective. Specifically, each interaction has been labeled both temporally
(with action segments) and spatially (with active object bounding boxes). With
the proposed dataset, we investigate four different tasks including 1) action
recognition, 2) active object detection, 3) active object recognition and 4)
egocentric human-object interaction detection, which is a revisited version of
the standard human-object interaction detection task. Baseline results show
that the MECCANO dataset is a challenging benchmark to study egocentric
human-object interactions in industrial-like scenarios. We publicy release the
dataset at https://iplab.dmi.unict.it/MECCANO.",1333,112
"['cs.LG', 'cs.CV']",Regularizing Generative Adversarial Networks under Limited Data,"Recent years have witnessed the rapid progress of generative adversarial
networks (GANs). However, the success of the GAN models hinges on a large
amount of training data. This work proposes a regularization approach for
training robust GAN models on limited data. We theoretically show a connection
between the regularized loss and an f-divergence called LeCam-divergence, which
we find is more robust under limited training data. Extensive experiments on
several benchmark datasets demonstrate that the proposed regularization scheme
1) improves the generalization performance and stabilizes the learning dynamics
of GAN models under limited training data, and 2) complements the recent data
augmentation methods. These properties facilitate training GAN models to
achieve state-of-the-art performance when only limited training data of the
ImageNet benchmark is available.",875,63
"['cs.LG', 'cs.AI', 'stat.ML']",IL-Net: Using Expert Knowledge to Guide the Design of Furcated Neural Networks,"Deep neural networks (DNN) excel at extracting patterns. Through
representation learning and automated feature engineering on large datasets,
such models have been highly successful in computer vision and natural language
applications. Designing optimal network architectures from a principled or
rational approach however has been less than successful, with the best
successful approaches utilizing an additional machine learning algorithm to
tune the network hyperparameters. However, in many technical fields, there
exist established domain knowledge and understanding about the subject matter.
In this work, we develop a novel furcated neural network architecture that
utilizes domain knowledge as high-level design principles of the network. We
demonstrate proof-of-concept by developing IL-Net, a furcated network for
predicting the properties of ionic liquids, which is a class of complex
multi-chemicals entities. Compared to existing state-of-the-art approaches, we
show that furcated networks can improve model accuracy by approximately 20-35%,
without using additional labeled data. Lastly, we distill two key design
principles for furcated networks that can be adapted to other domains.",1198,78
['cs.LG'],Quantitatively Evaluating GANs With Divergences Proposed for Training,"Generative adversarial networks (GANs) have been extremely effective in
approximating complex distributions of high-dimensional, input data samples,
and substantial progress has been made in understanding and improving GAN
performance in terms of both theory and application. However, we currently lack
quantitative methods for model assessment. Because of this, while many GAN
variants are being proposed, we have relatively little understanding of their
relative abilities. In this paper, we evaluate the performance of various types
of GANs using divergence and distance functions typically used only for
training. We observe consistency across the various proposed metrics and,
interestingly, the test-time metrics do not favour networks that use the same
training-time criterion. We also compare the proposed metrics to human
perceptual scores.",849,69
"['stat.ML', 'cs.LG']","Comments on ""Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?""","In a recently published paper [1], it is shown that deep neural networks
(DNNs) with random Gaussian weights preserve the metric structure of the data,
with the property that the distance shrinks more when the angle between the two
data points is smaller. We agree that the random projection setup considered in
[1] preserves distances with a high probability. But as far as we are
concerned, the relation between the angle of the data points and the output
distances is quite the opposite, i.e., smaller angles result in a weaker
distance shrinkage. This leads us to conclude that Theorem 3 and Figure 5 in
[1] are not accurate. Hence the usage of random Gaussian weights in DNNs cannot
provide an ability of universal classification or treating in-class and
out-of-class data separately. Consequently, the behavior of networks consisting
of random Gaussian weights only is not useful to explain how DNNs achieve
state-of-art results in a large variety of problems.",966,101
"['stat.ML', 'cs.LG', 'stat.ME']",Causal Inference on Time Series using Structural Equation Models,"Causal inference uses observations to infer the causal structure of the data
generating system. We study a class of functional models that we call Time
Series Models with Independent Noise (TiMINo). These models require independent
residual time series, whereas traditional methods like Granger causality
exploit the variance of residuals. There are two main contributions: (1)
Theoretical: By restricting the model class (e.g. to additive noise) we can
provide a more general identifiability result than existing ones. This result
incorporates lagged and instantaneous effects that can be nonlinear and do not
need to be faithful, and non-instantaneous feedbacks between the time series.
(2) Practical: If there are no feedback loops between time series, we propose
an algorithm based on non-linear independence tests of time series. When the
data are causally insufficient, or the data generating process does not satisfy
the model assumptions, this algorithm may still give partial results, but
mostly avoids incorrect answers. An extension to (non-instantaneous) feedbacks
is possible, but not discussed. It outperforms existing methods on artificial
and real data. Code can be provided upon request.",1204,64
"['cs.CV', 'cs.LG']",Learning to decompose for object detection and instance segmentation,"Although deep convolutional neural networks(CNNs) have achieved remarkable
results on object detection and segmentation, pre- and post-processing steps
such as region proposals and non-maximum suppression(NMS), have been required.
These steps result in high computational complexity and sensitivity to
hyperparameters, e.g. thresholds for NMS. In this work, we propose a novel
end-to-end trainable deep neural network architecture, which consists of
convolutional and recurrent layers, that generates the correct number of object
instances and their bounding boxes (or segmentation masks) given an image,
using only a single network evaluation without any pre- or post-processing
steps. We have tested on detecting digits in multi-digit images synthesized
using MNIST, automatically segmenting digits in these images, and detecting
cars in the KITTI benchmark dataset. The proposed approach outperforms a strong
CNN baseline on the synthesized digits datasets and shows promising results on
KITTI car detection.",1011,68
['cs.LG'],Transfer Learning Using Feature Selection,"We present three related ways of using Transfer Learning to improve feature
selection. The three methods address different problems, and hence share
different kinds of information between tasks or feature classes, but all three
are based on the information theoretic Minimum Description Length (MDL)
principle and share the same underlying Bayesian interpretation. The first
method, MIC, applies when predictive models are to be built simultaneously for
multiple tasks (``simultaneous transfer'') that share the same set of features.
MIC allows each feature to be added to none, some, or all of the task models
and is most beneficial for selecting a small set of predictive features from a
large pool of features, as is common in genomic and biological datasets. Our
second method, TPC (Three Part Coding), uses a similar methodology for the case
when the features can be divided into feature classes. Our third method,
Transfer-TPC, addresses the ``sequential transfer'' problem in which the task
to which we want to transfer knowledge may not be known in advance and may have
different amounts of data than the other tasks. Transfer-TPC is most beneficial
when we want to transfer knowledge between tasks which have unequal amounts of
labeled data, for example the data for disambiguating the senses of different
verbs. We demonstrate the effectiveness of these approaches with experimental
results on real world data pertaining to genomics and to Word Sense
Disambiguation (WSD).",1482,41
['cs.CV'],Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks,"We present Convolutional Oriented Boundaries (COB), which produces multiscale
oriented contours and region hierarchies starting from generic image
classification Convolutional Neural Networks (CNNs). COB is computationally
efficient, because it requires a single CNN forward pass for multi-scale
contour detection and it uses a novel sparse boundary representation for
hierarchical segmentation; it gives a significant leap in performance over the
state-of-the-art, and it generalizes very well to unseen categories and
datasets. Particularly, we show that learning to estimate not only contour
strength but also orientation provides more accurate results. We perform
extensive experiments for low-level applications on BSDS, PASCAL Context,
PASCAL Segmentation, and NYUD to evaluate boundary detection performance,
showing that COB provides state-of-the-art contours and region hierarchies in
all datasets. We also evaluate COB on high-level tasks when coupled with
multiple pipelines for object proposals, semantic contours, semantic
segmentation, and object detection on MS-COCO, SBD, and PASCAL; showing that
COB also improves the results for all tasks.",1157,78
['cs.CV'],Learning Efficient Image Representation for Person Re-Identification,"Color names based image representation is successfully used in person
re-identification, due to the advantages of being compact, intuitively
understandable as well as being robust to photometric variance. However, there
exists the diversity between underlying distribution of color names' RGB values
and that of image pixels' RGB values, which may lead to inaccuracy when
directly comparing them in Euclidean space. In this paper, we propose a new
method named soft Gaussian mapping (SGM) to address this problem. We model the
discrepancies between color names and pixels using a Gaussian and utilize the
inverse of covariance matrix to bridge the gap between them. Based on SGM, an
image could be converted to several soft Gaussian maps. In each soft Gaussian
map, we further seek to establish stable and robust descriptors within a local
region through a max pooling operation. Then, a robust image representation
based on color names is obtained by concatenating the statistical descriptors
in each stripe. When labeled data are available, one discriminative subspace
projection matrix is learned to build efficient representations of an image via
cross-view coupling learning. Experiments on the public datasets - VIPeR,
PRID450S and CUHK03, demonstrate the effectiveness of our method.",1290,68
['cs.CV'],SVIRO: Synthetic Vehicle Interior Rear Seat Occupancy Dataset and Benchmark,"We release SVIRO, a synthetic dataset for sceneries in the passenger
compartment of ten different vehicles, in order to analyze machine
learning-based approaches for their generalization capacities and reliability
when trained on a limited number of variations (e.g. identical backgrounds and
textures, few instances per class). This is in contrast to the intrinsically
high variability of common benchmark datasets, which focus on improving the
state-of-the-art of general tasks. Our dataset contains bounding boxes for
object detection, instance segmentation masks, keypoints for pose estimation
and depth images for each synthetic scenery as well as images for each
individual seat for classification. The advantage of our use-case is twofold:
The proximity to a realistic application to benchmark new approaches under
novel circumstances while reducing the complexity to a more tractable
environment, such that applications and theoretical questions can be tested on
a more challenging dataset as toy problems. The data and evaluation server are
available under https://sviro.kl.dfki.de.",1091,75
"['cs.CV', 'cs.LG']",Adaptive Streaming Perception using Deep Reinforcement Learning,"Executing computer vision models on streaming visual data, or streaming
perception is an emerging problem, with applications in self-driving, embodied
agents, and augmented/virtual reality. The development of such systems is
largely governed by the accuracy and latency of the processing pipeline. While
past work has proposed numerous approximate execution frameworks, their
decision functions solely focus on optimizing latency, accuracy, or energy,
etc. This results in sub-optimum decisions, affecting the overall system
performance. We argue that the streaming perception systems should holistically
maximize the overall system performance (i.e., considering both accuracy and
latency simultaneously). To this end, we describe a new approach based on deep
reinforcement learning to learn these tradeoffs at runtime for streaming
perception. This tradeoff optimization is formulated as a novel deep contextual
bandit problem and we design a new reward function that holistically integrates
latency and accuracy into a single metric. We show that our agent can learn a
competitive policy across multiple decision dimensions, which outperforms
state-of-the-art policies on public datasets.",1191,63
"['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']",VUSFA:Variational Universal Successor Features Approximator to Improve Transfer DRL for Target Driven Visual Navigation,"In this paper, we show how novel transfer reinforcement learning techniques
can be applied to the complex task of target driven navigation using the
photorealistic AI2THOR simulator. Specifically, we build on the concept of
Universal Successor Features with an A3C agent. We introduce the novel
architectural contribution of a Successor Feature Dependant Policy (SFDP) and
adopt the concept of Variational Information Bottlenecks to achieve state of
the art performance. VUSFA, our final architecture, is a straightforward
approach that can be implemented using our open source repository. Our approach
is generalizable, showed greater stability in training, and outperformed recent
approaches in terms of transfer learning ability.",732,119
"['cs.LG', 'stat.ML', '37Nxx', 'I.2.1']",Supporting Optimal Phase Space Reconstructions Using Neural Network Architecture for Time Series Modeling,"The reconstruction of phase spaces is an essential step to analyze time
series according to Dynamical System concepts. A regression performed on such
spaces unveils the relationships among system states from which we can derive
their generating rules, that is, the most probable set of functions responsible
for generating observations along time. In this sense, most approaches rely on
Takens' embedding theorem to unfold the phase space, which requires the
embedding dimension and the time delay. Moreover, although several methods have
been proposed to empirically estimate those parameters, they still face
limitations due to their lack of consistency and robustness, which has
motivated this paper. As an alternative, we here propose an artificial neural
network with a forgetting mechanism to implicitly learn the phase spaces
properties, whatever they are. Such network trains on forecasting errors and,
after converging, its architecture is used to estimate the embedding
parameters. Experimental results confirm that our approach is either as
competitive as or better than most state-of-the-art strategies while revealing
the temporal relationship among time-series observations.",1188,105
['cs.CV'],OptiBox: Breaking the Limits of Proposals for Visual Grounding,"The problem of language grounding has attracted much attention in recent
years due to its pivotal role in more general image-lingual high level
reasoning tasks (e.g., image captioning, VQA). Despite the tremendous progress
in visual grounding, the performance of most approaches has been hindered by
the quality of bounding box proposals obtained in the early stages of all
recent pipelines. To address this limitation, we propose a general progressive
query-guided bounding box refinement architecture (OptiBox) that leverages
global image encoding for added context. We apply this architecture in the
context of the GroundeR model, first introduced in 2016, which has a number of
unique and appealing properties, such as the ability to learn in the
semi-supervised setting by leveraging cyclic language-reconstruction. Using
GroundeR + OptiBox and a simple semantic language reconstruction loss that we
propose, we achieve state-of-the-art grounding performance in the supervised
setting on Flickr30k Entities dataset. More importantly, we are able to surpass
many recent fully supervised models with only 50% of training data and perform
competitively with as low as 3%.",1173,62
"['cs.LG', '68T07']",An Introduction to Deep Generative Modeling,"Deep generative models (DGM) are neural networks with many hidden layers
trained to approximate complicated, high-dimensional probability distributions
using a large number of samples. When trained successfully, we can use the DGMs
to estimate the likelihood of each observation and to create new samples from
the underlying distribution. Developing DGMs has become one of the most hotly
researched fields in artificial intelligence in recent years. The literature on
DGMs has become vast and is growing rapidly. Some advances have even reached
the public sphere, for example, the recent successes in generating
realistic-looking images, voices, or movies; so-called deep fakes. Despite
these successes, several mathematical and practical issues limit the broader
use of DGMs: given a specific dataset, it remains challenging to design and
train a DGM and even more challenging to find out why a particular model is or
is not effective. To help advance the theoretical understanding of DGMs, we
introduce DGMs and provide a concise mathematical framework for modeling the
three most popular approaches: normalizing flows (NF), variational autoencoders
(VAE), and generative adversarial networks (GAN). We illustrate the advantages
and disadvantages of these basic approaches using numerical experiments. Our
goal is to enable and motivate the reader to contribute to this proliferating
research area. Our presentation also emphasizes relations between generative
modeling and optimal transport.",1494,43
"['cs.CV', 'cs.LG']",Self-supervised Video Object Segmentation,"The objective of this paper is self-supervised representation learning, with
the goal of solving semi-supervised video object segmentation (a.k.a. dense
tracking). We make the following contributions: (i) we propose to improve the
existing self-supervised approach, with a simple, yet more effective memory
mechanism for long-term correspondence matching, which resolves the challenge
caused by the dis-appearance and reappearance of objects; (ii) by augmenting
the self-supervised approach with an online adaptation module, our method
successfully alleviates tracker drifts caused by spatial-temporal
discontinuity, e.g. occlusions or dis-occlusions, fast motions; (iii) we
explore the efficiency of self-supervised representation learning for dense
tracking, surprisingly, we show that a powerful tracking model can be trained
with as few as 100 raw video clips (equivalent to a duration of 11mins),
indicating that low-level statistics have already been effective for tracking
tasks; (iv) we demonstrate state-of-the-art results among the self-supervised
approaches on DAVIS-2017 and YouTube-VOS, as well as surpassing most of methods
trained with millions of manual segmentation annotations, further bridging the
gap between self-supervised and supervised learning. Codes are released to
foster any further research (https://github.com/fangruizhu/self_sup_semiVOS).",1369,41
"['cs.LG', 'cs.RO']",Integrated Decision and Control: Towards Interpretable and Computationally Efficient Driving Intelligence,"Decision and control are core functionalities of high-level automated
vehicles. Current mainstream methods, such as functionality decomposition and
end-to-end reinforcement learning (RL), either suffer high time complexity or
poor interpretability and adaptability on real-world autonomous driving tasks.
In this paper, we present an interpretable and computationally efficient
framework called integrated decision and control (IDC) for automated vehicles,
which decomposes the driving task into static path planning and dynamic optimal
tracking that are structured hierarchically. First, the static path planning
generates several candidate paths only considering static traffic elements.
Then, the dynamic optimal tracking is designed to track the optimal path while
considering the dynamic obstacles. To that end, we formulate a constrained
optimal control problem (OCP) for each candidate path, optimize them separately
and follow the one with the best tracking performance. To unload the heavy
online computation, we propose a model-based reinforcement learning (RL)
algorithm that can be served as an approximate constrained OCP solver.
Specifically, the OCPs for all paths are considered together to construct a
single complete RL problem and then solved offline in the form of value and
policy networks, for real-time online path selecting and tracking respectively.
We verify our framework in both simulations and the real world. Results show
that compared with baseline methods IDC has an order of magnitude higher online
computing efficiency, as well as better driving performance including traffic
efficiency and safety. In addition, it yields great interpretability and
adaptability among different driving tasks. The effectiveness of the proposed
method is also demonstrated in real road tests with complicated traffic
conditions.",1844,105
"['cs.LG', 'cs.AI']",Combining Bayesian Approaches and Evolutionary Techniques for the Inference of Breast Cancer Networks,"Gene and protein networks are very important to model complex large-scale
systems in molecular biology. Inferring or reverseengineering such networks can
be defined as the process of identifying gene/protein interactions from
experimental data through computational analysis. However, this task is
typically complicated by the enormously large scale of the unknowns in a rather
small sample size. Furthermore, when the goal is to study causal relationships
within the network, tools capable of overcoming the limitations of correlation
networks are required. In this work, we make use of Bayesian Graphical Models
to attach this problem and, specifically, we perform a comparative study of
different state-of-the-art heuristics, analyzing their performance in inferring
the structure of the Bayesian Network from breast cancer data.",832,101
['cs.CV'],Efficient Semantic Video Segmentation with Per-frame Inference,"For semantic segmentation, most existing real-time deep models trained with
each frame independently may produce inconsistent results for a video sequence.
Advanced methods take into considerations the correlations in the video
sequence, e.g., by propagating the results to the neighboring frames using
optical flow, or extracting the frame representations with other frames, which
may lead to inaccurate results or unbalanced latency. In this work, we process
efficient semantic video segmentation in a per-frame fashion during the
inference process. Different from previous per-frame models, we explicitly
consider the temporal consistency among frames as extra constraints during the
training process and embed the temporal consistency into the segmentation
network. Therefore, in the inference process, we can process each frame
independently with no latency, and improve the temporal consistency with no
extra computational cost and post-processing. We employ compact models for
real-time execution. To narrow the performance gap between compact models and
large models, new knowledge distillation methods are designed. Our results
outperform previous keyframe based methods with a better trade-off between the
accuracy and the inference speed on popular benchmarks, including the
Cityscapes and Camvid. The temporal consistency is also improved compared with
corresponding baselines which are trained with each frame independently. Code
is available at: https://tinyurl.com/segment-video",1493,62
['cs.CV'],Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields,"In this article, we tackle the problem of depth estimation from single
monocular images. Compared with depth estimation using multiple images such as
stereo depth perception, depth from monocular images is much more challenging.
Prior work typically focuses on exploiting geometric priors or additional
sources of information, most using hand-crafted features. Recently, there is
mounting evidence that features from deep convolutional neural networks (CNN)
set new records for various vision applications. On the other hand, considering
the continuous characteristic of the depth values, depth estimations can be
naturally formulated as a continuous conditional random field (CRF) learning
problem. Therefore, here we present a deep convolutional neural field model for
estimating depths from single monocular images, aiming to jointly explore the
capacity of deep CNN and continuous CRF. In particular, we propose a deep
structured learning scheme which learns the unary and pairwise potentials of
continuous CRF in a unified deep CNN framework. We then further propose an
equally effective model based on fully convolutional networks and a novel
superpixel pooling method, which is $\sim 10$ times faster, to speedup the
patch-wise convolutions in the deep model. With this more efficient model, we
are able to design deeper networks to pursue better performance. Experiments on
both indoor and outdoor scene datasets demonstrate that the proposed method
outperforms state-of-the-art depth estimation approaches.",1515,82
['cs.CV'],Capturing human category representations by sampling in deep feature spaces,"Understanding how people represent categories is a core problem in cognitive
science. Decades of research have yielded a variety of formal theories of
categories, but validating them with naturalistic stimuli is difficult. The
challenge is that human category representations cannot be directly observed
and running informative experiments with naturalistic stimuli such as images
requires a workable representation of these stimuli. Deep neural networks have
recently been successful in solving a range of computer vision tasks and
provide a way to compactly represent image features. Here, we introduce a
method to estimate the structure of human categories that combines ideas from
cognitive science and machine learning, blending human-based algorithms with
state-of-the-art deep image generators. We provide qualitative and quantitative
results as a proof-of-concept for the method's feasibility. Samples drawn from
human distributions rival those from state-of-the-art generative models in
quality and outperform alternative methods for estimating the structure of
human categories.",1088,75
"['cs.LG', 'cs.RO', 'cs.SY', 'eess.SY']",Inverse Constrained Reinforcement Learning,"In real world settings, numerous constraints are present which are hard to
specify mathematically. However, for the real world deployment of reinforcement
learning (RL), it is critical that RL agents are aware of these constraints, so
that they can act safely. In this work, we consider the problem of learning
constraints from demonstrations of a constraint-abiding agent's behavior. We
experimentally validate our approach and show that our framework can
successfully learn the most likely constraints that the agent respects. We
further show that these learned constraints are \textit{transferable} to new
agents that may have different morphologies and/or reward functions. Previous
works in this regard have either mainly been restricted to tabular (discrete)
settings, specific types of constraints or assume the environment's transition
dynamics. In contrast, our framework is able to learn arbitrary
\textit{Markovian} constraints in high-dimensions in a completely model-free
setting. The code can be found it:
\url{https://github.com/shehryar-malik/icrl}.",1065,42
['cs.CV'],Explaining decision of model from its prediction,"This document summarizes different visual explanations methods such as CAM,
Grad-CAM, Localization using Multiple Instance Learning - Saliency-based
methods, Saliency-driven Class-Impressions, Muting pixels in input image -
Adversarial methods and Activation visualization, Convolution filter
visualization - Feature-based methods. We have also shown the results produced
by different methods and a comparison between CAM, GradCAM, and Guided
Backpropagation.",459,48
"['cs.CV', 'cs.LG', 'q-fin.ST', 'q-fin.TR']",Visual Time Series Forecasting: An Image-driven Approach,"In this work, we address time-series forecasting as a computer vision task.
We capture input data as an image and train a model to produce the subsequent
image. This approach results in predicting distributions as opposed to
pointwise values. To assess the robustness and quality of our approach, we
examine various datasets and multiple evaluation metrics. Our experiments show
that our forecasting tool is effective for cyclic data but somewhat less for
irregular data such as stock prices. Importantly, when using image-based
evaluation metrics, we find our method to outperform various baselines,
including ARIMA, and a numerical variation of our deep learning approach.",674,56
"['cs.CV', 'cs.AI']",A Competitive Method to VIPriors Object Detection Challenge,"In this report, we introduce the technical details of our submission to the
VIPriors object detection challenge. Our solution is based on mmdetction of a
strong baseline open-source detection toolbox. Firstly, we introduce an
effective data augmentation method to address the lack of data problem, which
contains bbox-jitter, grid-mask, and mix-up. Secondly, we present a robust
region of interest (ROI) extraction method to learn more significant ROI
features via embedding global context features. Thirdly, we propose a
multi-model integration strategy to refinement the prediction box, which
weighted boxes fusion (WBF). Experimental results demonstrate that our approach
can significantly improve the average precision (AP) of object detection on the
subset of the COCO2017 dataset.",786,59
"['cs.LG', 'cs.AI', 'stat.ML']",Hashing over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning,"In deep reinforcement learning (RL) tasks, an efficient exploration mechanism
should be able to encourage an agent to take actions that lead to less frequent
states which may yield higher accumulative future return. However, both knowing
about the future and evaluating the frequentness of states are non-trivial
tasks, especially for deep RL domains, where a state is represented by
high-dimensional image frames. In this paper, we propose a novel informed
exploration framework for deep RL, where we build the capability for an RL
agent to predict over the future transitions and evaluate the frequentness for
the predicted future frames in a meaningful manner. To this end, we train a
deep prediction model to predict future frames given a state-action pair, and a
convolutional autoencoder model to hash over the seen frames. In addition, to
utilize the counts derived from the seen frames to evaluate the frequentness
for the predicted frames, we tackle the challenge of matching the predicted
future frames and their corresponding seen frames at the latent feature level.
In this way, we derive a reliable metric for evaluating the novelty of the
future direction pointed by each action, and hence inform the agent to explore
the least frequent one.",1255,92
['cs.CV'],Fastidious Attention Network for Navel Orange Segmentation,"Deep learning achieves excellent performance in many domains, so we not only
apply it to the navel orange semantic segmentation task to solve the two
problems of distinguishing defect categories and identifying the stem end and
blossom end, but also propose a fastidious attention mechanism to further
improve model performance. This lightweight attention mechanism includes two
learnable parameters, activations and thresholds, to capture long-range
dependence. Specifically, the threshold picks out part of the spatial feature
map and the activation excite this area. Based on activations and thresholds
training from different types of feature maps, we design fastidious
self-attention module (FSAM) and fastidious inter-attention module (FIAM). And
then construct the Fastidious Attention Network (FANet), which uses U-Net as
the backbone and embeds these two modules, to solve the problems with semantic
segmentation for stem end, blossom end, flaw and ulcer. Compared with some
state-of-the-art deep-learning-based networks under our navel orange dataset,
experiments show that our network is the best performance with pixel accuracy
99.105%, mean accuracy 77.468%, mean IU 70.375% and frequency weighted IU
98.335%. And embedded modules show better discrimination of 5 categories
including background, especially the IU of flaw is increased by 3.165%.",1358,58
"['cs.LG', 'cs.SI']","Anomaly Mining -- Past, Present and Future","Anomaly mining is an important problem that finds numerous applications in
various real world domains such as environmental monitoring, cybersecurity,
finance, healthcare and medicine, to name a few. In this article, I focus on
two areas, (1) point-cloud and (2) graph-based anomaly mining. I aim to present
a broad view of each area, and discuss classes of main research problems,
recent trends and future directions. I conclude with key take-aways and
overarching open problems.",480,42
"['cs.LG', 'cs.AI', 'cs.RO']",DeepRacer: Educational Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning,"DeepRacer is a platform for end-to-end experimentation with RL and can be
used to systematically investigate the key challenges in developing intelligent
control systems. Using the platform, we demonstrate how a 1/18th scale car can
learn to drive autonomously using RL with a monocular camera. It is trained in
simulation with no additional tuning in physical world and demonstrates: 1)
formulation and solution of a robust reinforcement learning algorithm, 2)
narrowing the reality gap through joint perception and dynamics, 3) distributed
on-demand compute architecture for training optimal policies, and 4) a robust
evaluation method to identify when to stop training. It is the first successful
large-scale deployment of deep reinforcement learning on a robotic control
agent that uses only raw camera images as observations and a model-free
learning method to perform robust path planning. We open source our code and
video demo on GitHub: https://git.io/fjxoJ.",967,106
"['cs.CV', 'cs.LG']",Deeply-Recursive Convolutional Network for Image Super-Resolution,"We propose an image super-resolution method (SR) using a deeply-recursive
convolutional network (DRCN). Our network has a very deep recursive layer (up
to 16 recursions). Increasing recursion depth can improve performance without
introducing new parameters for additional convolutions. Albeit advantages,
learning a DRCN is very hard with a standard gradient descent method due to
exploding/vanishing gradients. To ease the difficulty of training, we propose
two extensions: recursive-supervision and skip-connection. Our method
outperforms previous methods by a large margin.",576,65
"['cs.LG', 'cs.CV', 'stat.ML']",A Robust Learning Approach to Domain Adaptive Object Detection,"Domain shift is unavoidable in real-world applications of object detection.
For example, in self-driving cars, the target domain consists of unconstrained
road environments which cannot all possibly be observed in training data.
Similarly, in surveillance applications sufficiently representative training
data may be lacking due to privacy regulations. In this paper, we address the
domain adaptation problem from the perspective of robust learning and show that
the problem may be formulated as training with noisy labels. We propose a
robust object detection framework that is resilient to noise in bounding box
class labels, locations and size annotations. To adapt to the domain shift, the
model is trained on the target domain using a set of noisy object bounding
boxes that are obtained by a detection model trained only in the source domain.
We evaluate the accuracy of our approach in various source/target domain pairs
and demonstrate that the model significantly improves the state-of-the-art on
multiple domain adaptation scenarios on the SIM10K, Cityscapes and KITTI
datasets.",1089,62
"['cs.LG', 'stat.ML']",Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning,"Exogenous state variables and rewards can slow down reinforcement learning by
injecting uncontrolled variation into the reward signal. We formalize exogenous
state variables and rewards and identify conditions under which an MDP with
exogenous state can be decomposed into an exogenous Markov Reward Process
involving only the exogenous state+reward and an endogenous Markov Decision
Process defined with respect to only the endogenous rewards. We also derive a
variance-covariance condition under which Monte Carlo policy evaluation on the
endogenous MDP is accelerated compared to using the full MDP. Similar speedups
are likely to carry over to all RL algorithms. We develop two algorithms for
discovering the exogenous variables and test them on several MDPs. Results show
that the algorithms are practical and can significantly speed up reinforcement
learning.",865,89
"['cs.CV', 'cs.GR', 'cs.LG']",BSP-Net: Generating Compact Meshes via Binary Space Partitioning,"Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only
played a minor role in the deep learning revolution. Leading methods for
learning generative models of shapes rely on implicit functions, and generate
meshes only after expensive iso-surfacing routines. To overcome these
challenges, we are inspired by a classical spatial data structure from computer
graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core
ingredient of BSP is an operation for recursive subdivision of space to obtain
convex sets. By exploiting this property, we devise BSP-Net, a network that
learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net
is unsupervised since no convex shape decompositions are needed for training.
The network is trained to reconstruct a shape using a set of convexes obtained
from a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can
be easily extracted to form a polygon mesh, without any need for iso-surfacing.
The generated meshes are compact (i.e., low-poly) and well suited to represent
sharp geometry; they are guaranteed to be watertight and can be easily
parameterized. We also show that the reconstruction quality by BSP-Net is
competitive with state-of-the-art methods while using much fewer primitives.
Code is available at https://github.com/czq142857/BSP-NET-original.",1374,64
['cs.LG'],"Provable Model-based Nonlinear Bandit and Reinforcement Learning: Shelve Optimism, Embrace Virtual Curvature","This paper studies model-based bandit and reinforcement learning (RL) with
nonlinear function approximations. We propose to study convergence to
approximate local maxima because we show that global convergence is
statistically intractable even for one-layer neural net bandit with a
deterministic reward. For both nonlinear bandit and RL, the paper presents a
model-based algorithm, Virtual Ascent with Online Model Learner (ViOlin), which
provably converges to a local maximum with sample complexity that only depends
on the sequential Rademacher complexity of the model class. Our results imply
novel global or local regret bounds on several concrete settings such as linear
bandit with finite or sparse model class, and two-layer neural net bandit. A
key algorithmic insight is that optimism may lead to over-exploration even for
two-layer neural net model class. On the other hand, for convergence to local
maxima, it suffices to maximize the virtual return if the model can also
reasonably predict the size of the gradient and Hessian of the real return.",1059,108
"['cs.CV', 'cs.GR', 'cs.LG']",A Deeper Look at 3D Shape Classifiers,"We investigate the role of representations and architectures for classifying
3D shapes in terms of their computational efficiency, generalization, and
robustness to adversarial transformations. By varying the number of training
examples and employing cross-modal transfer learning we study the role of
initialization of existing deep architectures for 3D shape classification. Our
analysis shows that multiview methods continue to offer the best generalization
even without pretraining on large labeled image datasets, and even when trained
on simplified inputs such as binary silhouettes. Furthermore, the performance
of voxel-based 3D convolutional networks and point-based architectures can be
improved via cross-modal transfer from image representations. Finally, we
analyze the robustness of 3D shape classifiers to adversarial transformations
and present a novel approach for generating adversarial perturbations of a 3D
shape for multiview classifiers using a differentiable renderer. We find that
point-based networks are more robust to point position perturbations while
voxel-based and multiview networks are easily fooled with the addition of
imperceptible noise to the input.",1187,37
"['cs.LG', 'stat.ML']",Learning Representations for Axis-Aligned Decision Forests through Input Perturbation,"Axis-aligned decision forests have long been the leading class of machine
learning algorithms for modeling tabular data. In many applications of machine
learning such as learning-to-rank, decision forests deliver remarkable
performance. They also possess other coveted characteristics such as
interpretability. Despite their widespread use and rich history, decision
forests to date fail to consume raw structured data such as text, or learn
effective representations for them, a factor behind the success of deep neural
networks in recent years. While there exist methods that construct smoothed
decision forests to achieve representation learning, the resulting models are
decision forests in name only: They are no longer axis-aligned, use stochastic
decisions, or are not interpretable. Furthermore, none of the existing methods
are appropriate for problems that require a Transfer Learning treatment. In
this work, we present a novel but intuitive proposal to achieve representation
learning for decision forests without imposing new restrictions or
necessitating structural changes. Our model is simply a decision forest,
possibly trained using any forest learning algorithm, atop a deep neural
network. By approximating the gradients of the decision forest through input
perturbation, a purely analytical procedure, the decision forest directs the
neural network to learn or fine-tune representations. Our framework has the
advantage that it is applicable to any arbitrary decision forest and that it
allows the use of arbitrary deep neural networks for representation learning.
We demonstrate the feasibility and effectiveness of our proposal through
experiments on synthetic and benchmark classification datasets.",1722,85
['cs.CV'],Distortion Robust Image Classification using Deep Convolutional Neural Network with Discrete Cosine Transform,"Convolutional Neural Network is good at image classification. However, it is
found to be vulnerable to image quality degradation. Even a small amount of
distortion such as noise or blur can severely hamper the performance of these
CNN architectures. Most of the work in the literature strives to mitigate this
problem simply by fine-tuning a pre-trained CNN on mutually exclusive or a
union set of distorted training data. This iterative fine-tuning process with
all known types of distortion is exhaustive and the network struggles to handle
unseen distortions. In this work, we propose distortion robust DCT-Net, a
Discrete Cosine Transform based module integrated into a deep network which is
built on top of VGG16. Unlike other works in the literature, DCT-Net is ""blind""
to the distortion type and level in an image both during training and testing.
As a part of the training process, the proposed DCT module discards input
information which mostly represents the contribution of high frequencies. The
DCT-Net is trained ""blindly"" only once and applied in generic situation without
further retraining. We also extend the idea of traditional dropout and present
a training adaptive version of the same. We evaluate our proposed method
against Gaussian blur, motion blur, salt and pepper noise, Gaussian noise and
speckle noise added to CIFAR-10/100 and ImageNet test sets. Experimental
results demonstrate that once trained, DCT-Net not only generalizes well to a
variety of unseen image distortions but also outperforms other methods in the
literature.",1557,109
"['cs.LG', 'stat.ML']","Breadth-first, Depth-next Training of Random Forests","In this paper we analyze, evaluate, and improve the performance of training
Random Forest (RF) models on modern CPU architectures. An exact,
state-of-the-art binary decision tree building algorithm is used as the basis
of this study. Firstly, we investigate the trade-offs between using different
tree building algorithms, namely breadth-first-search (BFS) and
depth-search-first (DFS). We design a novel, dynamic, hybrid BFS-DFS algorithm
and demonstrate that it performs better than both BFS and DFS, and is more
robust in the presence of workloads with different characteristics. Secondly,
we identify CPU performance bottlenecks when generating trees using this
approach, and propose optimizations to alleviate them. The proposed hybrid tree
building algorithm for RF is implemented in the Snap Machine Learning
framework, and speeds up the training of RFs by 7.8x on average when compared
to state-of-the-art RF solvers (sklearn, H2O, and xgboost) on a range of
datasets, RF configurations, and multi-core CPU architectures.",1029,52
"['stat.ML', 'cs.AI', 'cs.LG', 'stat.CO']",Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam,"Uncertainty computation in deep learning is essential to design robust and
reliable systems. Variational inference (VI) is a promising approach for such
computation, but requires more effort to implement and execute compared to
maximum-likelihood methods. In this paper, we propose new natural-gradient
algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms
can be implemented within the Adam optimizer by perturbing the network weights
during gradient evaluations, and uncertainty estimates can be cheaply obtained
by using the vector that adapts the learning rate. This requires lower memory,
computation, and implementation effort than existing VI methods, while
obtaining uncertainty estimates of comparable quality. Our empirical results
confirm this and further suggest that the weight-perturbation in our algorithm
could be useful for exploration in reinforcement learning and stochastic
optimization.",930,71
"['cs.LG', 'stat.ML']",ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series Modeling,"The spatio-temporal graph learning is becoming an increasingly important
object of graph study. Many application domains involve highly dynamic graphs
where temporal information is crucial, e.g. traffic networks and financial
transaction graphs. Despite the constant progress made on learning structured
data, there is still a lack of effective means to extract dynamic complex
features from spatio-temporal structures. Particularly, conventional models
such as convolutional networks or recurrent neural networks are incapable of
revealing the temporal patterns in short or long terms and exploring the
spatial properties in local or global scope from spatio-temporal graphs
simultaneously. To tackle this problem, we design a novel multi-scale
architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series
modeling. In this U-shaped network, a paired sampling operation is proposed in
spacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in
spatial from its deterministic partition while abstracts multi-resolution
temporal dependencies through dilated recurrent skip connections; based on
previous settings in the downsampling, the unpooling (ST-Unpool) restores the
original structure of spatio-temporal graphs and resumes regular intervals
within graph sequences. Experiments on spatio-temporal prediction tasks
demonstrate that our model effectively captures comprehensive features in
multiple scales and achieves substantial improvements over mainstream methods
on several real-world datasets.",1542,78
['cs.CV'],Efficient Vision Transformers via Fine-Grained Manifold Distillation,"This paper studies the model compression problem of vision transformers.
Benefit from the self-attention module, transformer architectures have shown
extraordinary performance on many computer vision tasks. Although the network
performance is boosted, transformers are often required more computational
resources including memory usage and the inference complexity. Compared with
the existing knowledge distillation approaches, we propose to excavate useful
information from the teacher transformer through the relationship between
images and the divided patches. We then explore an efficient fine-grained
manifold distillation approach that simultaneously calculates cross-images,
cross-patch, and random-selected manifolds in teacher and student models.
Experimental results conducted on several benchmarks demonstrate the
superiority of the proposed algorithm for distilling portable transformer
models with higher performance. For example, our approach achieves 75.06% Top-1
accuracy on the ImageNet-1k dataset for training a DeiT-Tiny model, which
outperforms other ViT distillation methods.",1096,68
"['cs.LG', 'q-bio.NC']",Improved Training of Sparse Coding Variational Autoencoder via Weight Normalization,"Learning a generative model of visual information with sparse and
compositional features has been a challenge for both theoretical neuroscience
and machine learning communities. Sparse coding models have achieved great
success in explaining the receptive fields of mammalian primary visual cortex
with sparsely activated latent representation. In this paper, we focus on a
recently proposed model, sparse coding variational autoencoder (SVAE) (Barello
et al., 2018), and show that the end-to-end training scheme of SVAE leads to a
large group of decoding filters not fully optimized with noise-like receptive
fields. We propose a few heuristics to improve the training of SVAE and show
that a unit $L_2$ norm constraint on the decoder is critical to produce sparse
coding filters. Such normalization can be considered as local lateral
inhibition in the cortex. We verify this claim empirically on both natural
image patches and MNIST dataset and show that projection of the filters onto
unit norm drastically increases the number of active filters. Our results
highlight the importance of weight normalization for learning sparse
representation from data and suggest a new way of reducing the number of
inactive latent components in VAE learning.",1246,83
['cs.CV'],Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks,"Autonomous vehicles commonly rely on highly detailed birds-eye-view maps of
their environment, which capture both static elements of the scene such as road
layout as well as dynamic elements such as other cars and pedestrians.
Generating these map representations on the fly is a complex multi-stage
process which incorporates many important vision-based elements, including
ground plane estimation, road segmentation and 3D object detection. In this
work we present a simple, unified approach for estimating maps directly from
monocular images using a single end-to-end deep learning architecture. For the
maps themselves we adopt a semantic Bayesian occupancy grid framework, allowing
us to trivially accumulate information over multiple cameras and timesteps. We
demonstrate the effectiveness of our approach by evaluating against several
challenging baselines on the NuScenes and Argoverse datasets, and show that we
are able to achieve a relative improvement of 9.1% and 22.3% respectively
compared to the best-performing existing method.",1043,84
"['stat.ML', 'cs.CV', 'cs.LG']",Mutual Information Gradient Estimation for Representation Learning,"Mutual Information (MI) plays an important role in representation learning.
However, MI is unfortunately intractable in continuous and high-dimensional
settings. Recent advances establish tractable and scalable MI estimators to
discover useful representation. However, most of the existing methods are not
capable of providing an accurate estimation of MI with low-variance when the MI
is large. We argue that directly estimating the gradients of MI is more
appealing for representation learning than estimating MI in itself. To this
end, we propose the Mutual Information Gradient Estimator (MIGE) for
representation learning based on the score estimation of implicit
distributions. MIGE exhibits a tight and smooth gradient estimation of MI in
the high-dimensional and large-MI settings. We expand the applications of MIGE
in both unsupervised learning of deep representations based on InfoMax and the
Information Bottleneck method. Experimental results have indicated significant
performance improvement in learning useful representation.",1041,66
['cs.CV'],Human Synthesis and Scene Compositing,"Generating good quality and geometrically plausible synthetic images of
humans with the ability to control appearance, pose and shape parameters, has
become increasingly important for a variety of tasks ranging from photo
editing, fashion virtual try-on, to special effects and image compression. In
this paper, we propose HUSC, a HUman Synthesis and Scene Compositing framework
for the realistic synthesis of humans with different appearance, in novel poses
and scenes. Central to our formulation is 3d reasoning for both people and
scenes, in order to produce realistic collages, by correctly modeling
perspective effects and occlusion, by taking into account scene semantics and
by adequately handling relative scales. Conceptually our framework consists of
three components: (1) a human image synthesis model with controllable pose and
appearance, based on a parametric representation, (2) a person insertion
procedure that leverages the geometry and semantics of the 3d scene, and (3) an
appearance compositing process to create a seamless blending between the colors
of the scene and the generated human image, and avoid visual artifacts. The
performance of our framework is supported by both qualitative and quantitative
results, in particular state-of-the art synthesis scores for the DeepFashion
dataset.",1313,37
['cs.CV'],ShuffleDet: Real-Time Vehicle Detection Network in On-board Embedded UAV Imagery,"On-board real-time vehicle detection is of great significance for UAVs and
other embedded mobile platforms. We propose a computationally inexpensive
detection network for vehicle detection in UAV imagery which we call
ShuffleDet. In order to enhance the speed-wise performance, we construct our
method primarily using channel shuffling and grouped convolutions. We apply
inception modules and deformable modules to consider the size and geometric
shape of the vehicles. ShuffleDet is evaluated on CARPK and PUCPR+ datasets and
compared against the state-of-the-art real-time object detection networks.
ShuffleDet achieves 3.8 GFLOPs while it provides competitive performance on
test sets of both datasets. We show that our algorithm achieves real-time
performance by running at the speed of 14 frames per second on NVIDIA Jetson
TX2 showing high potential for this method for real-time processing in UAVs.",905,80
"['cs.LG', 'cs.CL']",Efficient Explanations from Empirical Explainers,"Amid a discussion about Green AI in which we see explainability neglected, we
explore the possibility to efficiently approximate computationally expensive
explainers. To this end, we propose feature attribution modelling with
Empirical Explainers. Empirical Explainers learn from data to predict the
attribution maps of expensive explainers. We train and test Empirical
Explainers in the language domain and find that they model their expensive
counterparts surprisingly well, at a fraction of the cost. They could thus
mitigate the computational burden of neural explanations significantly, in
applications that tolerate an approximation error.",645,48
"['cs.CV', 'cs.AI', 'eess.IV']",Sea-Net: Squeeze-And-Excitation Attention Net For Diabetic Retinopathy Grading,"Diabetes is one of the most common disease in individuals. \textit{Diabetic
retinopathy} (DR) is a complication of diabetes, which could lead to blindness.
Automatic DR grading based on retinal images provides a great diagnostic and
prognostic value for treatment planning. However, the subtle differences among
severity levels make it difficult to capture important features using
conventional methods. To alleviate the problems, a new deep learning
architecture for robust DR grading is proposed, referred to as SEA-Net, in
which, spatial attention and channel attention are alternatively carried out
and boosted with each other, improving the classification performance. In
addition, a hybrid loss function is proposed to further maximize the
inter-class distance and reduce the intra-class variability. Experimental
results have shown the effectiveness of the proposed architecture.",886,78
"['cs.CV', 'cs.LG', 'eess.IV']",Developing High Quality Training Samples for Deep Learning Based Local Climate Zone Classification in Korea,"Two out of three people will be living in urban areas by 2050, as projected
by the United Nations, emphasizing the need for sustainable urban development
and monitoring. Common urban footprint data provide high-resolution city
extents but lack essential information on the distribution, pattern, and
characteristics. The Local Climate Zone (LCZ) offers an efficient and
standardized framework that can delineate the internal structure and
characteristics of urban areas. Global-scale LCZ mapping has been explored, but
are limited by low accuracy, variable labeling quality, or domain adaptation
challenges. Instead, this study developed a custom LCZ data to map key Korean
cities using a multi-scale convolutional neural network. Results demonstrated
that using a novel, custom LCZ data with deep learning can generate more
accurate LCZ map results compared to conventional community-based LCZ mapping
with machine learning as well as transfer learning of the global So2Sat
dataset.",983,107
"['cs.LG', 'cs.AI']",An Iterative Closest Points Approach to Neural Generative Models,"We present a simple way to learn a transformation that maps samples of one
distribution to the samples of another distribution. Our algorithm comprises an
iteration of 1) drawing samples from some simple distribution and transforming
them using a neural network, 2) determining pairwise correspondences between
the transformed samples and training data (or a minibatch), and 3) optimizing
the weights of the neural network being trained to minimize the distances
between the corresponding vectors. This can be considered as a variant of the
Iterative Closest Points (ICP) algorithm, common in geometric computer vision,
although ICP typically operates on sensor point clouds and linear transforms
instead of random sample sets and neural nonlinear transforms. We demonstrate
the algorithm on simple synthetic data and MNIST data. We furthermore
demonstrate that the algorithm is capable of handling distributions with both
continuous and discrete variables.",957,64
"['cs.LG', 'cs.CV', 'cs.NI']",Video Segment Copy Detection Using Memory Constrained Hierarchical Batch-Normalized LSTM Autoencoder,"In this report, we introduce a video hashing method for scalable video
segment copy detection. The objective of video segment copy detection is to
find the video (s) present in a large database, one of whose segments (cropped
in time) is a (transformed) copy of the given query video. This transformation
may be temporal (for example frame dropping, change in frame rate) or spatial
(brightness and contrast change, addition of noise etc.) in nature although the
primary focus of this report is detecting temporal attacks. The video hashing
method proposed by us uses a deep learning neural network to learn variable
length binary hash codes for the entire video considering both temporal and
spatial features into account. This is in contrast to most existing video
hashing methods, as they use conventional image hashing techniques to obtain
hash codes for a video after extracting features for every frame or certain key
frames, in which case the temporal information present in the video is not
exploited. Our hashing method is specifically resilient to time cropping making
it extremely useful in video segment copy detection. Experimental results
obtained on the large augmented dataset consisting of around 25,000 videos with
segment copies demonstrate the efficacy of our proposed video hashing method.",1310,100
['cs.CV'],SOLQ: Segmenting Objects by Learning Queries,"In this paper, we propose an end-to-end framework for instance segmentation.
Based on the recently introduced DETR [1], our method, termed SOLQ, segments
objects by learning unified queries. In SOLQ, each query represents one object
and has multiple representations: class, location and mask. The object queries
learned perform classification, box regression and mask encoding simultaneously
in an unified vector form. During training phase, the mask vectors encoded are
supervised by the compression coding of raw spatial masks. In inference time,
mask vectors produced can be directly transformed to spatial masks by the
inverse process of compression coding. Experimental results show that SOLQ can
achieve state-of-the-art performance, surpassing most of existing approaches.
Moreover, the joint learning of unified query representation can greatly
improve the detection performance of original DETR. We hope our SOLQ can serve
as a strong baseline for the Transformer-based instance segmentation. Code is
available at https://github.com/megvii-research/SOLQ.",1063,44
"['cs.CV', 'eess.IV']",Representation Based Regression for Object Distance Estimation,"In this study, we propose a novel approach to predict the distances of the
detected objects in an observed scene. The proposed approach modifies the
recently proposed Convolutional Support Estimator Networks (CSENs). CSENs are
designed to compute a direct mapping for the Support Estimation (SE) task in a
representation-based classification problem. We further propose and demonstrate
that representation-based methods (sparse or collaborative representation) can
be used in well-designed regression problems. To the best of our knowledge,
this is the first representation-based method proposed for performing a
regression task by utilizing the modified CSENs; and hence, we name this novel
approach as Representation-based Regression (RbR). The initial version of CSENs
has a proxy mapping stage (i.e., a coarse estimation for the support set) that
is required for the input. In this study, we improve the CSEN model by
proposing Compressive Learning CSEN (CL-CSEN) that has the ability to jointly
optimize the so-called proxy mapping stage along with convolutional layers. The
experimental evaluations using the KITTI 3D Object Detection distance
estimation dataset show that the proposed method can achieve a significantly
improved distance estimation performance over all competing methods. Finally,
the software implementations of the methods are publicly shared at
https://github.com/meteahishali/CSENDistance.",1417,62
['cs.CV'],Fast Convergence for Object Detection by Learning how to Combine Error Functions,"In this paper, we introduce an innovative method to improve the convergence
speed and accuracy of object detection neural networks. Our approach,
CONVERGE-FAST-AUXNET, is based on employing multiple, dependent loss metrics
and weighting them optimally using an on-line trained auxiliary network.
Experiments are performed in the well-known RoboCup@Work challenge environment.
A fully convolutional segmentation network is trained on detecting objects'
pickup points. We empirically obtain an approximate measure for the rate of
success of a robotic pickup operation based on the accuracy of the object
detection network. Our experiments show that adding an optimally weighted
Euclidean distance loss to a network trained on the commonly used Intersection
over Union (IoU) metric reduces the convergence time by 42.48%. The estimated
pickup rate is improved by 39.90%. Compared to state-of-the-art task weighting
methods, the improvement is 24.5% in convergence, and 15.8% on the estimated
pickup rate.",1001,80
"['cs.CV', 'cs.GR']",Explicit Clothing Modeling for an Animatable Full-Body Avatar,"Recent work has shown great progress in building photorealistic animatable
full-body codec avatars, but these avatars still face difficulties in
generating high-fidelity animation of clothing. To address the difficulties, we
propose a method to build an animatable clothed body avatar with an explicit
representation of the clothing on the upper body from multi-view captured
videos. We use a two-layer mesh representation to separately register the 3D
scans with templates. In order to improve the photometric correspondence across
different frames, texture alignment is then performed through inverse rendering
of the clothing geometry and texture predicted by a variational autoencoder. We
then train a new two-layer codec avatar with separate modeling of the upper
clothing and the inner body layer. To learn the interaction between the body
dynamics and clothing states, we use a temporal convolution network to predict
the clothing latent code based on a sequence of input skeletal poses. We show
photorealistic animation output for three different actors, and demonstrate the
advantage of our clothed-body avatars over single-layer avatars in the previous
work. We also show the benefit of an explicit clothing model which allows the
clothing texture to be edited in the animation output.",1295,61
"['cs.CV', 'cs.LG', 'stat.ML']",Exchangeable deep neural networks for set-to-set matching and learning,"Matching two different sets of items, called heterogeneous set-to-set
matching problem, has recently received attention as a promising problem. The
difficulties are to extract features to match a correct pair of different sets
and also preserve two types of exchangeability required for set-to-set
matching: the pair of sets, as well as the items in each set, should be
exchangeable. In this study, we propose a novel deep learning architecture to
address the abovementioned difficulties and also an efficient training
framework for set-to-set matching. We evaluate the methods through experiments
based on two industrial applications: fashion set recommendation and group
re-identification. In these experiments, we show that the proposed method
provides significant improvements and results compared with the
state-of-the-art methods, thereby validating our architecture for the
heterogeneous set matching problem.",916,70
"['cs.LG', 'cs.AI']",System Design for a Data-driven and Explainable Customer Sentiment Monitor,"The most important goal of customer services is to keep the customer
satisfied. However, service resources are always limited and must be
prioritized. Therefore, it is important to identify customers who potentially
become unsatisfied and might lead to escalations. Today this prioritization of
customers is often done manually. Data science on IoT data (esp. log data) for
machine health monitoring, as well as analytics on enterprise data for customer
relationship management (CRM) have mainly been researched and applied
independently. In this paper, we present a framework for a data-driven decision
support system which combines IoT and enterprise data to model customer
sentiment. Such decision support systems can help to prioritize customers and
service resources to effectively troubleshoot problems or even avoid them. The
framework is applied in a real-world case study with a major medical device
manufacturer. This includes a fully automated and interpretable machine
learning pipeline designed to meet the requirements defined with domain experts
and end users. The overall framework is currently deployed, learns and
evaluates predictive models from terabytes of IoT and enterprise data to
actively monitor the customer sentiment for a fleet of thousands of high-end
medical devices. Furthermore, we provide an anonymized industrial benchmark
dataset for the research community.",1393,74
"['cs.LG', 'stat.ML']",MHVAE: a Human-Inspired Deep Hierarchical Generative Model for Multimodal Representation Learning,"Humans are able to create rich representations of their external reality.
Their internal representations allow for cross-modality inference, where
available perceptions can induce the perceptual experience of missing input
modalities. In this paper, we contribute the Multimodal Hierarchical
Variational Auto-encoder (MHVAE), a hierarchical multimodal generative model
for representation learning. Inspired by human cognitive models, the MHVAE is
able to learn modality-specific distributions, of an arbitrary number of
modalities, and a joint-modality distribution, responsible for cross-modality
inference. We formally derive the model's evidence lower bound and propose a
novel methodology to approximate the joint-modality posterior based on
modality-specific representation dropout. We evaluate the MHVAE on standard
multimodal datasets. Our model performs on par with other state-of-the-art
generative models regarding joint-modality reconstruction from arbitrary input
modalities and cross-modality inference.",1016,97
['cs.CV'],Zero-Annotation Object Detection with Web Knowledge Transfer,"Object detection is one of the major problems in computer vision, and has
been extensively studied. Most of the existing detection works rely on
labor-intensive supervision, such as ground truth bounding boxes of objects or
at least image-level annotations. On the contrary, we propose an object
detection method that does not require any form of human annotation on target
tasks, by exploiting freely available web images. In order to facilitate
effective knowledge transfer from web images, we introduce a multi-instance
multi-label domain adaption learning framework with two key innovations. First
of all, we propose an instance-level adversarial domain adaptation network with
attention on foreground objects to transfer the object appearances from web
domain to target domain. Second, to preserve the class-specific semantic
structure of transferred object features, we propose a simultaneous transfer
mechanism to transfer the supervision across domains through pseudo strong
label generation. With our end-to-end framework that simultaneously learns a
weakly supervised detector and transfers knowledge across domains, we achieved
significant improvements over baseline methods on the benchmark datasets.",1212,60
"['cs.LG', 'cs.AI', 'cs.MA', 'q-bio.NC', 'stat.ML']",A Story of Two Streams: Reinforcement Learning Models from Human Behavior and Neuropsychiatry,"Drawing an inspiration from behavioral studies of human decision making, we
propose here a more general and flexible parametric framework for reinforcement
learning that extends standard Q-learning to a two-stream model for processing
positive and negative rewards, and allows to incorporate a wide range of
reward-processing biases -- an important component of human decision making
which can help us better understand a wide spectrum of multi-agent interactions
in complex real-world socioeconomic systems, as well as various
neuropsychiatric conditions associated with disruptions in normal reward
processing. From the computational perspective, we observe that the proposed
Split-QL model and its clinically inspired variants consistently outperform
standard Q-Learning and SARSA methods, as well as recently proposed Double
Q-Learning approaches, on simulated tasks with particular reward distributions,
a real-world dataset capturing human decision-making in gambling tasks, and the
Pac-Man game in a lifelong learning setting across different reward
stationarities.",1072,93
['stat.ML'],Interactive Graphics for Visually Diagnosing Forest Classifiers in R,"This paper describes structuring data and constructing plots to explore
forest classification models interactively. A forest classifier is an example
of an ensemble, produced by bagging multiple trees. The process of bagging and
combining results from multiple trees, produces numerous diagnostics which,
with interactive graphics, can provide a lot of insight into class structure in
high dimensions. Various aspects are explored in this paper, to assess model
complexity, individual model contributions, variable importance and dimension
reduction, and uncertainty in prediction associated with individual
observations. The ideas are applied to the random forest algorithm, and to the
projection pursuit forest, but could be more broadly applied to other bagged
ensembles. Interactive graphics are built in R, using the ggplot2, plotly, and
shiny packages.",858,68
['cs.LG'],Memory-based Optimization Methods for Model-Agnostic Meta-Learning,"Recently, model-agnostic meta-learning (MAML) has garnered tremendous
attention. However, stochastic optimization of MAML is still immature. Existing
algorithms for MAML are based on the ``episode"" idea by sampling a number of
tasks and a number of data points for each sampled task at each iteration for
updating the meta-model. However, they either do not necessarily guarantee
convergence with a constant mini-batch size or require processing a larger
number of tasks at every iteration, which is not viable for continual learning
or cross-device federated learning where only a small number of tasks are
available per-iteration or per-round. This paper addresses these issues by (i)
proposing efficient memory-based stochastic algorithms for MAML with a
diminishing convergence error, which only requires sampling a constant number
of tasks and a constant number of examples per-task per-iteration; (ii)
proposing communication-efficient distributed memory-based MAML algorithms for
personalized federated learning in both the cross-device (w/ client sampling)
and the cross-silo (w/o client sampling) settings. The key novelty of the
proposed algorithms is to maintain an individual personalized model (aka
memory) for each task besides the meta-model and only update them for the
sampled tasks by a momentum method that incorporates historical updates at each
iteration. The theoretical results significantly improve the optimization
theory for MAML and the empirical results also corroborate the theory.",1510,66
['cs.CV'],Paving the Way for Image Understanding: A New Kind of Image Decomposition is Desired,"In this paper we present an unconventional image segmentation approach which
is devised to meet the requirements of image understanding and pattern
recognition tasks. Generally image understanding assumes interplay of two
sub-processes: image information content discovery and image information
content interpretation. Despite of its widespread use, the notion of ""image
information content"" is still ill defined, intuitive, and ambiguous. Most
often, it is used in the Shannon's sense, which means information content
assessment averaged over the whole signal ensemble. Humans, however,rarely
resort to such estimates. They are very effective in decomposing images into
their meaningful constituents and focusing attention to the perceptually
relevant image parts. We posit that following the latest findings in human
attention vision studies and the concepts of Kolmogorov's complexity theory an
unorthodox segmentation approach can be proposed that provides effective image
decomposition to information preserving image fragments well suited for
subsequent image interpretation. We provide some illustrative examples,
demonstrating effectiveness of this approach.",1166,84
['cs.CV'],Efficient Information Theoretic Clustering on Discrete Lattices,"We consider the problem of clustering data that reside on discrete, low
dimensional lattices. Canonical examples for this setting are found in image
segmentation and key point extraction. Our solution is based on a recent
approach to information theoretic clustering where clusters result from an
iterative procedure that minimizes a divergence measure. We replace costly
processing steps in the original algorithm by means of convolutions. These
allow for highly efficient implementations and thus significantly reduce
runtime. This paper therefore bridges a gap between machine learning and signal
processing.",611,63
"['cs.LG', 'cs.AI', 'cs.MA', 'stat.ML']",Health-Informed Policy Gradients for Multi-Agent Reinforcement Learning,"This paper proposes a definition of system health in the context of multiple
agents optimizing a joint reward function. We use this definition as a credit
assignment term in a policy gradient algorithm to distinguish the contributions
of individual agents to the global reward. The health-informed credit
assignment is then extended to a multi-agent variant of the proximal policy
optimization algorithm and demonstrated on particle and multiwalker robot
environments that have characteristics such as system health, risk-taking,
semi-expendable agents, continuous action spaces, and partial observability. We
show significant improvement in learning performance compared to policy
gradient methods that do not perform multi-agent credit assignment.",749,71
"['cs.CV', 'eess.IV', 'eess.SP']",Point Cloud Attribute Compression via Successive Subspace Graph Transform,"Inspired by the recently proposed successive subspace learning (SSL)
principles, we develop a successive subspace graph transform (SSGT) to address
point cloud attribute compression in this work. The octree geometry structure
is utilized to partition the point cloud, where every node of the octree
represents a point cloud subspace with a certain spatial size. We design a
weighted graph with self-loop to describe the subspace and define a graph
Fourier transform based on the normalized graph Laplacian. The transforms are
applied to large point clouds from the leaf nodes to the root node of the
octree recursively, while the represented subspace is expanded from the
smallest one to the whole point cloud successively. It is shown by experimental
results that the proposed SSGT method offers better R-D performances than the
previous Region Adaptive Haar Transform (RAHT) method.",884,73
['cs.CV'],Recursive Visual Attention in Visual Dialog,"Visual dialog is a challenging vision-language task, which requires the agent
to answer multi-round questions about an image. It typically needs to address
two major problems: (1) How to answer visually-grounded questions, which is the
core challenge in visual question answering (VQA); (2) How to infer the
co-reference between questions and the dialog history. An example of visual
co-reference is: pronouns (\eg, ``they'') in the question (\eg, ``Are they on
or off?'') are linked with nouns (\eg, ``lamps'') appearing in the dialog
history (\eg, ``How many lamps are there?'') and the object grounded in the
image. In this work, to resolve the visual co-reference for visual dialog, we
propose a novel attention mechanism called Recursive Visual Attention (RvA).
Specifically, our dialog agent browses the dialog history until the agent has
sufficient confidence in the visual co-reference resolution, and refines the
visual attention recursively. The quantitative and qualitative experimental
results on the large-scale VisDial v0.9 and v1.0 datasets demonstrate that the
proposed RvA not only outperforms the state-of-the-art methods, but also
achieves reasonable recursion and interpretable attention maps without
additional annotations. The code is available at
\url{https://github.com/yuleiniu/rva}.",1308,43
"['cs.CV', 'cs.AI', 'cs.CL', 'cs.LO']",HySTER: A Hybrid Spatio-Temporal Event Reasoner,"The task of Video Question Answering (VideoQA) consists in answering natural
language questions about a video and serves as a proxy to evaluate the
performance of a model in scene sequence understanding. Most methods designed
for VideoQA up-to-date are end-to-end deep learning architectures which
struggle at complex temporal and causal reasoning and provide limited
transparency in reasoning steps. We present the HySTER: a Hybrid
Spatio-Temporal Event Reasoner to reason over physical events in videos. Our
model leverages the strength of deep learning methods to extract information
from video frames with the reasoning capabilities and explainability of
symbolic artificial intelligence in an answer set programming framework. We
define a method based on general temporal, causal and physics rules which can
be transferred across tasks. We apply our model to the CLEVRER dataset and
demonstrate state-of-the-art results in question answering accuracy. This work
sets the foundations for the incorporation of inductive logic programming in
the field of VideoQA.",1065,47
"['cs.CV', 'cs.LG']",Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR,"Self-supervised monocular depth prediction provides a cost-effective solution
to obtain the 3D location of each pixel. However, the existing approaches
usually lead to unsatisfactory accuracy, which is critical for autonomous
robots. In this paper, we propose a novel two-stage network to advance the
self-supervised monocular dense depth learning by leveraging low-cost sparse
(e.g. 4-beam) LiDAR. Unlike the existing methods that use sparse LiDAR mainly
in a manner of time-consuming iterative post-processing, our model fuses
monocular image features and sparse LiDAR features to predict initial depth
maps. Then, an efficient feed-forward refine network is further designed to
correct the errors in these initial depth maps in pseudo-3D space with
real-time performance. Extensive experiments show that our proposed model
significantly outperforms all the state-of-the-art self-supervised methods, as
well as the sparse-LiDAR-based methods on both self-supervised monocular depth
prediction and completion tasks. With the accurate dense depth prediction, our
model outperforms the state-of-the-art sparse-LiDAR-based method
(Pseudo-LiDAR++) by more than 68% for the downstream task monocular 3D object
detection on the KITTI Leaderboard.",1241,68
"['cs.LG', 'cs.AI', 'stat.ML']",MDP Playground: A Design and Debug Testbed for Reinforcement Learning,"We present \emph{MDP Playground}, an efficient testbed for Reinforcement
Learning (RL) agents with \textit{orthogonal} dimensions that can be controlled
independently to challenge agents in different ways and obtain varying degrees
of hardness in generated environments. We consider and allow control over a
wide variety of dimensions, including \textit{delayed rewards},
\textit{rewardable sequences}, \textit{density of rewards},
\textit{stochasticity}, \textit{image representations}, \textit{irrelevant
features}, \textit{time unit}, \textit{action range} and more. We define a
parameterised collection of fast-to-run toy environments in \textit{OpenAI Gym}
by varying these dimensions and propose to use these for the initial design and
development of agents. We also provide wrappers that inject these dimensions
into complex environments from \textit{Atari} and \textit{Mujoco} to allow for
evaluating agent robustness. We further provide various example use-cases and
instructions on how to use \textit{MDP Playground} to design and debug agents.
We believe that \textit{MDP Playground} is a valuable testbed for researchers
designing new, adaptive and intelligent RL agents and those wanting to unit
test their agents.",1227,69
"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']",Generative Counterfactual Introspection for Explainable Deep Learning,"In this work, we propose an introspection technique for deep neural networks
that relies on a generative model to instigate salient editing of the input
image for model interpretation. Such modification provides the fundamental
interventional operation that allows us to obtain answers to counterfactual
inquiries, i.e., what meaningful change can be made to the input image in order
to alter the prediction. We demonstrate how to reveal interesting properties of
the given classifiers by utilizing the proposed introspection approach on both
the MNIST and the CelebA dataset.",576,69
['cs.CV'],A New Approach of Improving CFA Image for Digital Camera's,"This paper work directly towards the improving the quality of the image for
the digital cameras and other visual capturing products. In this Paper, the
authors clearly defines the problems occurs in the CFA image. A different
methodology for removing the noise is discuses in the paper for color
correction and color balancing of the image. At the same time, the authors also
proposed a new methodology of providing denoisiing process before the
demosaickingfor the improving the image quality of CFA which is much efficient
then the other previous defined. The demosaicking process for producing the
colors in the image in a best way is also discuss.",651,58
['cs.CV'],Adversarial Framework for Unsupervised Learning of Motion Dynamics in Videos,"Human behavior understanding in videos is a complex, still unsolved problem
and requires to accurately model motion at both the local (pixel-wise dense
prediction) and global (aggregation of motion cues) levels. Current approaches
based on supervised learning require large amounts of annotated data, whose
scarce availability is one of the main limiting factors to the development of
general solutions. Unsupervised learning can instead leverage the vast amount
of videos available on the web and it is a promising solution for overcoming
the existing limitations. In this paper, we propose an adversarial GAN-based
framework that learns video representations and dynamics through a
self-supervision mechanism in order to perform dense and global prediction in
videos. Our approach synthesizes videos by 1) factorizing the process into the
generation of static visual content and motion, 2) learning a suitable
representation of a motion latent space in order to enforce spatio-temporal
coherency of object trajectories, and 3) incorporating motion estimation and
pixel-wise dense prediction into the training procedure. Self-supervision is
enforced by using motion masks produced by the generator, as a co-product of
its generation process, to supervise the discriminator network in performing
dense prediction. Performance evaluation, carried out on standard benchmarks,
shows that our approach is able to learn, in an unsupervised way, both local
and global video dynamics. The learned representations, then, support the
training of video object segmentation methods with sensibly less (about 50%)
annotations, giving performance comparable to the state of the art.
Furthermore, the proposed method achieves promising performance in generating
realistic videos, outperforming state-of-the-art approaches especially on
motion-related metrics.",1845,76
['cs.CV'],Deep Learning-based Face Super-Resolution: A Survey,"Face super-resolution (FSR), also known as face hallucination, which is aimed
at enhancing the resolution of low-resolution (LR) face images to generate
high-resolution (HR) face images, is a domain-specific image super-resolution
problem. Recently, FSR has received considerable attention and witnessed
dazzling advances with the development of deep learning techniques. To date,
few summaries of the studies on the deep learning-based FSR are available. In
this survey, we present a comprehensive review of deep learning-based FSR
methods in a systematic manner. First, we summarize the problem formulation of
FSR and introduce popular assessment metrics and loss functions. Second, we
elaborate on the facial characteristics and popular datasets used in FSR.
Third, we roughly categorize existing methods according to the utilization of
facial characteristics. In each category, we start with a general description
of design principles, then present an overview of representative approaches,
and then discuss the pros and cons among them. Fourth, we evaluate the
performance of some state-of-the-art methods. Fifth, joint FSR and other tasks,
and FSR-related applications are roughly introduced. Finally, we envision the
prospects of further technological advancement in this field. A curated list of
papers and resources to face super-resolution are available at
\url{https://github.com/junjun-jiang/Face-Hallucination-Benchmark}",1433,51
"['cs.LG', 'stat.ML']",METEOR: Learning Memory and Time Efficient Representations from Multi-modal Data Streams,"Many learning tasks involve multi-modal data streams, where continuous data
from different modes convey a comprehensive description about objects. A major
challenge in this context is how to efficiently interpret multi-modal
information in complex environments. This has motivated numerous studies on
learning unsupervised representations from multi-modal data streams. These
studies aim to understand higher-level contextual information (e.g., a Twitter
message) by jointly learning embeddings for the lower-level semantic units in
different modalities (e.g., text, user, and location of a Twitter message).
However, these methods directly associate each low-level semantic unit with a
continuous embedding vector, which results in high memory requirements. Hence,
deploying and continuously learning such models in low-memory devices (e.g.,
mobile devices) becomes a problem. To address this problem, we present METEOR,
a novel MEmory and Time Efficient Online Representation learning technique,
which: (1) learns compact representations for multi-modal data by sharing
parameters within semantically meaningful groups and preserves the
domain-agnostic semantics; (2) can be accelerated using parallel processes to
accommodate different stream rates while capturing the temporal changes of the
units; and (3) can be easily extended to capture implicit/explicit external
knowledge related to multi-modal data streams. We evaluate METEOR using two
types of multi-modal data streams (i.e., social media streams and shopping
transaction streams) to demonstrate its ability to adapt to different domains.
Our results show that METEOR preserves the quality of the representations while
reducing memory usage by around 80% compared to the conventional
memory-intensive embeddings.",1775,88
['cs.LG'],Error Bounds of Imitating Policies and Environments,"Imitation learning trains a policy by mimicking expert demonstrations.
Various imitation methods were proposed and empirically evaluated, meanwhile,
their theoretical understanding needs further studies. In this paper, we
firstly analyze the value gap between the expert policy and imitated policies
by two imitation methods, behavioral cloning and generative adversarial
imitation. The results support that generative adversarial imitation can reduce
the compounding errors compared to behavioral cloning, and thus has a better
sample complexity. Noticed that by considering the environment transition model
as a dual agent, imitation learning can also be used to learn the environment
model. Therefore, based on the bounds of imitating policies, we further analyze
the performance of imitating environments. The results show that environment
models can be more effectively imitated by generative adversarial imitation
than behavioral cloning, suggesting a novel application of adversarial
imitation for model-based reinforcement learning. We hope these results could
inspire future advances in imitation learning and model-based reinforcement
learning.",1154,51
"['cs.LG', 'physics.flu-dyn']",Simulating Continuum Mechanics with Multi-Scale Graph Neural Networks,"Continuum mechanics simulators, numerically solving one or more partial
differential equations, are essential tools in many areas of science and
engineering, but their performance often limits application in practice. Recent
modern machine learning approaches have demonstrated their ability to
accelerate spatio-temporal predictions, although, with only moderate accuracy
in comparison. Here we introduce MultiScaleGNN, a novel multi-scale graph
neural network model for learning to infer unsteady continuum mechanics.
MultiScaleGNN represents the physical domain as an unstructured set of nodes,
and it constructs one or more graphs, each of them encoding different scales of
spatial resolution. Successive learnt message passing between these graphs
improves the ability of GNNs to capture and forecast the system state in
problems encompassing a range of length scales. Using graph representations,
MultiScaleGNN can impose periodic boundary conditions as an inductive bias on
the edges in the graphs, and achieve independence to the nodes' positions. We
demonstrate this method on advection problems and incompressible fluid
dynamics. Our results show that the proposed model can generalise from uniform
advection fields to high-gradient fields on complex domains at test time and
infer long-term Navier-Stokes solutions within a range of Reynolds numbers.
Simulations obtained with MultiScaleGNN are between two and four orders of
magnitude faster than the ones on which it was trained.",1492,69
"['cs.LG', 'cs.AI']",Deep-RBF Networks for Anomaly Detection in Automotive Cyber-Physical Systems,"Deep Neural Networks (DNNs) are popularly used for implementing autonomy
related tasks in automotive Cyber-Physical Systems (CPSs). However, these
networks have been shown to make erroneous predictions to anomalous inputs,
which manifests either due to Out-of-Distribution (OOD) data or adversarial
attacks. To detect these anomalies, a separate DNN called assurance monitor is
often trained and used in parallel to the controller DNN, increasing the
resource burden and latency. We hypothesize that a single network that can
perform controller predictions and anomaly detection is necessary to reduce the
resource requirements. Deep-Radial Basis Function (RBF) networks provide a
rejection class alongside the class predictions, which can be utilized for
detecting anomalies at runtime. However, the use of RBF activation functions
limits the applicability of these networks to only classification tasks. In
this paper, we show how the deep-RBF network can be used for detecting
anomalies in CPS regression tasks such as continuous steering predictions.
Further, we design deep-RBF networks using popular DNNs such as NVIDIA DAVE-II,
and ResNet20, and then use the resulting rejection class for detecting
adversarial attacks such as a physical attack and data poison attack. Finally,
we evaluate these attacks and the trained deep-RBF networks using a hardware
CPS testbed called DeepNNCar and a real-world German Traffic Sign Benchmark
(GTSB) dataset. Our results show that the deep-RBF networks can robustly detect
these attacks in a short time without additional resource requirements.",1589,76
"['cs.LG', 'cs.NE', 'stat.ML']",A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS,"This work proposes a novel Graph-based neural ArchiTecture Encoding Scheme,
a.k.a. GATES, to improve the predictor-based neural architecture search.
Specifically, different from existing graph-based schemes, GATES models the
operations as the transformation of the propagating information, which mimics
the actual data processing of neural architecture. GATES is a more reasonable
modeling of the neural architectures, and can encode architectures from both
the ""operation on node"" and ""operation on edge"" cell search spaces
consistently. Experimental results on various search spaces confirm GATES's
effectiveness in improving the performance predictor. Furthermore, equipped
with the improved performance predictor, the sample efficiency of the
predictor-based neural architecture search (NAS) flow is boosted. Codes are
available at https://github.com/walkerning/aw_nas.",873,81
"['cs.CV', 'cs.CL']",Dense Contrastive Visual-Linguistic Pretraining,"Inspired by the success of BERT, several multimodal representation learning
approaches have been proposed that jointly represent image and text. These
approaches achieve superior performance by capturing high-level semantic
information from large-scale multimodal pretraining. In particular, LXMERT and
UNITER adopt visual region feature regression and label classification as
pretext tasks. However, they tend to suffer from the problems of noisy labels
and sparse semantic annotations, based on the visual features having been
pretrained on a crowdsourced dataset with limited and inconsistent semantic
labeling. To overcome these issues, we propose unbiased Dense Contrastive
Visual-Linguistic Pretraining (DCVLP), which replaces the region regression and
classification with cross-modality region contrastive learning that requires no
annotations. Two data augmentation strategies (Mask Perturbation and
Intra-/Inter-Adversarial Perturbation) are developed to improve the quality of
negative samples used in contrastive learning. Overall, DCVLP allows
cross-modality dense region contrastive learning in a self-supervised setting
independent of any object annotations. We compare our method against prior
visual-linguistic pretraining frameworks to validate the superiority of dense
contrastive learning on multimodal representation learning.",1346,47
['cs.CV'],"Dressing in Order: Recurrent Person Image Generation for Pose Transfer, Virtual Try-on and Outfit Editing","We proposes a flexible person generation framework called Dressing in Order
(DiOr), which supports 2D pose transfer, virtual try-on, and several fashion
editing tasks. The key to DiOr is a novel recurrent generation pipeline to
sequentially put garments on a person, so that trying on the same garments in
different orders will result in different looks. Our system can produce
dressing effects not achievable by existing work, including different
interactions of garments (e.g., wearing a top tucked into the bottom or over
it), as well as layering of multiple garments of the same type (e.g., jacket
over shirt over t-shirt). DiOr explicitly encodes the shape and texture of each
garment, enabling these elements to be edited separately. Joint training on
pose transfer and inpainting helps with detail preservation and coherence of
generated garments. Extensive evaluations show that DiOr outperforms other
recent methods like ADGAN in terms of output quality, and handles a wide range
of editing functions for which there is no direct supervision.",1051,105
['cs.CV'],Robust pedestrian detection in thermal imagery using synthesized images,"In this paper we propose a method for improving pedestrian detection in the
thermal domain using two stages: first, a generative data augmentation approach
is used, then a domain adaptation method using generated data adapts an RGB
pedestrian detector. Our model, based on the Least-Squares Generative
Adversarial Network, is trained to synthesize realistic thermal versions of
input RGB images which are then used to augment the limited amount of labeled
thermal pedestrian images available for training. We apply our generative data
augmentation strategy in order to adapt a pretrained YOLOv3 pedestrian detector
to detection in the thermal-only domain. Experimental results demonstrate the
effectiveness of our approach: using less than 50\% of available real thermal
training data, and relying on synthesized data generated by our model in the
domain adaptation phase, our detector achieves state-of-the-art results on the
KAIST Multispectral Pedestrian Detection Benchmark; even if more real thermal
data is available adding GAN generated images to the training data results in
improved performance, thus showing that these images act as an effective form
of data augmentation. To the best of our knowledge, our detector achieves the
best single-modality detection results on KAIST with respect to the
state-of-the-art.",1324,71
['cs.CV'],Cross-MPI: Cross-scale Stereo for Image Super-Resolution using Multiplane Images,"Various combinations of cameras enrich computational photography, among which
reference-based superresolution (RefSR) plays a critical role in multiscale
imaging systems. However, existing RefSR approaches fail to accomplish
high-fidelity super-resolution under a large resolution gap, e.g., 8x
upscaling, due to the lower consideration of the underlying scene structure. In
this paper, we aim to solve the RefSR problem in actual multiscale camera
systems inspired by multiplane image (MPI) representation. Specifically, we
propose Cross-MPI, an end-to-end RefSR network composed of a novel plane-aware
attention-based MPI mechanism, a multiscale guided upsampling module as well as
a super-resolution (SR) synthesis and fusion module. Instead of using a direct
and exhaustive matching between the cross-scale stereo, the proposed
plane-aware attention mechanism fully utilizes the concealed scene structure
for efficient attention-based correspondence searching. Further combined with a
gentle coarse-to-fine guided upsampling strategy, the proposed Cross-MPI can
achieve a robust and accurate detail transmission. Experimental results on both
digitally synthesized and optical zoom cross-scale data show that the Cross-MPI
framework can achieve superior performance against the existing RefSR methods
and is a real fit for actual multiscale camera systems even with large-scale
differences.",1393,80
['cs.CV'],Superquadric Object Representation for Optimization-based Semantic SLAM,"Introducing semantically meaningful objects to visual Simultaneous
Localization And Mapping (SLAM) has the potential to improve both the accuracy
and reliability of pose estimates, especially in challenging scenarios with
significant view-point and appearance changes. However, how semantic objects
should be represented for an efficient inclusion in optimization-based SLAM
frameworks is still an open question. Superquadrics(SQs) are an efficient and
compact object representation, able to represent most common object types to a
high degree, and typically retrieved from 3D point-cloud data. However,
accurate 3D point-cloud data might not be available in all applications. Recent
advancements in machine learning enabled robust object recognition and semantic
mask measurements from camera images under many different appearance
conditions. We propose a pipeline to leverage such semantic mask measurements
to fit SQ parameters to multi-view camera observations using a multi-stage
initialization and optimization procedure. We demonstrate the system's ability
to retrieve randomly generated SQ parameters from multi-view mask observations
in preliminary simulation experiments and evaluate different initialization
stages and cost functions.",1246,71
"['cs.LG', 'stat.ML']",Theory and Evaluation Metrics for Learning Disentangled Representations,"We make two theoretical contributions to disentanglement learning by (a)
defining precise semantics of disentangled representations, and (b)
establishing robust metrics for evaluation. First, we characterize the concept
""disentangled representations"" used in supervised and unsupervised methods
along three dimensions-informativeness, separability and interpretability -
which can be expressed and quantified explicitly using information-theoretic
constructs. This helps explain the behaviors of several well-known
disentanglement learning models. We then propose robust metrics for measuring
informativeness, separability and interpretability. Through a comprehensive
suite of experiments, we show that our metrics correctly characterize the
representations learned by different methods and are consistent with
qualitative (visual) results. Thus, the metrics allow disentanglement learning
methods to be compared on a fair ground. We also empirically uncovered new
interesting properties of VAE-based methods and interpreted them with our
formulation. These findings are promising and hopefully will encourage the
design of more theoretically driven models for learning disentangled
representations.",1200,71
"['cs.CV', 'cs.LG', 'cs.RO']",EfficientLPS: Efficient LiDAR Panoptic Segmentation,"Panoptic segmentation of point clouds is a crucial task that enables
autonomous vehicles to comprehend their vicinity using their highly accurate
and reliable LiDAR sensors. Existing top-down approaches tackle this problem by
either combining independent task-specific networks or translating methods from
the image domain ignoring the intricacies of LiDAR data and thus often
resulting in sub-optimal performance. In this paper, we present the novel
top-down Efficient LiDAR Panoptic Segmentation (EfficientLPS) architecture that
addresses multiple challenges in segmenting LiDAR point clouds including
distance-dependent sparsity, severe occlusions, large scale-variations, and
re-projection errors. EfficientLPS comprises of a novel shared backbone that
encodes with strengthened geometric transformation modeling capacity and
aggregates semantically rich range-aware multi-scale features. It incorporates
new scale-invariant semantic and instance segmentation heads along with the
panoptic fusion module which is supervised by our proposed panoptic periphery
loss function. Additionally, we formulate a regularized pseudo labeling
framework to further improve the performance of EfficientLPS by training on
unlabelled data. We benchmark our proposed model on two large-scale LiDAR
datasets: nuScenes, for which we also provide ground truth annotations, and
SemanticKITTI. Notably, EfficientLPS sets the new state-of-the-art on both
these datasets.",1451,51
['cs.LG'],Risk-Averse Offline Reinforcement Learning,"Training Reinforcement Learning (RL) agents in high-stakes applications might
be too prohibitive due to the risk associated to exploration. Thus, the agent
can only use data previously collected by safe policies. While previous work
considers optimizing the average performance using offline data, we focus on
optimizing a risk-averse criteria, namely the CVaR. In particular, we present
the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that
is able to learn risk-averse policies in a fully offline setting. We show that
O-RAAC learns policies with higher CVaR than risk-neutral approaches in
different robot control tasks. Furthermore, considering risk-averse criteria
guarantees distributional robustness of the average performance with respect to
particular distribution shifts. We demonstrate empirically that in the presence
of natural distribution-shifts, O-RAAC learns policies with good average
performance.",939,42
"['cs.CV', 'eess.IV']",Deep Transfer Learning Methods for Colon Cancer Classification in Confocal Laser Microscopy Images,"Purpose: The gold standard for colorectal cancer metastases detection in the
peritoneum is histological evaluation of a removed tissue sample. For feedback
during interventions, real-time in-vivo imaging with confocal laser microscopy
has been proposed for differentiation of benign and malignant tissue by manual
expert evaluation. Automatic image classification could improve the surgical
workflow further by providing immediate feedback.
  Methods: We analyze the feasibility of classifying tissue from confocal laser
microscopy in the colon and peritoneum. For this purpose, we adopt both
classical and state-of-the-art convolutional neural networks to directly learn
from the images. As the available dataset is small, we investigate several
transfer learning strategies including partial freezing variants and full
fine-tuning. We address the distinction of different tissue types, as well as
benign and malignant tissue.
  Results: We present a thorough analysis of transfer learning strategies for
colorectal cancer with confocal laser microscopy. In the peritoneum, metastases
are classified with an AUC of 97.1 and in the colon, the primarius is
classified with an AUC of 73.1. In general, transfer learning substantially
improves performance over training from scratch. We find that the optimal
transfer learning strategy differs for models and classification tasks.
  Conclusions: We demonstrate that convolutional neural networks and transfer
learning can be used to identify cancer tissue with confocal laser microscopy.
We show that there is no generally optimal transfer learning strategy and model
as well as task-specific engineering is required. Given the high performance
for the peritoneum, even with a small dataset, application for intraoperative
decision support could be feasible.",1805,98
['cs.CV'],Attention Model Enhanced Network for Classification of Breast Cancer Image,"Breast cancer classification remains a challenging task due to inter-class
ambiguity and intra-class variability. Existing deep learning-based methods try
to confront this challenge by utilizing complex nonlinear projections. However,
these methods typically extract global features from entire images, neglecting
the fact that the subtle detail information can be crucial in extracting
discriminative features. In this study, we propose a novel method named
Attention Model Enhanced Network (AMEN), which is formulated in a multi-branch
fashion with pixel-wised attention model and classification submodular.
Specifically, the feature learning part in AMEN can generate pixel-wised
attention map, while the classification submodular are utilized to classify the
samples. To focus more on subtle detail information, the sample image is
enhanced by the pixel-wised attention map generated from former branch.
Furthermore, boosting strategy are adopted to fuse classification results from
different branches for better performance. Experiments conducted on three
benchmark datasets demonstrate the superiority of the proposed method under
various scenarios.",1155,74
"['cs.LG', 'cs.CV', 'cs.NE']",World-GAN: a Generative Model for Minecraft Worlds,"This work introduces World-GAN, the first method to perform data-driven
Procedural Content Generation via Machine Learning in Minecraft from a single
example. Based on a 3D Generative Adversarial Network (GAN) architecture, we
are able to create arbitrarily sized world snippets from a given sample. We
evaluate our approach on creations from the community as well as structures
generated with the Minecraft World Generator. Our method is motivated by the
dense representations used in Natural Language Processing (NLP) introduced with
word2vec [1]. The proposed block2vec representations make World-GAN independent
from the number of different blocks, which can vary a lot in Minecraft, and
enable the generation of larger levels. Finally, we demonstrate that changing
this new representation space allows us to change the generated style of an
already trained generator. World-GAN enables its users to generate Minecraft
worlds based on parts of their creations.",964,50
"['stat.ML', 'cs.LG']",Classification in biological networks with hypergraphlet kernels,"Biological and cellular systems are often modeled as graphs in which vertices
represent objects of interest (genes, proteins, drugs) and edges represent
relational ties among these objects (binds-to, interacts-with, regulates). This
approach has been highly successful owing to the theory, methodology and
software that support analysis and learning on graphs. Graphs, however, often
suffer from information loss when modeling physical systems due to their
inability to accurately represent multiobject relationships. Hypergraphs, a
generalization of graphs, provide a framework to mitigate information loss and
unify disparate graph-based methodologies. In this paper, we present a
hypergraph-based approach for modeling physical systems and formulate vertex
classification, edge classification and link prediction problems on
(hyper)graphs as instances of vertex classification on (extended, dual)
hypergraphs in a semi-supervised setting. We introduce a novel kernel method on
vertex- and edge-labeled (colored) hypergraphs for analysis and learning. The
method is based on exact and inexact (via hypergraph edit distances)
enumeration of small simple hypergraphs, referred to as hypergraphlets, rooted
at a vertex of interest. We extensively evaluate this method and show its
potential use in a positive-unlabeled setting to estimate the number of missing
and false positive links in protein-protein interaction networks.",1425,64
"['cs.LG', 'cs.AI', 'cs.RO', 'cs.SY', 'stat.ML']",Supervised Policy Update for Deep Reinforcement Learning,"We propose a new sample-efficient methodology, called Supervised Policy
Update (SPU), for deep reinforcement learning. Starting with data generated by
the current policy, SPU formulates and solves a constrained optimization
problem in the non-parameterized proximal policy space. Using supervised
regression, it then converts the optimal non-parameterized policy to a
parameterized policy, from which it draws new samples. The methodology is
general in that it applies to both discrete and continuous action spaces, and
can handle a wide variety of proximity constraints for the non-parameterized
optimization problem. We show how the Natural Policy Gradient and Trust Region
Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization
(PPO) problem can be addressed by this methodology. The SPU implementation is
much simpler than TRPO. In terms of sample efficiency, our extensive
experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and
outperforms PPO in Atari video game tasks.",1019,56
"['cs.LG', 'math.OC']",Adam revisited: a weighted past gradients perspective,"Adaptive learning rate methods have been successfully applied in many fields,
especially in training deep neural networks. Recent results have shown that
adaptive methods with exponential increasing weights on squared past gradients
(i.e., ADAM, RMSPROP) may fail to converge to the optimal solution. Though many
algorithms, such as AMSGRAD and ADAMNC, have been proposed to fix the
non-convergence issues, achieving a data-dependent regret bound similar to or
better than ADAGRAD is still a challenge to these methods. In this paper, we
propose a novel adaptive method weighted adaptive algorithm (WADA) to tackle
the non-convergence issues. Unlike AMSGRAD and ADAMNC, we consider using a
milder growing weighting strategy on squared past gradient, in which weights
grow linearly. Based on this idea, we propose weighted adaptive gradient method
framework (WAGMF) and implement WADA algorithm on this framework. Moreover, we
prove that WADA can achieve a weighted data-dependent regret bound, which could
be better than the original regret bound of ADAGRAD when the gradients decrease
rapidly. This bound may partially explain the good performance of ADAM in
practice. Finally, extensive experiments demonstrate the effectiveness of WADA
and its variants in comparison with several variants of ADAM on training convex
problems and deep neural networks.",1353,53
"['cs.LG', 'stat.ML']",SecDD: Efficient and Secure Method for Remotely Training Neural Networks,"We leverage what are typically considered the worst qualities of deep
learning algorithms - high computational cost, requirement for large data, no
explainability, high dependence on hyper-parameter choice, overfitting, and
vulnerability to adversarial perturbations - in order to create a method for
the secure and efficient training of remotely deployed neural networks over
unsecured channels.",396,72
"['cs.LG', 'stat.ML']",Impact of Low-bitwidth Quantization on the Adversarial Robustness for Embedded Neural Networks,"As the will to deploy neural networks models on embedded systems grows, and
considering the related memory footprint and energy consumption issues, finding
lighter solutions to store neural networks such as weight quantization and more
efficient inference methods become major research topics. Parallel to that,
adversarial machine learning has risen recently with an impressive and
significant attention, unveiling some critical flaws of machine learning
models, especially neural networks. In particular, perturbed inputs called
adversarial examples have been shown to fool a model into making incorrect
predictions. In this article, we investigate the adversarial robustness of
quantized neural networks under different threat models for a classical
supervised image classification task. We show that quantization does not offer
any robust protection, results in severe form of gradient masking and advance
some hypotheses to explain it. However, we experimentally observe poor
transferability capacities which we explain by quantization value shift
phenomenon and gradient misalignment and explore how these results can be
exploited with an ensemble-based defense.",1168,94
"['cs.LG', 'cs.DB', 'stat.ML']",An End-to-End Deep RL Framework for Task Arrangement in Crowdsourcing Platforms,"In this paper, we propose a Deep Reinforcement Learning (RL) framework for
task arrangement, which is a critical problem for the success of crowdsourcing
platforms. Previous works conduct the personalized recommendation of tasks to
workers via supervised learning methods. However, the majority of them only
consider the benefit of either workers or requesters independently. In
addition, they cannot handle the dynamic environment and may produce
sub-optimal results. To address these issues, we utilize Deep Q-Network (DQN),
an RL-based method combined with a neural network to estimate the expected
long-term return of recommending a task. DQN inherently considers the immediate
and future reward simultaneously and can be updated in real-time to deal with
evolving data and dynamic changes. Furthermore, we design two DQNs that capture
the benefit of both workers and requesters and maximize the profit of the
platform. To learn value functions in DQN effectively, we also propose novel
state representations, carefully design the computation of Q values, and
predict transition probabilities and future states. Experiments on synthetic
and real datasets demonstrate the superior performance of our framework.",1213,79
['cs.LG'],Using Twitter Attribute Information to Predict Stock Prices,"Being able to predict stock prices might be the unspoken wish of stock
investors. Although stock prices are complicated to predict, there are many
theories about what affects their movements, including interest rates, news and
social media. With the help of Machine Learning, complex patterns in data can
be identified beyond the human intellect. In this thesis, a Machine Learning
model for time series forecasting is created and tested to predict stock
prices. The model is based on a neural network with several layers of LSTM and
fully connected layers. It is trained with historical stock values, technical
indicators and Twitter attribute information retrieved, extracted and
calculated from posts on the social media platform Twitter. These attributes
are sentiment score, favourites, followers, retweets and if an account is
verified. To collect data from Twitter, Twitter's API is used. Sentiment
analysis is conducted with VADER. The results show that by adding more Twitter
attributes, the MSE between the predicted prices and the actual prices improved
by 3%. With technical analysis taken into account, MSE decreases from 0.1617 to
0.1437, which is an improvement of around 11%. The restrictions of this study
include that the selected stock has to be publicly listed on the stock market
and popular on Twitter and among individual investors. Besides, the stock
markets' opening hours differ from Twitter, which constantly available. It may
therefore introduce noises in the model.",1494,59
['cs.CV'],Graph Regularized Nonnegative Tensor Ring Decomposition for Multiway Representation Learning,"Tensor ring (TR) decomposition is a powerful tool for exploiting the low-rank
nature of multiway data and has demonstrated great potential in a variety of
important applications. In this paper, nonnegative tensor ring (NTR)
decomposition and graph regularized NTR (GNTR) decomposition are proposed,
where the former equips TR decomposition with local feature extraction by
imposing nonnegativity on the core tensors and the latter is additionally able
to capture manifold geometry information of tensor data, both significantly
extend the applications of TR decomposition for nonnegative multiway
representation learning. Accelerated proximal gradient based methods are
derived for NTR and GNTR. The experimental result demonstrate that the proposed
algorithms can extract parts-based basis with rich colors and rich lines from
tensor objects that provide more interpretable and meaningful representation,
and hence yield better performance than the state-of-the-art tensor based
methods in clustering and classification tasks.",1027,92
"['cs.LG', 'stat.ML']",Identifying through Flows for Recovering Latent Representations,"Identifiability, or recovery of the true latent representations from which
the observed data originates, is de facto a fundamental goal of representation
learning. Yet, most deep generative models do not address the question of
identifiability, and thus fail to deliver on the promise of the recovery of the
true latent sources that generate the observations. Recent work proposed
identifiable generative modelling using variational autoencoders (iVAE) with a
theory of identifiability. Due to the intractablity of KL divergence between
variational approximate posterior and the true posterior, however, iVAE has to
maximize the evidence lower bound (ELBO) of the marginal likelihood, leading to
suboptimal solutions in both theory and practice. In contrast, we propose an
identifiable framework for estimating latent representations using a flow-based
model (iFlow). Our approach directly maximizes the marginal likelihood,
allowing for theoretical guarantees on identifiability, thereby dispensing with
variational approximations. We derive its optimization objective in analytical
form, making it possible to train iFlow in an end-to-end manner. Simulations on
synthetic data validate the correctness and effectiveness of our proposed
method and demonstrate its practical advantages over other existing methods.",1314,63
['cs.CV'],Deep Transfer Learning for Person Re-identification,"Person re-identification (Re-ID) poses a unique challenge to deep learning:
how to learn a deep model with millions of parameters on a small training set
of few or no labels. In this paper, a number of deep transfer learning models
are proposed to address the data sparsity problem. First, a deep network
architecture is designed which differs from existing deep Re-ID models in that
(a) it is more suitable for transferring representations learned from large
image classification datasets, and (b) classification loss and verification
loss are combined, each of which adopts a different dropout strategy. Second, a
two-stepped fine-tuning strategy is developed to transfer knowledge from
auxiliary datasets. Third, given an unlabelled Re-ID dataset, a novel
unsupervised deep transfer learning model is developed based on co-training.
The proposed models outperform the state-of-the-art deep Re-ID models by large
margins: we achieve Rank-1 accuracy of 85.4\%, 83.7\% and 56.3\% on CUHK03,
Market1501, and VIPeR respectively, whilst on VIPeR, our unsupervised model
(45.1\%) beats most supervised models.",1105,51
"['cs.LG', 'stat.ML']",Propositionalization and Embeddings: Two Sides of the Same Coin,"Data preprocessing is an important component of machine learning pipelines,
which requires ample time and resources. An integral part of preprocessing is
data transformation into the format required by a given learning algorithm.
This paper outlines some of the modern data processing techniques used in
relational learning that enable data fusion from different input data types and
formats into a single table data representation, focusing on the
propositionalization and embedding data transformation approaches. While both
approaches aim at transforming data into tabular data format, they use
different terminology and task definitions, are perceived to address different
goals, and are used in different contexts. This paper contributes a unifying
framework that allows for improved understanding of these two data
transformation techniques by presenting their unified definitions, and by
explaining the similarities and differences between the two approaches as
variants of a unified complex data transformation task. In addition to the
unifying framework, the novelty of this paper is a unifying methodology
combining propositionalization and embeddings, which benefits from the
advantages of both in solving complex data transformation and learning tasks.
We present two efficient implementations of the unifying methodology: an
instance-based PropDRM approach, and a feature-based PropStar approach to data
transformation and learning, together with their empirical evaluation on
several relational problems. The results show that the new algorithms can
outperform existing relational learners and can solve much larger problems.",1639,63
['cs.CV'],Anatomy-Aware Siamese Network: Exploiting Semantic Asymmetry for Accurate Pelvic Fracture Detection in X-ray Images,"Visual cues of enforcing bilaterally symmetric anatomies as normal findings
are widely used in clinical practice to disambiguate subtle abnormalities from
medical images. So far, inadequate research attention has been received on
effectively emulating this practice in CAD methods. In this work, we exploit
semantic anatomical symmetry or asymmetry analysis in a complex CAD scenario,
i.e., anterior pelvic fracture detection in trauma PXRs, where semantically
pathological (refer to as fracture) and non-pathological (e.g., pose)
asymmetries both occur. Visually subtle yet pathologically critical fracture
sites can be missed even by experienced clinicians, when limited diagnosis time
is permitted in emergency care. We propose a novel fracture detection framework
that builds upon a Siamese network enhanced with a spatial transformer layer to
holistically analyze symmetric image features. Image features are spatially
formatted to encode bilaterally symmetric anatomies. A new contrastive feature
learning component in our Siamese network is designed to optimize the deep
image features being more salient corresponding to the underlying semantic
asymmetries (caused by pelvic fracture occurrences). Our proposed method have
been extensively evaluated on 2,359 PXRs from unique patients (the largest
study to-date), and report an area under ROC curve score of 0.9771. This is the
highest among state-of-the-art fracture detection methods, with improved
clinical indications.",1480,115
